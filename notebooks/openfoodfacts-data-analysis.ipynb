{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a90c3ff726d132f",
   "metadata": {},
   "source": [
    "# Data Preparation for Public Health Organization - OpenClassrooms Project\n",
    "\n",
    "Hello,\n",
    "\n",
    "The Open Food Facts dataset is available on the official website (or can be downloaded from this [link](https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/parcours-data-scientist/P2/fr.openfoodfacts.org.products.csv.zip)). The variables are defined at this [address](https://world.openfoodfacts.org/data/data-fields.txt). The fields are separated into four sections:\n",
    "\n",
    "1. General information about the product sheet: name, modification date, etc.\n",
    "2. A set of tags: product category, location, origin, etc.\n",
    "3. Ingredients composing the products and their possible additives.\n",
    "4. Nutritional information: quantity in grams of a nutrient per 100 grams of the product.\n",
    "\n",
    "To simplify your approach, I propose starting by establishing the feasibility of suggesting missing values for a variable where more than 50% of the values are missing.\n",
    "\n",
    "Here are the different steps to clean and explore the data:\n",
    "\n",
    "1. **Data Processing**\n",
    "    - Identify relevant variables for future processing and necessary for suggesting missing values.\n",
    "    - Clean the data by:\n",
    "        - Highlighting any missing values among the selected relevant variables, with at least 3 methods of treatment adapted to the concerned variables.\n",
    "        - Identifying and treating any outliers of each variable.\n",
    "    - Automate these processes to avoid repeating these operations. Note that the client wants the program to function if the database is slightly modified (e.g., adding entries).\n",
    "\n",
    "2. **Visualization and Univariate Analysis**\n",
    "    - Throughout the analysis, produce visualizations to better understand the data.\n",
    "    - Perform univariate analysis for each interesting variable to summarize its behavior. Note that the client requests a presentation that explains the analyses made to a lay audience. Therefore, pay attention to readability: text size, color choices, sufficient sharpness, and vary the graphics (box plots, histograms, pie charts, scatter plots, etc.) to best illustrate your points.\n",
    "\n",
    "3. **Variable Selection/Creation using Multivariate Analysis**\n",
    "    - Perform appropriate statistical tests to verify the significance of the results.\n",
    "\n",
    "4. **Report Writing**\n",
    "    - Write an exploration report and a conclusion to explain the feasibility of the requested application.\n",
    "\n",
    "5. **RGPD Compliance**\n",
    "    - Even if the data does not include personal information, explain in a presentation how this project complies with the 5 main principles of GDPR. Santé publique France would like to publish something on the Open Food Facts website to address questions about GDPR compliance that they sometimes receive.\n",
    "\n",
    "Please proceed with these steps to prepare the data for the public health organization's analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726105cf817b2722",
   "metadata": {},
   "source": [
    "## Data Cleaning and Objectives\n",
    "\n",
    "The project is conducted for educational purposes, and the requirements may not necessarily be approached in the same manner in a professional context. The primary goal is to acquire and practice techniques.\n",
    "\n",
    "### Determining Objectives and Implementation of Data Cleaning\n",
    "- **CE1**: You have defined the objectives of your data cleaning in relation to the business issue.\n",
    "- **CE2**: You have defined your approach to data preparation and cleaning.\n",
    "\n",
    "### Cleaning of Structured Data\n",
    "- **CE1**: You have eliminated variables that are not relevant to the application's issues.\n",
    "- **CE2**: You have proposed and justified at least three methods for handling missing values, tailored to the concerned variables (median, setting to zero, IterativeImputer, KNN, deletion, etc.).\n",
    "- **CE3**: You have identified, quantified, and treated outliers for each variable, taking into account the business context.\n",
    "- **CE4**: You have addressed duplicates in variables and records.\n",
    "- **CE5**: You have implemented automation of certain treatments, using appropriate functions and methods.\n",
    "- **CE6**: You have ensured compliance with GDPR norms during the cleaning operations.\n",
    "\n",
    "## Statistical Analysis\n",
    "\n",
    "### Univariate, Bivariate, and Multivariate Analysis\n",
    "- **CE1**: You have highlighted and statistically analyzed potential outliers.\n",
    "- **CE2**: You have correctly characterized the observed distributions (unimodal, bimodal, multimodal).\n",
    "- **CE3**: You have used appropriate metrics (mean or median depending on dispersion).\n",
    "- **CE4**: You have correctly defined the term \"quantiles\".\n",
    "\n",
    "### Bivariate and Multivariate Statistical Analysis\n",
    "- **CE5**: You have presented and analyzed at least three bivariate analysis graphs.\n",
    "- **CE6**: You have explained, justified, and applied at least one method of descriptive multivariate analysis.\n",
    "- **CE7**: You have explained, justified, and applied at least one method of explanatory multivariate analysis.\n",
    "\n",
    "## Data Representation Through Graphics\n",
    "- **CE1**: You have identified cases where it was necessary to create a graph.\n",
    "- **CE2**: You have created readable graphs.\n",
    "- **CE3**: You have implemented at least one of each of the following types of graphs: boxplot, barplot, pie chart, histogram, scatter plot.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91c88b3a3d7ece2",
   "metadata": {},
   "source": [
    "## Using the Anaconda Distribution Manager\n",
    "\n",
    "Anaconda is a popular open-source distribution for Python and R, mainly used for scientific computing and data science. Its goal is to simplify package management and deployment. Package versions are managed through the package manager, conda.\n",
    "To install Anaconda, download the installer from [Anaconda's website](https://www.anaconda.com/products/distribution) and follow the installation guide provided. You can choose between different installers tailored for your operating system.\n",
    "Using Miniconda for a Lightweight Alternative\n",
    "Miniconda is a minimal version of Anaconda that includes only conda, Python, the packages they depend on, and a small number of other useful packages. If you prefer to have more control over your environment and are mindful of space, Miniconda may be the right choice for you.\n",
    "Installation: Visit the [Miniconda repository](https://docs.conda.io/en/latest/miniconda.html) on GitHub or [Miniconda's website](https://docs.conda.io/en/latest/miniconda.html) to download and follow the installation instructions.\n",
    "Creating a Virtual Environment for Each Project\n",
    "It's best practice to create a new virtual environment for each of your projects to avoid conflicts between package versions. To create a new virtual environment, you can use the following command in your terminal\n",
    "\n",
    "## Or You Can Use Google Colab\n",
    "\n",
    "Google Colab is a free cloud service that provides a Jupyter notebook environment for Python developers. It allows you to write and execute Python code in the browser, with no setup required and free access to GPUs.\n",
    "\n",
    "To get started with Google Colab, visit the [Google Colab website](https://colab.research.google.com/). You can create a new notebook, upload an existing notebook, or collaborate with others in real-time.\n",
    "\n",
    "## Features\n",
    "\n",
    "Google Colab provides a range of features, including code execution, text, images, hyperlinks, and more. You can also install additional libraries, access files from Google Drive, and use GPU acceleration.\n",
    "\n",
    "## Utilizing Google Drive for Flexible File Access\n",
    "\n",
    "For this project, the approach chosen is to integrate Google Drive for flexible file access in both Anaconda environments running locally (like in PyCharm) and Google Colab.\n",
    "\n",
    "- **In PyCharm**: You can access data stored in Google Drive by mounting the drive on your system or by syncing files locally.\n",
    "\n",
    "- **In Google Colab**: Mounting Google Drive is straightforward using the following code snippet, which integrates your Drive files directly into your Colab environment:\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```\n",
    "\n",
    "## The code adapts to both local (e.g. Pycharm IDE) and Google Colab environments:\n",
    "\"\"\"\n",
    "This code snippet adapts to both local and Google Colab environments. \n",
    "\n",
    "To use it in Google Colab, follow these steps:\n",
    "1. Create a Gmail account and sign in to Google Drive.\n",
    "2. Import this notebook into Google Colab.\n",
    "3. Before running the notebook, create a folder named \"OC_IA_P3_Prepare_data_for_a_public_health_organization\" in your Google Drive.\n",
    "4. Execute the cell that allow to detect if the notebook is being run on Google Colab (which is on the notebook) below to mount Google Drive and detect the environment if necessary.\n",
    "5. Place the CSV file \"fr.openfoodfacts.org.products.csv\" in the \"data\" subfolder within the project folder. \n",
    "6. Run the notebook cells to load and analyze the data.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b1a6f39d50f859",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Content:\n",
    "\n",
    "### A - Preliminary Exploration of the Dataset\n",
    "- **1. Loading the Data**\n",
    "   - Load data from source.\n",
    "- **2. Initial Data Exploration**\n",
    "   - Descriptive statistics, distribution of values, and identification of obvious data issues.\n",
    "\n",
    "### B - Methodological Approach to Data Cleaning\n",
    "- **1. Handling Duplicates**\n",
    "   - Identify and remove duplicate records\n",
    "- **2. Removal of Irrelevant Variables**\n",
    "    - 2.1 Code for Removing Irrelevant Variables according to Nutri-Score Calculation (domain-specific)\n",
    "        - Nutri-Score Calculation Analysis\n",
    "    - 2.2 Code for Removing Irrelevant Variables according to usual Data Cleaning (non-domain-specific)\n",
    "    - 2.3 Conduct a Detailed Univariate Analysis of Each Variable in the Dataset\n",
    "        - 2.3.1 Statistical Indicators for Quantitative Variables\n",
    "        - 2.3.2 Statistical Indicators for Qualitative Variables\n",
    "        - 2.3.3 Modalities count for qualitative variables  \n",
    "        - 2.3.4 Modalities count for qualitative variables\n",
    "    - Pie chart for the percentage of missing values in each column? CE3 A faire\n",
    "- ## 3 Missing Values Treatment Strategies - CE2\n",
    "    - 3.1 Analyse products that do not have nutrition_score_fr_100g (Nutri-Score) because they are not relevant for the analysis\n",
    "    - 3.2 Analyze the extent and patterns of missing data across different variables.\n",
    "    - 3.3 Reminder of the columns to keep for Nutri-Score calculation: 'nutrition_score_fr_100g ','energy_100g', 'sugars_100g', 'saturated_fat_100g', 'sodium_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g'\n",
    "    - 3.3.1 Analysis of Missing Data for Nutri-Score Calculation Variables\n",
    "    - 3.3.2 Analyse Data Completeness for Nutri-Score Calculation\n",
    "    - 3.3.3 Display missing values in a matrix form (Entities x Variables) for all columns\n",
    "    - 3.3.4 Features  potentially to considered for the prediction model (with less than 50% missing values) and Nutri-Score features used in the calculation \n",
    "- ### 4.Handling Outliers**\n",
    "    - 4.1 Visualizations for Outlier Detection in the domain of Nutri-Score Calculation\n",
    "    - 4.1.1 Business Analysis of Outlier Values for Nutri-Score Variables   \n",
    "    - 4.1.2 Cleaning Outliers in Nutri-Score Variables from a Business Perspective\n",
    "    - 4.1.3 Box Plot Visualization for Outlier Detection for energy_100g since it has a significantly higher range than other variables\n",
    "    - 4.1.4 Definition of Maximum Thresholds for Nutri-Score Variables According to Business Study\n",
    "    - 4.1.5 Visualizations for Outlier Detection in Nutri-Score Variables after Business Cleaning -The 'energy_100g' column is omitted from the nutri_score_columns list. This adjustment ensures better visualization since 'energy_100g' has a significantly higher range than other variables, which can distort the box plot visualization \n",
    "    - 4.1.6 Box Plot Visualization for Nutri-Score Variables after Business outlier cleaning excluding 'energy_100g'\n",
    "    - 4.1.7 Statistical Treatment of Outliers Using the IQR Method¶\n",
    "        - Ensure Consistent Data Types for Nutri-Score Columns only for checking the data types\n",
    "        - Outlier Cleaning Using the IQR Method\n",
    "    - 4.1.8 Boxplot visualizations for energy_100g Outlier Detection in Nutri-Score Variables after IQR Outlier Cleaning\n",
    "    - 4.2 Outlier Analysis from a Statistical Perspective - CE1\n",
    "- **2. Identifying Missing Values**\n",
    "   - Analyze the extent and patterns of missing data across different variables.\n",
    "- **3. Handling Outliers**\n",
    "   - Detect outliers using statistical methods or visualizations.\n",
    "   - Determine whether outliers are due to data entry errors or are genuine extreme values.\n",
    "- **4. Addressing Data Types**\n",
    "   - Ensure correct data types for analysis (e.g., converting strings to numeric where appropriate).\n",
    "- **5. Imputing Missing Values: Strategies - CE2**\n",
    "   - Implementation Sequence and Considerations\n",
    "   - 5.1 Imputation Based on Median Values - CE1 - Median Imputation on Filtered Data Where Nutri-Score Is Present\n",
    "   - 5.2 Zero Imputation for Missing Values - CE1 - Imputation with mean values on Filtered Data Where Nutri-Score Is Present\n",
    "   - 5.3 Zero Imputation for Missing Values - CE1 - Imputation on Filtered Data Where Nutri-Score Is Present\n",
    "   - 5.4 KNN Imputation for Missing Values - CE1 - Imputation on Filtered Data Where Nutri-Score Is Present\n",
    "   - 5.5 Iterative Imputer (Advanced Linear Models) - CE1 - Applied to Filtered Data Where Nutri-Score Is Present\n",
    "      - Understanding and Using Iterative Imputer in Nutri-Score Calculation\n",
    "   - 5.6 filling missing values with the most frequent value in each column\n",
    "   - 5.7 Verification of Data Completeness After Imputation Methods  - CE1\n",
    "   - 5.8 Saving the Imputed Data to a New CSV File - CE1 using the mean imputation method => non analyser et certainement median !!!!!!!\n",
    "   - 5.9 RGPD Compliance and Data Protections Considerations - CE1\n",
    "      - 5.9.1 General Overview of GDPR Principles\n",
    "      - 5.9.2 Data Cleaning Process and GDPR Compliance\n",
    "- **6. Bivariate and Multivariate Analysis**\n",
    "- Descriptive multivariate analysis - CE7**? où cela se trouve dans le plan ci-dessous? => non car univarié discrete et continue\n",
    "- Explanatory multivariate analysis. où cela se trouve dans le plan ci-dessous? => Bivariate ok  fait\n",
    "- \n",
    "-    - 6.1 Bivariate Analysis Graphics - CE5\n",
    "      - 6.1.1 Matrix of correlation between Nutri-Score variables => je dois avoir toutes les nombres de correlation entre les variables de nutriscore!!!!!!!\n",
    "            - Correlation Analysis Observations\n",
    "      - 6.1.2 Scatter Plots for Bivariate Analysis between sugar_100g and energy_100g\n",
    "      - 6.1.3 Scatter Plots for Bivariate Analysis between several Nutri-Score variables => a dupliquer pour chaque variable de nutriscore\n",
    "      - 6.1.4 Quantitative Variables versus quantitatives variables ? Déja fait, les plots\n",
    "      - 6.1.5 Qualitative Variables versus qualitatives variables ? => ANOVA, si PNS different group; idée de verifer que si pnns impacte sur boisson ou sucre. Je le mets à l'endroit le plus pertinent!!!!!!\n",
    "      - 6.1.6 Qualitative Variables versus qualitative variables ? => Ki2\n",
    "      - Observations and justify from  multivariate Analysis ? je fais dans chaque partie ou global ?     \n",
    "- **7. Explanatory Multivariate Analysis Methods - CE7**\n",
    "    - 7.1 Principal Component Analysis (PCA) for Dimensionality Reduction - CE7\n",
    "        - 7.1.1 Data Preparation for PCA ?\n",
    "        - 7.1.2 PCA Implementation and Visualization ? 3D graph ? mettre les 2 dessins, en faire plusieurs to see the best!!!!!!!\n",
    "        - 7.1.3 PCA Analysis and Interpretation ?\n",
    "    - 7.2 Analysis of Variance (ANOVA) for Categorical Variables - CE7 ?\n",
    "        - 7.2.1 Data Preparation for ANOVA ?\n",
    "        - 7.2.2 ANOVA Implementation and Visualization ? juste un dataframe!!!!!\n",
    "        - 7.2.3 ANOVA Analysis and Interpretation ? 3D graph ?\n",
    "    - 7.3 Exploratory Data Analysis of Nutritional Information Across Nutrition Grades            - \n",
    "    - 7.4 CCA Canonical Correlation Analysis ? déja fait concerne uniquement le bivariate analysis=> a supprimer ou compléter dans le bivariate analysis sommaire\n",
    "        - 7.4.1 Data Preparation for CCA ?\n",
    "        - 7.4.2 CCA Implementation and Visualization ? 3D graph ?\n",
    "        - 7.4.3 CCA Analysis and Interpretation ?\n",
    "    - 7.5 explain the choice of the method used for the multivariate analysis according to objective=> en implementant le PCA est ce que je reduis les varaviables de nutriscore ?  necessaire ou pas ? est que avec 4 variables j'ai le même nutriscore idee!\n",
    "- **8. Saving Cleaned Data**\n",
    "   - Persist the cleaned and structured data for subsequent analysis phases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15711495de14ba9d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Content:\n",
    "\n",
    "### A - Preliminary Exploration of the Dataset\n",
    "- **1. Loading the Data**\n",
    "   - Load data from source.\n",
    "- **2. Initial Data Exploration**\n",
    "   - Descriptive statistics, distribution of values, and identification of obvious data issues.\n",
    "\n",
    "### B - Methodological Approach to Data Cleaning\n",
    "- **1. Handling Duplicates**\n",
    "   - Identify and remove duplicate records\n",
    "- **2. Removal of Irrelevant Variables**\n",
    "   - 2.1 Code for Removing Irrelevant Variables according to Nutri-Score Calculation (domain-specific)\n",
    "       - Nutri-Score Calculation Analysis\n",
    "   - 2.2 Code for Removing Irrelevant Variables according to usual Data Cleaning (non-domain-specific)\n",
    "   - 2.3 Conduct a Detailed Univariate Analysis of Each Variable in the Dataset\n",
    "       - 2.3.1 Statistical Indicators for Quantitative Variables\n",
    "       - 2.3.2 Statistical Indicators for Qualitative Variables\n",
    "       - 2.3.3 Modalities count for qualitative variables\n",
    "       - 2.3.4 Modalities count for qualitative variables\n",
    "   - 2.4 Automatisation of the data cleaning process for all the steps or some of them?\n",
    "   - Pie chart for the percentage of missing values in each column? CE3 A faire\n",
    "\n",
    "### C - Two-Level Inference: Imputing Predictive and Target Variables\n",
    "- **1. Imputation of Predictive Variables**\n",
    "   - 1.1 Selection and Implementation of Imputation Methods\n",
    "       - 1.1.1 Median Imputation: Apply median values for imputation on filtered data where Nutri-Score is present.\n",
    "       - 1.1.2 Mean Imputation: Apply mean values for imputation with zero values on filtered data where Nutri-Score is present.\n",
    "       - 1.1.3 Mode Imputation: Fill missing values with the most frequent value in each column.\n",
    "       - 1.1.4 KNN Imputation: Use KNN imputation method on filtered data where Nutri-Score is present.\n",
    "       - (Optionally, if compliant with project requirements) 1.1.5 Iterative Imputer (Advanced Linear Models): Implement iterative imputation using advanced linear models applied to filtered data where Nutri-Score is present.\n",
    "   - 1.2 Verification of Data Completeness After Imputation\n",
    "       - Assess imputation impact by comparing distributions before and after imputation.\n",
    "   - 1.3 Saving the Imputed Predictive Data to a New Dataset\n",
    "       - Document each imputation technique and save the imputed datasets for further analysis.\n",
    "\n",
    "- **2. Recalculation/Inference of Target Variable (Nutri-Score)**\n",
    "   - 2.1 Using the Imputed Predictive Variables to Infer Missing Nutri-Score Values\n",
    "       - 2.1.1 Model Development and Validation: Develop a model to predict Nutri-Score using imputed variables.\n",
    "       - 2.1.2 Analysis of the Impact of Predictive Variable Imputation on Nutri-Score Accuracy: Assess how imputation affects the accuracy of Nutri-Score predictions.\n",
    "   - 2.2 Saving the Fully Imputed Dataset for Subsequent Analysis\n",
    "       - Save the dataset with both the imputed predictive variables and inferred Nutri-Score values for further analysis.\n",
    "\n",
    "- **3. Data Quality and Imputation Strategy Comparison**\n",
    "   - 3.1 Evaluating Imputation Strategies\n",
    "       - Compare the effectiveness and appropriateness of each imputation technique.\n",
    "       - Conduct a detailed comparison of the imputed datasets to the original dataset focusing on data quality and model performance.\n",
    "   - 3.2 GDPR Considerations in Imputation\n",
    "       - Discuss the process of selecting imputation methods in line with GDPR accuracy requirements.\n",
    "   - 3.3 Documentation and Reporting\n",
    "       - Provide a comprehensive record of the imputation process and the rationale behind method selection.\n",
    "       - Prepare a report or presentation summarizing the findings and implications of the imputation strategies used.\n",
    "\n",
    "- **4. Finalizing Dataset Post-Imputation**\n",
    "   - 4.1 Final Adjustments and Saving Cleaned Data\n",
    "       - Ensure all data modifications are final and save the dataset for analysis.\n",
    "   - 4.2 Documentation for Compliance and Transparency\n",
    "       - Document the cleaning and imputation process, decisions made, and methods used for data analysis and GDPR compliance.\n",
    "\n",
    "### D - Handling Outliers and Final Data Adjustments\n",
    "- **1. Handling Outliers**\n",
    "   - 1.1 Visualizations for Outlier Detection in the domain of Nutri-Score Calculation\n",
    "   - 1.2 Cleaning Outliers in Nutri-Score Variables from a Business Perspective\n",
    "   - 1.3 Box Plot Visualization for Outlier Detection for energy_100g\n",
    "   - 1.4 Statistical Treatment of Outliers Using the IQR Method\n",
    "\n",
    "- **2. Addressing Data Types and Final Validation**\n",
    "   - 2.1 Ensure correct data types for analysis\n",
    "   - 2.2 Final Verification of Data Completeness and Accuracy\n",
    "\n",
    "### E - Bivariate and Multivariate Analysis\n",
    "- **1. Bivariate Analysis**\n",
    "   - 1.1 Correlation Matrix and Scatter Plots for Nutri-Score Variables\n",
    "   - 1.2 Analysis and Observations from Bivariate Analysis\n",
    "\n",
    "- **2. Multivariate Analysis**\n",
    "   - 2.1 Principal Component Analysis (PCA) for Dimensionality Reduction\n",
    "   - 2.2 Analysis of Variance (ANOVA) for Categorical Variables\n",
    "   - 2.3 Further Multivariate Analysis Methods as Required\n",
    "\n",
    "### F - Saving and Documenting the Cleaned Data\n",
    "- **1. Saving Cleaned Data**\n",
    "   - Save the final cleaned and structured data for subsequent analysis phases.\n",
    "- **2. Documentation and Compliance**\n",
    "   - Detail the cleaning process to ensure transparency and reproducibility, with a focus on compliance with GDPR and other norms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263215861241a60e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## A - Preliminary Exploration of the Dataset¶"
   ]
  },
  {
   "cell_type": "code",
   "id": "27223d2572e6028",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-05-14T21:34:17.440839Z",
     "start_time": "2024-05-14T21:34:17.431300Z"
    }
   },
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import os\n",
    "import sys\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "outputs": [],
   "execution_count": 216
  },
  {
   "cell_type": "code",
   "id": "e73213502f362045",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-05-14T21:34:22.237199Z",
     "start_time": "2024-05-14T21:34:22.223669Z"
    }
   },
   "source": [
    "# Print the versions of libraries\n",
    "print(f'Pandas version: {pd.__version__}')\n",
    "print(f'Numpy version: {np.__version__}')\n",
    "print(f'Seaborn version: {sns.__version__}')\n",
    "print(f'Scikit-learn version: {sklearn.__version__}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.2.1\n",
      "Numpy version: 1.26.4\n",
      "Seaborn version: 0.12.2\n",
      "Scikit-learn version: 1.1.3\n"
     ]
    }
   ],
   "execution_count": 217
  },
  {
   "cell_type": "code",
   "id": "270fe69fec1b58d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T21:34:22.299422Z",
     "start_time": "2024-05-14T21:34:22.284969Z"
    }
   },
   "source": [
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 218
  },
  {
   "cell_type": "markdown",
   "id": "32f3e48ed5e2ed39",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 1.Loading the Data using the path defined above and detecting the environment"
   ]
  },
  {
   "cell_type": "code",
   "id": "c1406779be242c13",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-05-14T21:34:39.183425Z",
     "start_time": "2024-05-14T21:34:22.496357Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect if the notebook is being run on Google Colab\n",
    "in_colab = 'google.colab' in sys.modules\n",
    "\n",
    "if in_colab:\n",
    "    # Import and mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # Base path for your project folder on Google Drive\n",
    "    base_path = '/content/drive/My Drive/OC_IA_P3_Prepare_data_for_a_public_health_organization/'\n",
    "\n",
    "    # Change the current working directory to the base path\n",
    "    os.chdir(base_path)\n",
    "\n",
    "    # Create the main folder and subfolders if needed\n",
    "    subfolders = ['data', 'notebooks', 'docs', 'models']  # List of subfolders to create\n",
    "    for folder in subfolders:\n",
    "        folder_path = os.path.join(base_path, folder)\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            print(f'Folder created: {folder_path}')  # Confirm folder creation\n",
    "        else:\n",
    "            print(f'Folder already exists: {folder_path}')  # Indicate that the folder already exists\n",
    "\n",
    "    # Specify the path for the data\n",
    "    data_path = os.path.join(base_path, 'data', 'fr.openfoodfacts.org.products.csv')\n",
    "else:\n",
    "    # Local path for PyCharm or any other local environment\n",
    "    base_path = '../'  # Go up one level from the current script location to access the project root\n",
    "    data_path = os.path.join(base_path, 'data', 'fr.openfoodfacts.org.products.csv')  # Path to the data file\n",
    "\n",
    "# Load data using the defined path\n",
    "data = pd.read_csv(data_path, sep='\\t', low_memory=False, on_bad_lines='skip')\n",
    "\n",
    "# Display the first few rows to verify data loading\n",
    "print(data.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            code                                                url  \\\n",
      "0  0000000003087  http://world-fr.openfoodfacts.org/produit/0000...   \n",
      "1  0000000004530  http://world-fr.openfoodfacts.org/produit/0000...   \n",
      "2  0000000004559  http://world-fr.openfoodfacts.org/produit/0000...   \n",
      "3  0000000016087  http://world-fr.openfoodfacts.org/produit/0000...   \n",
      "4  0000000016094  http://world-fr.openfoodfacts.org/produit/0000...   \n",
      "\n",
      "                      creator   created_t      created_datetime  \\\n",
      "0  openfoodfacts-contributors  1474103866  2016-09-17T09:17:46Z   \n",
      "1             usda-ndb-import  1489069957  2017-03-09T14:32:37Z   \n",
      "2             usda-ndb-import  1489069957  2017-03-09T14:32:37Z   \n",
      "3             usda-ndb-import  1489055731  2017-03-09T10:35:31Z   \n",
      "4             usda-ndb-import  1489055653  2017-03-09T10:34:13Z   \n",
      "\n",
      "  last_modified_t last_modified_datetime                    product_name  \\\n",
      "0      1474103893   2016-09-17T09:18:13Z              Farine de blé noir   \n",
      "1      1489069957   2017-03-09T14:32:37Z  Banana Chips Sweetened (Whole)   \n",
      "2      1489069957   2017-03-09T14:32:37Z                         Peanuts   \n",
      "3      1489055731   2017-03-09T10:35:31Z          Organic Salted Nut Mix   \n",
      "4      1489055653   2017-03-09T10:34:13Z                 Organic Polenta   \n",
      "\n",
      "  generic_name quantity  ... ph_100g fruits-vegetables-nuts_100g  \\\n",
      "0          NaN      1kg  ...     NaN                         NaN   \n",
      "1          NaN      NaN  ...     NaN                         NaN   \n",
      "2          NaN      NaN  ...     NaN                         NaN   \n",
      "3          NaN      NaN  ...     NaN                         NaN   \n",
      "4          NaN      NaN  ...     NaN                         NaN   \n",
      "\n",
      "  collagen-meat-protein-ratio_100g cocoa_100g chlorophyl_100g  \\\n",
      "0                              NaN        NaN             NaN   \n",
      "1                              NaN        NaN             NaN   \n",
      "2                              NaN        NaN             NaN   \n",
      "3                              NaN        NaN             NaN   \n",
      "4                              NaN        NaN             NaN   \n",
      "\n",
      "  carbon-footprint_100g nutrition-score-fr_100g nutrition-score-uk_100g  \\\n",
      "0                   NaN                     NaN                     NaN   \n",
      "1                   NaN                    14.0                    14.0   \n",
      "2                   NaN                     0.0                     0.0   \n",
      "3                   NaN                    12.0                    12.0   \n",
      "4                   NaN                     NaN                     NaN   \n",
      "\n",
      "  glycemic-index_100g water-hardness_100g  \n",
      "0                 NaN                 NaN  \n",
      "1                 NaN                 NaN  \n",
      "2                 NaN                 NaN  \n",
      "3                 NaN                 NaN  \n",
      "4                 NaN                 NaN  \n",
      "\n",
      "[5 rows x 162 columns]\n"
     ]
    }
   ],
   "execution_count": 219
  },
  {
   "cell_type": "markdown",
   "id": "8be97b1f2a4bcdad",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Estimate missing lines"
   ]
  },
  {
   "cell_type": "code",
   "id": "9bcdfb0a5d34dd81",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-05-14T21:34:41.276002Z",
     "start_time": "2024-05-14T21:34:39.185431Z"
    }
   },
   "source": [
    "# Count the total lines in the file and calculate the estimated number of lines skipped\n",
    "try:\n",
    "    with open(data_path, 'r', encoding='utf-8') as file:\n",
    "        line_count = sum(1 for line in file)\n",
    "    print(f\"Estimated lines skipped: {line_count - data.shape[0]}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while counting lines: {e}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated lines skipped: 1\n"
     ]
    }
   ],
   "execution_count": 220
  },
  {
   "cell_type": "markdown",
   "id": "847e26bdc1312c5f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Display warning if more than 100 lines were skipped"
   ]
  },
  {
   "cell_type": "code",
   "id": "31dee1dc0f101148",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-05-14T21:34:41.292624Z",
     "start_time": "2024-05-14T21:34:41.278003Z"
    }
   },
   "source": [
    "if (line_count - data.shape[0]) > 100:  # Supposons que perdre plus de 100 lignes est critique\n",
    "    print(\"Warning: More than 100 lines were skipped during the CSV loading.\")"
   ],
   "outputs": [],
   "execution_count": 221
  },
  {
   "cell_type": "markdown",
   "id": "c94a8ddddf3c728c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "id": "dd1dfd5be676da0a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-05-14T21:34:41.307840Z",
     "start_time": "2024-05-14T21:34:41.296549Z"
    }
   },
   "source": [
    "data.shape # display the shape of the data  (rows, columns)\n",
    "print(f'The dataset has {data.shape[0]} rows and {data.shape[1]} columns.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 320772 rows and 162 columns.\n"
     ]
    }
   ],
   "execution_count": 222
  },
  {
   "cell_type": "code",
   "id": "178ffb329624edc2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-05-14T21:34:41.323563Z",
     "start_time": "2024-05-14T21:34:41.308841Z"
    }
   },
   "source": [
    "data.info() # display information about the data"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 320772 entries, 0 to 320771\n",
      "Columns: 162 entries, code to water-hardness_100g\n",
      "dtypes: float64(106), object(56)\n",
      "memory usage: 396.5+ MB\n"
     ]
    }
   ],
   "execution_count": 223
  },
  {
   "cell_type": "code",
   "id": "5648bc8bc496eda7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-05-14T21:34:41.354638Z",
     "start_time": "2024-05-14T21:34:41.325566Z"
    }
   },
   "source": [
    "data.dtypes.value_counts() # display the count of different data types in the data\n",
    "# print first observations of the data (before cleaning) number of quantitative and qualitative variables\n",
    "print(f'The dataset has contains {data.dtypes[data.dtypes == \"object\"].count()} qualitative variables and {data.dtypes[data.dtypes != \"object\"].count()} quantitative variables.')\n",
    "data.dtypes.value_counts()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has contains 56 qualitative variables and 106 quantitative variables.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "float64    106\n",
       "object      56\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 224
  },
  {
   "cell_type": "code",
   "id": "ad157d0b79d35dc3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-05-14T21:34:41.385209Z",
     "start_time": "2024-05-14T21:34:41.355640Z"
    }
   },
   "source": [
    "data.head() # display the first few rows of the data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            code                                                url  \\\n",
       "0  0000000003087  http://world-fr.openfoodfacts.org/produit/0000...   \n",
       "1  0000000004530  http://world-fr.openfoodfacts.org/produit/0000...   \n",
       "2  0000000004559  http://world-fr.openfoodfacts.org/produit/0000...   \n",
       "3  0000000016087  http://world-fr.openfoodfacts.org/produit/0000...   \n",
       "4  0000000016094  http://world-fr.openfoodfacts.org/produit/0000...   \n",
       "\n",
       "                      creator   created_t      created_datetime  \\\n",
       "0  openfoodfacts-contributors  1474103866  2016-09-17T09:17:46Z   \n",
       "1             usda-ndb-import  1489069957  2017-03-09T14:32:37Z   \n",
       "2             usda-ndb-import  1489069957  2017-03-09T14:32:37Z   \n",
       "3             usda-ndb-import  1489055731  2017-03-09T10:35:31Z   \n",
       "4             usda-ndb-import  1489055653  2017-03-09T10:34:13Z   \n",
       "\n",
       "  last_modified_t last_modified_datetime                    product_name  \\\n",
       "0      1474103893   2016-09-17T09:18:13Z              Farine de blé noir   \n",
       "1      1489069957   2017-03-09T14:32:37Z  Banana Chips Sweetened (Whole)   \n",
       "2      1489069957   2017-03-09T14:32:37Z                         Peanuts   \n",
       "3      1489055731   2017-03-09T10:35:31Z          Organic Salted Nut Mix   \n",
       "4      1489055653   2017-03-09T10:34:13Z                 Organic Polenta   \n",
       "\n",
       "  generic_name quantity  ... ph_100g fruits-vegetables-nuts_100g  \\\n",
       "0          NaN      1kg  ...     NaN                         NaN   \n",
       "1          NaN      NaN  ...     NaN                         NaN   \n",
       "2          NaN      NaN  ...     NaN                         NaN   \n",
       "3          NaN      NaN  ...     NaN                         NaN   \n",
       "4          NaN      NaN  ...     NaN                         NaN   \n",
       "\n",
       "  collagen-meat-protein-ratio_100g cocoa_100g chlorophyl_100g  \\\n",
       "0                              NaN        NaN             NaN   \n",
       "1                              NaN        NaN             NaN   \n",
       "2                              NaN        NaN             NaN   \n",
       "3                              NaN        NaN             NaN   \n",
       "4                              NaN        NaN             NaN   \n",
       "\n",
       "  carbon-footprint_100g nutrition-score-fr_100g nutrition-score-uk_100g  \\\n",
       "0                   NaN                     NaN                     NaN   \n",
       "1                   NaN                    14.0                    14.0   \n",
       "2                   NaN                     0.0                     0.0   \n",
       "3                   NaN                    12.0                    12.0   \n",
       "4                   NaN                     NaN                     NaN   \n",
       "\n",
       "  glycemic-index_100g water-hardness_100g  \n",
       "0                 NaN                 NaN  \n",
       "1                 NaN                 NaN  \n",
       "2                 NaN                 NaN  \n",
       "3                 NaN                 NaN  \n",
       "4                 NaN                 NaN  \n",
       "\n",
       "[5 rows x 162 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>url</th>\n",
       "      <th>creator</th>\n",
       "      <th>created_t</th>\n",
       "      <th>created_datetime</th>\n",
       "      <th>last_modified_t</th>\n",
       "      <th>last_modified_datetime</th>\n",
       "      <th>product_name</th>\n",
       "      <th>generic_name</th>\n",
       "      <th>quantity</th>\n",
       "      <th>...</th>\n",
       "      <th>ph_100g</th>\n",
       "      <th>fruits-vegetables-nuts_100g</th>\n",
       "      <th>collagen-meat-protein-ratio_100g</th>\n",
       "      <th>cocoa_100g</th>\n",
       "      <th>chlorophyl_100g</th>\n",
       "      <th>carbon-footprint_100g</th>\n",
       "      <th>nutrition-score-fr_100g</th>\n",
       "      <th>nutrition-score-uk_100g</th>\n",
       "      <th>glycemic-index_100g</th>\n",
       "      <th>water-hardness_100g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000000003087</td>\n",
       "      <td>http://world-fr.openfoodfacts.org/produit/0000...</td>\n",
       "      <td>openfoodfacts-contributors</td>\n",
       "      <td>1474103866</td>\n",
       "      <td>2016-09-17T09:17:46Z</td>\n",
       "      <td>1474103893</td>\n",
       "      <td>2016-09-17T09:18:13Z</td>\n",
       "      <td>Farine de blé noir</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1kg</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000000004530</td>\n",
       "      <td>http://world-fr.openfoodfacts.org/produit/0000...</td>\n",
       "      <td>usda-ndb-import</td>\n",
       "      <td>1489069957</td>\n",
       "      <td>2017-03-09T14:32:37Z</td>\n",
       "      <td>1489069957</td>\n",
       "      <td>2017-03-09T14:32:37Z</td>\n",
       "      <td>Banana Chips Sweetened (Whole)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000000004559</td>\n",
       "      <td>http://world-fr.openfoodfacts.org/produit/0000...</td>\n",
       "      <td>usda-ndb-import</td>\n",
       "      <td>1489069957</td>\n",
       "      <td>2017-03-09T14:32:37Z</td>\n",
       "      <td>1489069957</td>\n",
       "      <td>2017-03-09T14:32:37Z</td>\n",
       "      <td>Peanuts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000000016087</td>\n",
       "      <td>http://world-fr.openfoodfacts.org/produit/0000...</td>\n",
       "      <td>usda-ndb-import</td>\n",
       "      <td>1489055731</td>\n",
       "      <td>2017-03-09T10:35:31Z</td>\n",
       "      <td>1489055731</td>\n",
       "      <td>2017-03-09T10:35:31Z</td>\n",
       "      <td>Organic Salted Nut Mix</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000000016094</td>\n",
       "      <td>http://world-fr.openfoodfacts.org/produit/0000...</td>\n",
       "      <td>usda-ndb-import</td>\n",
       "      <td>1489055653</td>\n",
       "      <td>2017-03-09T10:34:13Z</td>\n",
       "      <td>1489055653</td>\n",
       "      <td>2017-03-09T10:34:13Z</td>\n",
       "      <td>Organic Polenta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 162 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 225
  },
  {
   "cell_type": "code",
   "id": "79662512116b4ec7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-05-14T21:34:41.415759Z",
     "start_time": "2024-05-14T21:34:41.386210Z"
    }
   },
   "source": [
    "data.tail() # display the last few rows of the data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                 code                                                url  \\\n",
       "320767  9948282780603  http://world-fr.openfoodfacts.org/produit/9948...   \n",
       "320768       99567453  http://world-fr.openfoodfacts.org/produit/9956...   \n",
       "320769  9970229501521  http://world-fr.openfoodfacts.org/produit/9970...   \n",
       "320770  9980282863788  http://world-fr.openfoodfacts.org/produit/9980...   \n",
       "320771   999990026839  http://world-fr.openfoodfacts.org/produit/9999...   \n",
       "\n",
       "                           creator   created_t      created_datetime  \\\n",
       "320767  openfoodfacts-contributors  1490631299  2017-03-27T16:14:59Z   \n",
       "320768             usda-ndb-import  1489059076  2017-03-09T11:31:16Z   \n",
       "320769                      tomato  1422099377  2015-01-24T11:36:17Z   \n",
       "320770  openfoodfacts-contributors  1492340089  2017-04-16T10:54:49Z   \n",
       "320771             usda-ndb-import  1489072709  2017-03-09T15:18:29Z   \n",
       "\n",
       "       last_modified_t last_modified_datetime  \\\n",
       "320767      1491244498   2017-04-03T18:34:58Z   \n",
       "320768      1491244499   2017-04-03T18:34:59Z   \n",
       "320769      1491244499   2017-04-03T18:34:59Z   \n",
       "320770      1492340089   2017-04-16T10:54:49Z   \n",
       "320771      1491244499   2017-04-03T18:34:59Z   \n",
       "\n",
       "                                             product_name  \\\n",
       "320767                                  Tomato & ricotta    \n",
       "320768  Mint Melange Tea A Blend Of Peppermint, Lemon ...   \n",
       "320769                                            乐吧泡菜味薯片   \n",
       "320770                           Tomates aux Vermicelles    \n",
       "320771                    Sugar Free Drink Mix, Peach Tea   \n",
       "\n",
       "                           generic_name quantity  ... ph_100g  \\\n",
       "320767                              NaN        1  ...     NaN   \n",
       "320768                              NaN      NaN  ...     NaN   \n",
       "320769  Leba pickle flavor potato chips     50 g  ...     NaN   \n",
       "320770                              NaN      67g  ...     NaN   \n",
       "320771                              NaN      NaN  ...     NaN   \n",
       "\n",
       "       fruits-vegetables-nuts_100g collagen-meat-protein-ratio_100g  \\\n",
       "320767                         NaN                              NaN   \n",
       "320768                         NaN                              NaN   \n",
       "320769                         NaN                              NaN   \n",
       "320770                         NaN                              NaN   \n",
       "320771                         NaN                              NaN   \n",
       "\n",
       "       cocoa_100g chlorophyl_100g carbon-footprint_100g  \\\n",
       "320767        NaN             NaN                   NaN   \n",
       "320768        NaN             NaN                   NaN   \n",
       "320769        NaN             NaN                   NaN   \n",
       "320770        NaN             NaN                   NaN   \n",
       "320771        NaN             NaN                   NaN   \n",
       "\n",
       "       nutrition-score-fr_100g nutrition-score-uk_100g glycemic-index_100g  \\\n",
       "320767                     NaN                     NaN                 NaN   \n",
       "320768                     0.0                     0.0                 NaN   \n",
       "320769                     NaN                     NaN                 NaN   \n",
       "320770                     NaN                     NaN                 NaN   \n",
       "320771                     NaN                     NaN                 NaN   \n",
       "\n",
       "       water-hardness_100g  \n",
       "320767                 NaN  \n",
       "320768                 NaN  \n",
       "320769                 NaN  \n",
       "320770                 NaN  \n",
       "320771                 NaN  \n",
       "\n",
       "[5 rows x 162 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>url</th>\n",
       "      <th>creator</th>\n",
       "      <th>created_t</th>\n",
       "      <th>created_datetime</th>\n",
       "      <th>last_modified_t</th>\n",
       "      <th>last_modified_datetime</th>\n",
       "      <th>product_name</th>\n",
       "      <th>generic_name</th>\n",
       "      <th>quantity</th>\n",
       "      <th>...</th>\n",
       "      <th>ph_100g</th>\n",
       "      <th>fruits-vegetables-nuts_100g</th>\n",
       "      <th>collagen-meat-protein-ratio_100g</th>\n",
       "      <th>cocoa_100g</th>\n",
       "      <th>chlorophyl_100g</th>\n",
       "      <th>carbon-footprint_100g</th>\n",
       "      <th>nutrition-score-fr_100g</th>\n",
       "      <th>nutrition-score-uk_100g</th>\n",
       "      <th>glycemic-index_100g</th>\n",
       "      <th>water-hardness_100g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320767</th>\n",
       "      <td>9948282780603</td>\n",
       "      <td>http://world-fr.openfoodfacts.org/produit/9948...</td>\n",
       "      <td>openfoodfacts-contributors</td>\n",
       "      <td>1490631299</td>\n",
       "      <td>2017-03-27T16:14:59Z</td>\n",
       "      <td>1491244498</td>\n",
       "      <td>2017-04-03T18:34:58Z</td>\n",
       "      <td>Tomato &amp; ricotta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320768</th>\n",
       "      <td>99567453</td>\n",
       "      <td>http://world-fr.openfoodfacts.org/produit/9956...</td>\n",
       "      <td>usda-ndb-import</td>\n",
       "      <td>1489059076</td>\n",
       "      <td>2017-03-09T11:31:16Z</td>\n",
       "      <td>1491244499</td>\n",
       "      <td>2017-04-03T18:34:59Z</td>\n",
       "      <td>Mint Melange Tea A Blend Of Peppermint, Lemon ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320769</th>\n",
       "      <td>9970229501521</td>\n",
       "      <td>http://world-fr.openfoodfacts.org/produit/9970...</td>\n",
       "      <td>tomato</td>\n",
       "      <td>1422099377</td>\n",
       "      <td>2015-01-24T11:36:17Z</td>\n",
       "      <td>1491244499</td>\n",
       "      <td>2017-04-03T18:34:59Z</td>\n",
       "      <td>乐吧泡菜味薯片</td>\n",
       "      <td>Leba pickle flavor potato chips</td>\n",
       "      <td>50 g</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320770</th>\n",
       "      <td>9980282863788</td>\n",
       "      <td>http://world-fr.openfoodfacts.org/produit/9980...</td>\n",
       "      <td>openfoodfacts-contributors</td>\n",
       "      <td>1492340089</td>\n",
       "      <td>2017-04-16T10:54:49Z</td>\n",
       "      <td>1492340089</td>\n",
       "      <td>2017-04-16T10:54:49Z</td>\n",
       "      <td>Tomates aux Vermicelles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67g</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320771</th>\n",
       "      <td>999990026839</td>\n",
       "      <td>http://world-fr.openfoodfacts.org/produit/9999...</td>\n",
       "      <td>usda-ndb-import</td>\n",
       "      <td>1489072709</td>\n",
       "      <td>2017-03-09T15:18:29Z</td>\n",
       "      <td>1491244499</td>\n",
       "      <td>2017-04-03T18:34:59Z</td>\n",
       "      <td>Sugar Free Drink Mix, Peach Tea</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 162 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 226
  },
  {
   "cell_type": "code",
   "id": "3afb5312adcf1a2d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "ExecuteTime": {
     "end_time": "2024-05-14T21:34:42.621860Z",
     "start_time": "2024-05-14T21:34:41.416760Z"
    }
   },
   "source": [
    "data.describe() # display summary statistics of the data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       no_nutriments    additives_n  ingredients_from_palm_oil_n  \\\n",
       "count            0.0  248939.000000                248939.000000   \n",
       "mean             NaN       1.936024                     0.019659   \n",
       "std              NaN       2.502019                     0.140524   \n",
       "min              NaN       0.000000                     0.000000   \n",
       "25%              NaN       0.000000                     0.000000   \n",
       "50%              NaN       1.000000                     0.000000   \n",
       "75%              NaN       3.000000                     0.000000   \n",
       "max              NaN      31.000000                     2.000000   \n",
       "\n",
       "       ingredients_from_palm_oil  ingredients_that_may_be_from_palm_oil_n  \\\n",
       "count                        0.0                            248939.000000   \n",
       "mean                         NaN                                 0.055246   \n",
       "std                          NaN                                 0.269207   \n",
       "min                          NaN                                 0.000000   \n",
       "25%                          NaN                                 0.000000   \n",
       "50%                          NaN                                 0.000000   \n",
       "75%                          NaN                                 0.000000   \n",
       "max                          NaN                                 6.000000   \n",
       "\n",
       "       ingredients_that_may_be_from_palm_oil  nutrition_grade_uk  \\\n",
       "count                                    0.0                 0.0   \n",
       "mean                                     NaN                 NaN   \n",
       "std                                      NaN                 NaN   \n",
       "min                                      NaN                 NaN   \n",
       "25%                                      NaN                 NaN   \n",
       "50%                                      NaN                 NaN   \n",
       "75%                                      NaN                 NaN   \n",
       "max                                      NaN                 NaN   \n",
       "\n",
       "        energy_100g  energy-from-fat_100g       fat_100g  ...    ph_100g  \\\n",
       "count  2.611130e+05            857.000000  243891.000000  ...  49.000000   \n",
       "mean   1.141915e+03            585.501214      12.730379  ...   6.425698   \n",
       "std    6.447154e+03            712.809943      17.578747  ...   2.047841   \n",
       "min    0.000000e+00              0.000000       0.000000  ...   0.000000   \n",
       "25%    3.770000e+02             49.400000       0.000000  ...   6.300000   \n",
       "50%    1.100000e+03            300.000000       5.000000  ...   7.200000   \n",
       "75%    1.674000e+03            898.000000      20.000000  ...   7.400000   \n",
       "max    3.251373e+06           3830.000000     714.290000  ...   8.400000   \n",
       "\n",
       "       fruits-vegetables-nuts_100g  collagen-meat-protein-ratio_100g  \\\n",
       "count                  3036.000000                        165.000000   \n",
       "mean                     31.458587                         15.412121   \n",
       "std                      31.967918                          3.753028   \n",
       "min                       0.000000                          8.000000   \n",
       "25%                       0.000000                         12.000000   \n",
       "50%                      23.000000                         15.000000   \n",
       "75%                      51.000000                         15.000000   \n",
       "max                     100.000000                         25.000000   \n",
       "\n",
       "       cocoa_100g  chlorophyl_100g  carbon-footprint_100g  \\\n",
       "count  948.000000              0.0             268.000000   \n",
       "mean    49.547785              NaN             341.700764   \n",
       "std     18.757932              NaN             425.211439   \n",
       "min      6.000000              NaN               0.000000   \n",
       "25%     32.000000              NaN              98.750000   \n",
       "50%     50.000000              NaN             195.750000   \n",
       "75%     64.250000              NaN             383.200000   \n",
       "max    100.000000              NaN            2842.000000   \n",
       "\n",
       "       nutrition-score-fr_100g  nutrition-score-uk_100g  glycemic-index_100g  \\\n",
       "count            221210.000000            221210.000000                  0.0   \n",
       "mean                  9.165535                 9.058049                  NaN   \n",
       "std                   9.055903                 9.183589                  NaN   \n",
       "min                 -15.000000               -15.000000                  NaN   \n",
       "25%                   1.000000                 1.000000                  NaN   \n",
       "50%                  10.000000                 9.000000                  NaN   \n",
       "75%                  16.000000                16.000000                  NaN   \n",
       "max                  40.000000                40.000000                  NaN   \n",
       "\n",
       "       water-hardness_100g  \n",
       "count                  0.0  \n",
       "mean                   NaN  \n",
       "std                    NaN  \n",
       "min                    NaN  \n",
       "25%                    NaN  \n",
       "50%                    NaN  \n",
       "75%                    NaN  \n",
       "max                    NaN  \n",
       "\n",
       "[8 rows x 106 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>no_nutriments</th>\n",
       "      <th>additives_n</th>\n",
       "      <th>ingredients_from_palm_oil_n</th>\n",
       "      <th>ingredients_from_palm_oil</th>\n",
       "      <th>ingredients_that_may_be_from_palm_oil_n</th>\n",
       "      <th>ingredients_that_may_be_from_palm_oil</th>\n",
       "      <th>nutrition_grade_uk</th>\n",
       "      <th>energy_100g</th>\n",
       "      <th>energy-from-fat_100g</th>\n",
       "      <th>fat_100g</th>\n",
       "      <th>...</th>\n",
       "      <th>ph_100g</th>\n",
       "      <th>fruits-vegetables-nuts_100g</th>\n",
       "      <th>collagen-meat-protein-ratio_100g</th>\n",
       "      <th>cocoa_100g</th>\n",
       "      <th>chlorophyl_100g</th>\n",
       "      <th>carbon-footprint_100g</th>\n",
       "      <th>nutrition-score-fr_100g</th>\n",
       "      <th>nutrition-score-uk_100g</th>\n",
       "      <th>glycemic-index_100g</th>\n",
       "      <th>water-hardness_100g</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0.0</td>\n",
       "      <td>248939.000000</td>\n",
       "      <td>248939.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>248939.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.611130e+05</td>\n",
       "      <td>857.000000</td>\n",
       "      <td>243891.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>3036.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>948.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>221210.000000</td>\n",
       "      <td>221210.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.936024</td>\n",
       "      <td>0.019659</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.141915e+03</td>\n",
       "      <td>585.501214</td>\n",
       "      <td>12.730379</td>\n",
       "      <td>...</td>\n",
       "      <td>6.425698</td>\n",
       "      <td>31.458587</td>\n",
       "      <td>15.412121</td>\n",
       "      <td>49.547785</td>\n",
       "      <td>NaN</td>\n",
       "      <td>341.700764</td>\n",
       "      <td>9.165535</td>\n",
       "      <td>9.058049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.502019</td>\n",
       "      <td>0.140524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.269207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.447154e+03</td>\n",
       "      <td>712.809943</td>\n",
       "      <td>17.578747</td>\n",
       "      <td>...</td>\n",
       "      <td>2.047841</td>\n",
       "      <td>31.967918</td>\n",
       "      <td>3.753028</td>\n",
       "      <td>18.757932</td>\n",
       "      <td>NaN</td>\n",
       "      <td>425.211439</td>\n",
       "      <td>9.055903</td>\n",
       "      <td>9.183589</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>-15.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.770000e+02</td>\n",
       "      <td>49.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.100000e+03</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195.750000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.674000e+03</td>\n",
       "      <td>898.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>64.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>383.200000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.251373e+06</td>\n",
       "      <td>3830.000000</td>\n",
       "      <td>714.290000</td>\n",
       "      <td>...</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2842.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 106 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 227
  },
  {
   "cell_type": "code",
   "id": "c584cc42061143e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T21:34:42.652797Z",
     "start_time": "2024-05-14T21:34:42.626118Z"
    }
   },
   "source": [
    "# Number of missing values in nutrition_grade_fr\n",
    "data['nutrition_grade_fr'].isna().sum()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99562"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 228
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T21:35:20.252530Z",
     "start_time": "2024-05-14T21:35:20.231143Z"
    }
   },
   "cell_type": "code",
   "source": [
    " # number of missing values in nutrition_score_fr_100g\n",
    "data['nutrition-score-fr_100g'].isna().sum()"
   ],
   "id": "129c77c9496a52f6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99562"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 230
  },
  {
   "cell_type": "markdown",
   "id": "4ce07fde759708df",
   "metadata": {},
   "source": [
    "### 2.1 Product pnn_group_1,pnns_group_2, main_category_fr and categories_fr, 4 variables that are important for the categorization of the products (french scope of products)  - CE1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713684f8fdf29b3e",
   "metadata": {},
   "source": [
    "| **Level** | **Classification Type**       | **Description**                                                                                                      |\n",
    "|-----------|-------------------------------|----------------------------------------------------------------------------------------------------------------------|\n",
    "| 1         | Categories (`categories_fr`)  | General types of food products such as beverages, snacks, dairy products. Influences the specific Nutri-Score calculations. |\n",
    "| 2         | Main Category (`main_category_fr`) | The primary category under which a product is classified, determining the main group for Nutri-Score calculations.    |\n",
    "| 3         | PNNS Groups 1 (`pnns_groups_1`) | Broad public health nutrition categories such as 'fruits and vegetables', 'sugary drinks'. Provides a public health context. |\n",
    "| 4         | PNNS Groups 2 (`pnns_groups_2`) | More detailed sub-categories within PNNS Groups 1, like distinguishing fresh fruits from fruit juices.               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4107f1ba03d7b7e",
   "metadata": {},
   "source": "### Standardizing Column Names for Python Compatibility"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T21:34:42.725894Z",
     "start_time": "2024-05-14T21:34:42.725894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assure that in columns names '-' is replaced with '_' because - is not allowed in python\n",
    "data.columns = data.columns.str.replace('-', '_')"
   ],
   "id": "c2c50e36e34bc56e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Standardizes the entries by converting them to lowercase and replacing hyphens with spaces for consistency and ease of analysis\n",
    "data[\"pnns_groups_1\"] = data[\"pnns_groups_1\"].str.lower().str.replace('-', ' ')"
   ],
   "id": "bcd1f54dd3d19142",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data[['categories_fr','pnns_groups_1','pnns_groups_2','main_category_fr',]].sample(100)",
   "id": "905e647a3362abc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Columns to analyze\n",
    "columns = ['categories_fr', 'pnns_groups_1', 'pnns_groups_2', 'main_category_fr','nutrition_grade_fr']\n",
    "\n",
    "# Loop through each column to display unique values and count missing values\n",
    "for col in columns:\n",
    "    print(f\"--- Analysis for {col} ---\")\n",
    "    \n",
    "    # Display the count of unique values in the column\n",
    "    print(f\"Count of unique values in '{col}': {data[col].nunique()}\")\n",
    "    \n",
    "    # Display value counts to see how many times each value appears\n",
    "    print(f\"Value counts for '{col}':\")\n",
    "    print(data[col].value_counts(dropna=False))\n",
    "    \n",
    "    # Check for missing values and print the total number\n",
    "    missing_count = data[col].isnull().sum()\n",
    "    print(f\"The dataset has {missing_count} missing values in the '{col}' variable.\\n\")\n"
   ],
   "id": "f08abfa46b1a788a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_entries = 320771 # Total number of entries in the dataset\n",
    "\n",
    "categories_missing_pct = (236361 / total_entries) * 100\n",
    "pnns_groups_1_missing_pct = (229259 / total_entries) * 100\n",
    "pnns_groups_2_missing_pct = (226281 / total_entries) * 100\n",
    "main_category_fr_missing_pct = (236406 / total_entries) * 100\n",
    "\n",
    "print(f\"Percentage of missing values for 'categories_fr': {categories_missing_pct:.2f}%\")\n",
    "print(f\"Percentage of missing values for 'pnns_groups_1': {pnns_groups_1_missing_pct:.2f}%\")\n",
    "print(f\"Percentage of missing values for 'pnns_groups_2': {pnns_groups_2_missing_pct:.2f}%\")\n",
    "print(f\"Percentage of missing values for 'main_category_fr': {main_category_fr_missing_pct:.2f}%\")\n"
   ],
   "id": "df4e84896a48a9fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Observation :\n",
    "A substantial amount of data is missing across all four categorical variables.\n",
    "The percentage of missing values indicates a need for significant data cleaning and possibly imputation strategies before any further analysis or model training.\n",
    "The impact of these missing values can be profound, especially if these categories are critical for subsequent analyses or model predictions.Observation: The 'categories_fr', 'pnns_groups_1', 'pnns_groups_2', and 'main_category_fr' columns contain a significant number of missing values. - CE1"
   ],
   "id": "f514c53bf5a56079"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data loading and setup (make sure to replace this with your actual data loading code)\n",
    "# data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Check for missing values in the specified columns\n",
    "missing_data = data[['categories_fr', 'main_category_fr', 'pnns_groups_1', 'pnns_groups_2']].isnull()\n",
    "\n",
    "# Show the number of missing values per column\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_data.sum())\n",
    "\n",
    "# Analyze patterns of missingness\n",
    "print(\"\\nPatterns of missingness:\")\n",
    "# Check if when 'categories_fr' is missing, 'main_category_fr' is also missing\n",
    "print(\"When 'categories_fr' is missing, is 'main_category_fr' also missing?\")\n",
    "print(missing_data[missing_data['categories_fr']]['main_category_fr'].all())\n",
    "\n",
    "# Check if when 'main_category_fr' is missing, 'pnns_groups_1' is also missing\n",
    "print(\"When 'main_category_fr' is missing, is 'pnns_groups_1' also missing?\")\n",
    "print(missing_data[missing_data['main_category_fr']]['pnns_groups_1'].all())\n",
    "\n",
    "# Check if when 'pnns_groups_1' is missing, 'pnns_groups_2' is also missing\n",
    "print(\"When 'pnns_groups_1' is missing, is 'pnns_groups_2' also missing?\")\n",
    "print(missing_data[missing_data['pnns_groups_1']]['pnns_groups_2'].all())"
   ],
   "id": "41aaae96d3e9a7b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "# Filtering the DataFrame to include rows where at least one of the specified columns is not NaN\n",
    "filtered_data = data.dropna(subset=['categories_fr', 'main_category_fr', 'pnns_groups_1', 'pnns_groups_2'], how='all')\n",
    "\n",
    "# Displaying the shape of the resulting DataFrame\n",
    "print(\"Shape of the DataFrame where at least one of the variables 'categories_fr', 'main_category_fr', 'pnns_groups_1', 'pnns_groups_2' is present:\", filtered_data.shape)\n",
    "\n"
   ],
   "id": "b4c80b488e2d6be8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Observation: After filtering out rows where at least one of the specified categorical variables is present, the resulting DataFrame has 94k rows and 162 columns. This indicates that many rows had missing values in all four categorical variables. - CE1",
   "id": "543708af6d759236"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# \n",
    "# # Path to the data file\n",
    "# data_path = os.path.join(base_path, 'data', 'fr.openfoodfacts.org.products.csv')\n",
    "# \n",
    "# # Load data using the defined path\n",
    "# data = pd.read_csv(data_path, sep='\\t', low_memory=False, on_bad_lines='skip')\n",
    "# \n",
    "# # Replace hyphens with underscores in column names\n",
    "# data.columns = data.columns.str.replace('-', '_')\n",
    "# \n",
    "# # Display the first few rows to verify data loading and column name changes\n",
    "# print(data.head())\n",
    "# \n",
    "# # List of qualitative variables that must be completely non-null, including 'nutrition_score_fr_100g'\n",
    "# required_non_null_columns = [\n",
    "#     'categories_fr', 'pnns_groups_1', 'pnns_groups_2', 'main_category_fr', 'nutrition_grade_fr', 'nutrition_score_fr_100g'\n",
    "# ]\n",
    "# \n",
    "# # List of quantitative variables\n",
    "# quantitative_columns = [\n",
    "#     'nutrition_score_fr_100g', 'energy_100g', 'sugars_100g', 'saturated_fat_100g', \n",
    "#     'sodium_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g'\n",
    "# ]\n",
    "# \n",
    "# # Check if all required columns are present in the DataFrame\n",
    "# missing_columns = [col for col in required_non_null_columns if col not in data.columns]\n",
    "# if missing_columns:\n",
    "#     raise KeyError(f\"Missing columns in the DataFrame: {missing_columns}\")\n",
    "# \n",
    "# # Filter out rows where any of the required qualitative columns or 'nutrition_score_fr_100g' are NaN\n",
    "# base_data_for_method = data.dropna(subset=required_non_null_columns).copy()\n",
    "# \n",
    "# # Replace NaNs and empty strings in quantitative variables with zeros using .loc to avoid SettingWithCopyWarning\n",
    "# quantitative_columns_to_fill = [col for col in quantitative_columns if col != 'nutrition_score_fr_100g']\n",
    "# for col in quantitative_columns_to_fill:\n",
    "#     base_data_for_method.loc[:, col] = base_data_for_method.loc[:, col].replace('', 0)\n",
    "#     base_data_for_method.loc[:, col] = base_data_for_method.loc[:, col].fillna(0)\n",
    "# \n",
    "# # Displaying the shape of the new DataFrame to verify the number of complete cases\n",
    "# print(\"Shape of the new DataFrame with selected columns:\", base_data_for_method.shape)\n",
    "# \n",
    "# # Path to save the cleaned DataFrame\n",
    "# output_path = os.path.join(base_path, 'data', 'base_data_for_method.csv')\n",
    "# \n",
    "# # Saving the cleaned DataFrame to the specified path\n",
    "# base_data_for_method.to_csv(output_path, index=False)\n",
    "# print(f'DataFrame saved to {output_path}')\n"
   ],
   "id": "3ff157f9c106bce6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "# Define the columns where you want to replace missing values and empty strings with 'unknown'\n",
    "columns = ['categories_fr', 'main_category_fr', 'pnns_groups_1', 'pnns_groups_2']\n",
    "\n",
    "# Replace missing values and empty strings in the specified columns with 'unknown'\n",
    "for col in columns:\n",
    "    data[col] = data[col].fillna('unknown')  # Replace NaN values with 'unknown'\n",
    "    data[col] = data[col].replace('', 'unknown')  # Replace empty strings with 'unknown'\n",
    "\n",
    "# Compute the count of 'unknown' in each column\n",
    "unknown_counts = {col: (data[col] == 'unknown').sum() for col in columns}\n",
    "\n",
    "# Create a DataFrame from the counts\n",
    "unknown_counts_df = pd.DataFrame(unknown_counts, index=[0])\n",
    "\n",
    "# Verify the replacement by showing a summary of null values in the DataFrame\n",
    "print(\"\\nVerification: Number of null values in each specified column after replacement:\")\n",
    "print(data[columns].isnull().sum())\n",
    "\n",
    "# Optionally, verify the replacement by showing a summary of 'unknown' values in the DataFrame\n",
    "print(\"Count of 'unknown' in each column:\")\n",
    "print(data[columns].apply(lambda x: (x == 'unknown').sum()))\n"
   ],
   "id": "1cd0b95be9b6e84f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the count of unique values in the 'pnns_groups_1' column and in the 'pnns_groups_2' column\n",
    "print(f\"The dataset has {data['pnns_groups_1'].nunique()} unique values in the 'pnns_groups_1' variable.\")\n",
    "print(f\"The dataset has {data['pnns_groups_2'].nunique()} unique values in the 'pnns_groups_2' variable.\")"
   ],
   "id": "7d442d0dcd30f968",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute value counts for each column\n",
    "pnns_groups_1_counts = data['pnns_groups_1'].value_counts()\n",
    "pnns_groups_2_counts = data['pnns_groups_2'].value_counts()\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "counts_df = pd.DataFrame({\n",
    "    'pnns_groups_1': pnns_groups_1_counts,\n",
    "    'pnns_groups_2': pnns_groups_2_counts\n",
    "})\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(counts_df)"
   ],
   "id": "ebc69f7f09235a6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Observation : we have now a total for fruits and vegetables and fruits-and-vegetables up to 6895 which was before 987 for fruits-and-vegetables and 5908 for fruits and vegetables. The pnns groups 1 and 2 are now standardized and ready for further analysis. - CE1",
   "id": "4aa74cb5abee4774"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your DataFrame containing the dataset\n",
    "# Group data by 'categories_fr' and aggregate unique 'main_category_fr' counts\n",
    "category_relations = data.groupby('categories_fr')['main_category_fr'].nunique()\n",
    "\n",
    "# Convert the Series to a DataFrame for better readability\n",
    "category_relations_df = category_relations.reset_index()\n",
    "category_relations_df.columns = ['categories_fr', 'unique_main_categories_count']\n",
    "\n",
    "# Sort the DataFrame by the count of unique main categories to see which categories have the most variety\n",
    "category_relations_df = category_relations_df.sort_values(by='unique_main_categories_count', ascending=False)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(category_relations_df)\n",
    "\n"
   ],
   "id": "349745b7d03a5931",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Observations:\n",
    "\n",
    "- Redundancy and Overlap in Categories:\n",
    "    Many entries in `categories_fr` contain multiple subcategories combined with commas, such as \"Produits laitiers,Yaourts,Yaourts aux fruits,Y...\". This suggests that some categories are not distinct and could be broken down into separate levels of categorization.\n",
    "\n",
    "- High Variety in Some Categories:\n",
    "    Some categories have a count of 2 unique main categories, indicating that these entries might be duplicated or not well-standardized. Example: \"Produits laitiers,Yaourts,Yaourts aux fruits,Y...\" appears multiple times with different counts.\n",
    "\n",
    "- Low Variety in Some Categories:\n",
    "    Many entries have a unique main category count of 1, indicating that these categories are more consistent. Example: \"Boissons,Boissons gazeuses,Sodas,Sodas au cola...\" appears with a count of 1, suggesting consistency in these categories.\n",
    "\n",
    "- Potential Data Quality Issues:\n",
    "    Categories such as \"Édulcorants,Sucres,ru:Сахар-прессованный\" mix French and other languages (Russian in this case), indicating a need for language standardization. Entries like these might affect the consistency and quality of the data.\n",
    "\n",
    "Potential Actions:\n",
    "\n",
    "- Standardize Category Names:\n",
    "    Clean and standardize category names to ensure consistency. This could involve:\n",
    "        - Converting all category names to lowercase.\n",
    "        - Replacing special characters with standard equivalents (e.g., replacing `,` with `;` if it's used as a separator).\n",
    "        - Translating non-French entries to French if the primary language of the dataset is French.\n",
    "\n",
    "- Break Down Composite Categories:\n",
    "    Decompose composite categories into individual levels for more granular analysis. Example: Split \"Produits laitiers,Yaourts,\".\n",
    "     \n",
    "- Actions to be Taken:\n",
    "    Further analysis and data cleaning are required to address the issues identified in the categories_fr column. This process will involve standardizing category names, breaking down composite categories, and ensuring consistency and accuracy in the data but need time and resources to be done. \n"
   ],
   "id": "c3afa0efac21710e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Conclusion of Analysis on Variables: 'categories_fr', 'pnns_groups_1', 'pnns_groups_2', 'main_category_fr'",
   "id": "568c75b33154a438"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Completeness and Missingness Patterns:\n",
    "\n",
    "- 1 Our analysis revealed a notable occurrence of \"unknown\" values in the variables categories_fr, pnns_groups_1, pnns_groups_2, and main_category_fr. Notably, when categories_fr is missing, main_category_fr is consistently missing as well, underscoring a strong hierarchical relationship between these variables. Conversely, the absence of values in main_category_fr does not predict similar missingness in pnns_groups_1 or pnns_groups_2, indicating a looser association at these deeper levels of categorization.\n",
    "- 2  Decision to Retain Variables:\n",
    "Given the initial findings, the decision to maintain all variables, including those with a substantial presence of \"unknown\" values, is validated by the necessity to uphold comprehensive data integrity for accurate Nutri-Score calculation.\n",
    "- 3 Despite significant occurrences of missing data in the variables categories_fr, pnns_groups_1, pnns_groups_2, and main_category_fr, our focus will shift towards the inference of variables crucial for calculating nutrition_score_fr_100g (Nutri-Score). Recognizing the gaps in our data, our strategic approach will concentrate on enhancing the accuracy and reliability of Nutri-Score predictions by leveraging the most directly relevant nutritional information available in the dataset."
   ],
   "id": "e0a384fdeba08257"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "175849a2d737060e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# display information about the data, verbose mode on means all columns will be displayed Dtype, Non-Null Count, Memory Usage\n",
    "data.info(verbose = True)"
   ],
   "id": "7a81097cf4883f56",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15ab62c8e70ad4aa",
   "metadata": {},
   "source": [
    "# display errors in columns names according to python naming conventions like '-' is not allowed\n",
    "data.columns # display the column names"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "88cc0fb48375fd12",
   "metadata": {},
   "source": [
    "# Assure that in columns names '-' is replaced with '_' because - is not allowed in python\n",
    "data.columns = data.columns.str.replace('-', '_')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30fef61e3a87a5de",
   "metadata": {},
   "source": [
    "# For colum quantitative_columns 'nutrition_score_fr_100g', 'energy_100g', 'sugars_100g', 'saturated_fat_100g', 'sodium_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g', replace NaN with 0 used for further imputation calculations\n",
    "data[['nutrition_score_fr_100g', 'energy_100g', 'sugars_100g', 'saturated_fat_100g', 'sodium_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g']] = data[['nutrition_score_fr_100g', 'energy_100g', 'sugars_100g', 'saturated_fat_100g', 'sodium_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g']].fillna(0)    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e0a53501914ac328",
   "metadata": {},
   "source": [
    "data.info(verbose = True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7c966a3c97e9a87",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## B - Methodological Approach to Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee34795bf412e8c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 1. Handling Duplicates- CE4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669884773850829a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "*Identify and remove duplicate records to prevent skewed analysis results.\n",
    "\n",
    "*Approach for detecting and removing duplicates within the data.\n",
    "\n",
    "The dataset may contain duplicate entries that can skew analyses and lead to inaccurate results. Therefore, it is crucial to identify and remove these duplicates to maintain the integrity of the dataset. The variables code, product_name, and brands are often sufficient to uniquely identify a product in an inventory or database. These fields are typically chosen because they represent key identifiers or attributes that, when combined, offer a high likelihood of uniqueness for each product entry. Extending duplicate removal to additional variables can unnecessarily increase data processing complexity and reduce analysis performance, especially if the dataset is large. It is often more effective to target duplicates based on specific criteria that most impact data integrity.\n",
    "\n",
    "### a. Unique Identifier - 'code'\n",
    "\n",
    "- **Rationale**: The 'code' column is presumed to be a unique identifier for each product. If there are duplicates in this column, it likely indicates completely redundant records in the dataset.\n",
    "- **Action**: We use `drop_duplicates(subset='code', inplace=True)` to remove these duplicates. This ensures that each product is represented only once based on its unique code.\n",
    "\n",
    "### b. Product Names and Brands - 'product_name' and 'brands'\n",
    "\n",
    "- **Rationale**: Products might be listed more than once with different codes but the same product name and brand. This can happen due to variations in other attributes or errors in data entry.\n",
    "- **Action**: After ensuring unique codes, we further clean the dataset by removing entries that have the same product name and brand. This step helps in reducing redundancy when the same product appears under multiple entries possibly due to listing in different regions or stores.\n",
    "\n",
    "### c. Overall Dataset Duplication Check\n",
    "\n",
    "- **Rationale**: Finally, a general check for any remaining duplicates across all columns ensures no other form of duplication is present.\n",
    "- **Action**: `data.duplicated().sum()` is used to verify if there are any more duplicates left in the dataset, ensuring a thorough cleanup.\n",
    "\n",
    "These steps are essential for maintaining a clean and reliable dataset, which is critical for any subsequent data analysis or machine learning models that depend on this data.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "856a3fd3c0ecfccf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Check and print the number of duplicated entries based on 'code'\n",
    "print(f\"The dataset has {data['code'].duplicated().sum()} duplicated entries in the 'code' variable.\")\n",
    "\n",
    "# Remove duplicates based on 'code' first if it should be a unique identifier\n",
    "data.drop_duplicates(subset='code', inplace=True)\n",
    "\n",
    "# Now check and remove duplicates based on 'product_name' and 'brands' if needed\n",
    "print(f\"The dataset has {data[['product_name', 'brands']].duplicated().sum()} duplicated entries in the 'product_name' and 'brands' variables.\")\n",
    "data.drop_duplicates(subset=['product_name', 'brands'], inplace=True)\n",
    "\n",
    "# Final check for any type of duplicates\n",
    "print(f\"The dataset has {data.duplicated().sum()} duplicated entries after removing double entries on code, product_name and brands.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "54e818e0662ffe0a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# display all the column names with their index for reference and type of data\n",
    "data.info(verbose = True) # display information about the data, verbose mode on means all columns will be displayed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f38d83583c5ab596",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2 Removal of Irrelevant Variables - CE1\n",
    "*Code and explanation for the removal of variables that do not contribute to the analysis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c2af39ed0f622e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.1 Code for Removing Irrelevant Variables according to Nutri-Score Calculation (domain-specific)"
   ]
  },
  {
   "cell_type": "code",
   "id": "96dead7acc5029bf",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supposons que votre DataFrame s'appelle 'data'\n",
    "# data = pd.DataFrame({'nutrition_score_fr_100g': [liste de scores], 'nutrition_grade_fr': [liste de grades]})\n",
    "\n",
    "# Analyse 1: Table de Contingence\n",
    "contingency_table = pd.crosstab(data['nutrition_grade_fr'], data['nutrition_score_fr_100g'])\n",
    "print(\"Table de Contingence :\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Analyse 2: Statistiques Résumées par Grade\n",
    "grouped_data = data.groupby('nutrition_grade_fr')['nutrition_score_fr_100g'].describe()\n",
    "print(\"\\nStatistiques Résumées par Grade :\")\n",
    "print(grouped_data)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b383fa62b3079749",
   "metadata": {},
   "source": [
    "## Observations from Your Data:\n",
    "\n",
    "- Grade A: Spans from scores of -15 to a maximum observed value of 17, though typical values are much lower (up to -1).\n",
    "- Grade B: Typically covers scores from -10 to 2.\n",
    "- Grade C: Spans from 2 to 10.\n",
    "- Grade D: Encompasses scores from 6 to 18.\n",
    "- Grade E: Starts from scores as low as 10 and extends up to 40."
   ]
  },
  {
   "cell_type": "code",
   "id": "421624396ac3b5d4",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'data' is your DataFrame\n",
    "# Display distribution of 'nutrition_score_fr_100g'\n",
    "plt.figure(figsize=(20, 10))\n",
    "data['nutrition_score_fr_100g'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Distribution of Nutrition Score per 100g')\n",
    "plt.xlabel('Nutrition Score per 100g')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Display distribution of 'nutrition_grade_fr'\n",
    "plt.figure(figsize=(20, 10))\n",
    "data['nutrition_grade_fr'].value_counts().plot(kind='bar')\n",
    "plt.title('Distribution of Nutrition Grades')\n",
    "plt.xlabel('Nutrition Grade')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Logic to summarize nutrition score to grade\n",
    "def score_to_grade(score):\n",
    "    # Adjusted based on detailed data analysis\n",
    "    if score <= -1:\n",
    "        return 'A'  # From very low scores up to -1\n",
    "    elif score <= 2:\n",
    "        return 'B'  # Covers slightly negative to 2\n",
    "    elif score <= 10:\n",
    "        return 'C'  # From 3 to 10\n",
    "    elif score <= 18:\n",
    "        return 'D'  # From 11 to 18\n",
    "    else:\n",
    "        return 'E'  # Above 18\n",
    "\n",
    "# Example of how to display unique outcomes without altering DataFrame\n",
    "unique_scores = data['nutrition_score_fr_100g'].unique()  # Extract unique scores\n",
    "unique_scores.sort()  # Sort the scores for better visibility\n",
    "\n",
    "# Map each unique score to its grade\n",
    "predicted_grades = [score_to_grade(score) for score in unique_scores]\n",
    "\n",
    "# Create and display a DataFrame for showing these unique mappings\n",
    "predicted_outcomes = pd.DataFrame({\n",
    "    'nutrition_score_fr_100g': unique_scores,\n",
    "    'predicted_grade': predicted_grades\n",
    "})\n",
    "print(predicted_outcomes.sort_values('nutrition_score_fr_100g'))\n",
    "\n",
    "# Explanation of grades based on score ranges\n",
    "print(\"\\nNutrition Grade Interpretation:\")\n",
    "print(\"Grade A: Nutrition score ≤ -1\")\n",
    "print(\"Grade B: Nutrition score between 0 and 2\")\n",
    "print(\"Grade C: Nutrition score between 3 and 10\")\n",
    "print(\"Grade D: Nutrition score between 11 and 18\")\n",
    "print(\"Grade E: Nutrition score > 18\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cb86b8640acd333d",
   "metadata": {},
   "source": [
    "Observation: The Nutri-Score calculation is based on the 'nutrition_score_fr_100g' variable, which is then mapped to the 'nutrition_grade_fr' variable. The score ranges for each grade are as follows: \n",
    "- Grade A: ≤ -1\n",
    "- Grade B: 0 to 2\n",
    "- Grade C: 3 to 10\n",
    "- Grade D: 11 to 18\n",
    "- Grade E: > 18\n",
    "- Problem :\n",
    "- Reevaluation of the Data:\n",
    "    Nutrition Scores Distribution (First Image): The histogram shows a dominant peak at score 0, which would imply that many products fall within this score.\n",
    "    Nutrition Grades Distribution (Second Image): Contrary to what would typically be expected, grade B appears to be the least frequent, despite the large number of products that have a score of 0.  \n",
    "- The discrepancy between the expected distribution of grades and the actual distribution of scores suggests that the current data may not accurately reflect the Nutri-Score calculation criteria. This discrepancy could be due to missing or incorrect data in the variables used for the Nutri-Score calculation. Further analysis is required to identify and address these issues.\n",
    "- Understanding Nutri-Score Discrepancies Across Food Categories\n",
    "\n",
    "The Nutri-Score system assigns a letter grade from A to E based on the nutritional quality of food and beverages, which can help explain discrepancies in grade distributions observed in the data. Here's a breakdown of how the scoring varies across different food categories:\n",
    "\n",
    "- Solid Foods:\n",
    "        Score 0 to Minimum: Grade A\n",
    "        Score 1 to 2: Grade B\n",
    "        Score 3 to 10: Grade C\n",
    "        Score 11 to 18: Grade D\n",
    "        Score 19 to Maximum: Grade E\n",
    "\n",
    "- Fats (Animal and Vegetable Fats, Nuts, Seeds):\n",
    "        Score -6 to Minimum: Grade A\n",
    "        Score -5 to 2: Grade B\n",
    "        Score 3 to 10: Grade C\n",
    "        Score 11 to 18: Grade D\n",
    "        Score 19 to Maximum: Grade E\n",
    "\n",
    "- Beverages:\n",
    "        Water: Grade A\n",
    "        Minimum to 2: Grade B for other beverages\n",
    "        Score 3 to 6: Grade C for non-water beverages\n",
    "        Score 7 to 9: Grade D for non-water beverages\n",
    "        Score 10 to Maximum: Grade E for non-water beverages\n",
    "\n",
    "-Key Points:\n",
    "\n",
    "Category-Specific Scoring: Nutri-Score ranges are specifically adapted to different types of foods and beverages, reflecting their unique nutritional characteristics.\n",
    "Impact on Grade Distribution: These category-specific scores might lead to uneven distributions of Nutri-Score grades across the dataset. For example, solid foods and fats have a lower threshold for achieving a higher grade compared to beverages. Understanding Discrepancies:     The observed discrepancies in grade distributions, especially the prevalence of certain grades, could be significantly influenced by the distribution of product types (solids, fats, or beverages) within the dataset.\n",
    "\n",
    "This categorization clearly illustrates that the distribution of Nutri-Score grades is heavily influenced by the type of product being scored, which is crucial for interpreting any aggregate data analysis involving Nutri-Scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66145080af1601cc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Nutri-Score Calculation Analysis\n",
    "\n",
    "To calculate the Nutri-Score of food products in France, we focus on a specific set of essential nutritional variables as per the scoring system. The Nutri-Score is calculated based on certain nutritional components of the foods, with points being assigned based on defined thresholds. Below are the main variables used for the Nutri-Score calculation:\n",
    "\n",
    "1. **Energy (kJ or kcal)** - directly affects the score.\n",
    "2. **Sugars** - amount of total sugars present in the food.\n",
    "3. **Saturated Fats** - amount of saturated fats present in the food.\n",
    "4. **Sodium** - amount of sodium, which is calculated from salt (salt is converted into sodium).\n",
    "5. **Dietary Fiber** - fiber content of the food.\n",
    "6. **Proteins** - protein content of the food.\n",
    "7. **Fruits, Vegetables, Nuts, and Rapeseed/Walnut/Olive Oils** - percentage of these ingredients in the food.\n",
    "\n",
    "Based on the description of the data structure, here is how we can map them in the DataFrame:\n",
    "\n",
    "- `energy_100g` : Energy expressed in kilojoules or kilocalories.\n",
    "- `sugars_100g` : Total sugars amount.\n",
    "- `saturated_fat_100g` : Amount of saturated fats.\n",
    "- `sodium_100g` : Amount of sodium.\n",
    "- `fiber_100g` : Fiber content.\n",
    "- `proteins_100g` : Protein content.\n",
    "- `fruits_vegetables_nuts_100g` : Estimated percentage of fruits, vegetables, and nuts.\n",
    "\n",
    "From your comprehensive description of the DataFrame columns, the following columns are thus essential for the Nutri-Score calculation and should be retained:\n",
    "\n",
    "- `energy_100g`\n",
    "- `sugars_100g`\n",
    "- `saturated_fat_100g`\n",
    "- `sodium_100g`\n",
    "- `fiber_100g`\n",
    "- `proteins_100g`\n",
    "- `fruits_vegetables_nuts_100g`\n",
    "\n",
    "All other columns could be removed if your goal is solely to calculate the Nutri-Score and to streamline your DataFrame for analysis specific to this calculation. However, it's important to consider that some of this information might be useful for other analyses or verifications, such as information on allergens, countries of origin, or other nutritional components that could be relevant for more detailed dietary assessments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fa8694d49ee333",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "![Nutriscore Calculation 1](calculation_nutriscore_1.jpg)\n",
    "![Nutriscore Calculation 2](calculation_nutriscore_2.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d472a983893d06f6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# list of columns to keep for Nutri-Score calculation\n",
    "nutri_score_columns = ['nutrition_grade_fr', 'nutrition_score_fr_100g','energy_100g', 'sugars_100g', 'saturated_fat_100g', 'sodium_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g']\n",
    "# drop irrelevant columns\n",
    "data_cleaned = data[nutri_score_columns]\n",
    "# display the first few rows of the cleaned data\n",
    "data_cleaned.head(20)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f7de24af93080476",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "data_cleaned.shape # display the shape of the cleaned data  (rows, columns)\n",
    "print( \"number of products after removing duplicates and irrelevant columns: \", data_cleaned.shape[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b84a8b4919d34e76",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.2 Code for Removing Irrelevant Variables according to usual Data Cleaning (non-domain-specific)"
   ]
  },
  {
   "cell_type": "code",
   "id": "7bb9a1488cb0129b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "data.describe() # display summary statistics of the data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b83457146b550f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.3 Conduct a Detailed Univariate Analysis of Each Variable in the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eeada2cb3e7bd0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 2.3.1 Statistical Indicators for Quantitative Variables"
   ]
  },
  {
   "cell_type": "code",
   "id": "5d9965bf8b0766b7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Display summary statistics of the quantitative variables only\n",
    "quantitative_data = data.select_dtypes(include=['number'])  # Select only numeric columns\n",
    "summary_statistics = quantitative_data.describe()  # Generate descriptive statistics\n",
    "summary_statistics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3432b63b35cbe8a7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# number of modalities for qualitative variables by descending order\n",
    "data.describe(include='object').T.sort_values('unique', ascending=False)['unique']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "16ab583c65c12ab0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 2.3.2 Statistical Indicators for Qualitative Variables"
   ]
  },
  {
   "cell_type": "code",
   "id": "59f772eb977e0bfd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Display summary statistics of the qualitative variables only\n",
    "qualitative_data = data.select_dtypes(include=['object'])  # Select only object columns\n",
    "summary_statistics_qualitative = qualitative_data.describe()  # Generate descriptive statistics\n",
    "summary_statistics_qualitative"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4fa3f8d1664d1193",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "id": "930d4327c8914099",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "# Modalities list for qualitative variables\n",
    "for col in data.select_dtypes(include='object').columns:\n",
    "    unique_modalities = data[col].unique()\n",
    "    # Uncomment the following line to display all unique modalities\n",
    "    # unique_modalities = data[col].unique()\n",
    "    if len(unique_modalities) > 5:\n",
    "        unique_modalities = np.append(unique_modalities[:5], 'all')\n",
    "    print(f'{col} : {unique_modalities}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "524498ca88f550ff",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.3.4 Modalities count for qualitative variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ef1dffcbe35e0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "id": "6476156af1e56fc4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Modalities count for qualitative variables\n",
    "for col in data.select_dtypes(include='object').columns:\n",
    "    print(f'{col} modalities count:')\n",
    "    print(data[col].value_counts())\n",
    "    print('\\n')  # Adds a newline for better readability between columns\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aefd338e5b694e1d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3 Missing Values Treatment Strategies - CE2\n",
    "*Overview of strategies used to handle missing data.*\n",
    "\n",
    "- Step 1: Infer variables with missing values that are necessary for the calculation of nutrition_score_fr_100g (Nutri-Score).\n",
    "- Step 2: Infer target variable (nutrition_score_fr_100g) only for those with missing values (i.e.nutrition_score_fr_100g) using imputation techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5674acf2701a69c5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 3.1 Analyse products that do not have nutrition_score_fr_100g (Nutri-Score) - we will infere the missing values in a second step for the Nutri-Score only for the nutrition_score_fr_100g that have NaN"
   ]
  },
  {
   "cell_type": "code",
   "id": "f81b0a90ecf72608",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Check for missing values in Nutri-Score related columns when 'nutrition_score_fr_100g' is missing\n",
    "missing_nutri_score = data[data['nutrition_score_fr_100g'].isnull()]\n",
    "\n",
    "# Calculate the sum of missing values in the specified Nutri-Score related columns\n",
    "missing_values_count = missing_nutri_score[nutri_score_columns].isnull().sum()\n",
    "\n",
    "# Print the number of missing values\n",
    "print(f\"Number of missing values in Nutri-Score related columns when 'nutrition_score_fr_100g' is missing: {missing_values_count}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dbd8d5bd82a7f257",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your original DataFrame\n",
    "# List of all qualitative and quantitative variables that must be present\n",
    "required_columns = [\n",
    "    'categories_fr', 'pnns_groups_1', 'pnns_groups_2', 'main_category_fr',\n",
    "    'nutrition_grade_fr', 'nutrition_score_fr_100g', 'energy_100g', 'sugars_100g',\n",
    "    'saturated_fat_100g', 'sodium_100g', 'fiber_100g', 'proteins_100g', \n",
    "    'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Creating a copy of the DataFrame where all specified columns are not null\n",
    "complete_data = data.dropna(subset=required_columns)\n",
    "\n",
    "# Displaying the shape of the new DataFrame to verify the number of complete cases\n",
    "print(\"Shape of the DataFrame with all required columns present:\", complete_data.shape)\n",
    "\n",
    "# Shape of the DataFrame with all required columns present: (new_rows, new_columns)\n",
    "complete_data.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "47d7e932f4653ac5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Suppose df is your DataFrame\n",
    "# You can use the following code to find both the number of unique values and the total count of entities for the column 'nutrition_score_fr_100g'\n",
    "\n",
    "modalities_count = data['nutrition_score_fr_100g'].value_counts()\n",
    "num_modalities = len(modalities_count)\n",
    "total_entities = data['nutrition_score_fr_100g'].count()\n",
    "\n",
    "print(\"Number of unique modalities for 'nutrition_score_fr_100g':\", num_modalities)\n",
    "print(\"Total count of entities for 'nutrition_score_fr_100g':\", total_entities)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce4424a67a671104",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "data.shape # display the shape of the data  (rows, columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "167faa4854a293e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# display the count of missing values in each column descending order\n",
    "data.isnull().sum().sort_values(ascending=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "72ba245a6d30022",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 3.2 Analyze the extent and patterns of missing data across different variables."
   ]
  },
  {
   "cell_type": "code",
   "id": "b7f6efd324205185",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# display missing values in a matrix form (Entities x Variables)\n",
    "plt.figure(figsize=(20, 10)) # set the figure size\n",
    "plt.title('Missing Values Matrix (Entities x Variables)') # set the title of the plot\n",
    "sns.heatmap(data.isna(), cbar=False) # plot the heatmap of missing values, cbar=False to remove color bar, data.isna() to check missing values \n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9db3e124afa9b739",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# display the percentage of missing values in each column and the number of missing values\n",
    "missing_values = data.isnull().sum() # count the missing values\n",
    "missing_values = missing_values[missing_values > 0] # select columns with missing values\n",
    "missing_values_percentage = missing_values / len(data) * 100 # calculate the percentage of missing values\n",
    "missing_values_df = pd.DataFrame({'Missing Values': missing_values, 'Percentage': missing_values_percentage}) # create a DataFrame\n",
    "missing_values_df.sort_values(by='Missing Values', ascending=False) # sort the DataFrame by the number of missing values\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46984ff5077d25a3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# display the percentage of missing values \n",
    "plt.figure(figsize=(20, 30)) # set the figure size\n",
    "plt.title('Percentage of Missing Values in Columns') # set the title of the plot\n",
    "missing_values_percentage.sort_values().plot(kind='barh') # plot the horizontal bar chart of missing values percentage\n",
    "plt.xlabel('Percentage of Missing Values') # set the label for x-axis\n",
    "plt.ylabel('Columns') # set the label for y-axis\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dbeef7da8e407e21",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 3.3 Reminder of the columns to keep for Nutri-Score calculation: 'nutrition_score_fr_100g ','energy_100g', 'sugars_100g', 'saturated_fat_100g', 'sodium_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g'"
   ]
  },
  {
   "cell_type": "code",
   "id": "c84e740c30d4c7ee",
   "metadata": {},
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# # Assurez-vous que missing_values_percentage est un DataFrame avec les bonnes colonnes\n",
    "# # Par exemple:\n",
    "# # missing_values_percentage = pd.DataFrame({\n",
    "# #     'column': ['nutrition_grade_fr', 'nutrition_score_fr_100g', 'energy_100g', 'sugars_100g', \n",
    "# #                'saturated_fat_100g', 'sodium_100g', 'fiber_100g', 'proteins_100g', \n",
    "# #                'fruits_vegetables_nuts_100g', 'code', 'product_name', 'brands'],\n",
    "# #     'missing_percentage': [25, 15, 10, 5, 20, 0, 30, 45, 50, 1, 2, 3]\n",
    "# # }).set_index('column')\n",
    "# \n",
    "# # Définissez les colonnes à afficher\n",
    "# columns_to_display = ['code', 'product_name', 'brands', 'nutrition_grade_fr', 'nutrition_score_fr_100g',\n",
    "#                       'energy_100g', 'sugars_100g', 'saturated_fat_100g', 'sodium_100g', 'fiber_100g', \n",
    "#                       'proteins_100g', 'fruits_vegetables_nuts_100g']\n",
    "# \n",
    "# # Filtrer pour obtenir seulement les pourcentages de valeurs manquantes pour les colonnes spécifiées\n",
    "# missing_values_percentage_filtered = missing_values_percentage.loc[columns_to_display]\n",
    "# \n",
    "# # Trier les valeurs\n",
    "# missing_values_percentage_sorted = missing_values_percentage_filtered.sort_values()\n",
    "# \n",
    "# # Créer la figure\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# plt.title('Percentage of Missing Values in Columns')\n",
    "# \n",
    "# # Tracer les barres\n",
    "# bars = plt.barh(missing_values_percentage_sorted.index, missing_values_percentage_sorted.values, color='blue')\n",
    "# \n",
    "# # Colorer certaines barres en rouge\n",
    "# nutri_score_columns = set(['nutrition_grade_fr', 'nutrition_score_fr_100g', 'energy_100g', 'sugars_100g', \n",
    "#                            'saturated_fat_100g', 'sodium_100g', 'fiber_100g', 'proteins_100g', \n",
    "#                            'fruits_vegetables_nuts_100g'])\n",
    "# for bar, label in zip(bars, missing_values_percentage_sorted.index):\n",
    "#     if label in nutri_score_columns:\n",
    "#         bar.set_color('red')\n",
    "# \n",
    "# # Ajouter les étiquettes des axes\n",
    "# plt.xlabel('Percentage of Missing Values')\n",
    "# plt.ylabel('Columns')\n",
    "# \n",
    "# # Afficher le graphique\n",
    "# plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41c86de221d008da",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# columns to display in the plot\n",
    "nutri_score_columns = ['nutrition_grade_fr', 'nutrition_score_fr_100g','energy_100g', 'sugars_100g', 'saturated_fat_100g', 'sodium_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g']\n",
    "columns_to_display = ['code', 'product_name', 'brands'] + nutri_score_columns\n",
    "\n",
    "# Filter missing values percentage for the specified columns\n",
    "missing_values_percentage_filtered = missing_values_percentage[missing_values_percentage.index.isin(columns_to_display)]\n",
    "\n",
    "# Sort the percentages of missing values from lowest to highest\n",
    "missing_values_percentage_sorted = missing_values_percentage_filtered.sort_values()\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Title of the plot\n",
    "plt.title('Percentage of Missing Values in Columns')\n",
    "\n",
    "# Plot the percentages of missing values\n",
    "bars = missing_values_percentage_sorted.plot(kind='barh', color='red') # horizontal bar chart with blue bars\n",
    "\n",
    "# Highlight columns from nutri_score_columns in red\n",
    "for bar in bars.patches: # iterate over the bars\n",
    "    if missing_values_percentage_sorted.index[bar.get_x()] in nutri_score_columns: # check if the column is in nutri_score_columns\n",
    "        bar.set_color('red') # set the color of the bar to red if it's in nutri_score_columns\n",
    "\n",
    "# Label for the x-axis\n",
    "plt.xlabel('Percentage of Values') \n",
    "\n",
    "# Label for the y-axis\n",
    "plt.ylabel('Columns')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "55990c469c0fdda9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# display the count of missing values in each row of the variables to keep for Nutri-Score calculation\n",
    "modalities_count = data['nutrition_score_fr_100g'].value_counts()\n",
    "num_modalities = len(modalities_count)\n",
    "total_entities = data['nutrition_score_fr_100g'].count()\n",
    "\n",
    "print(\"Number of unique modalities for 'nutrition_score_fr_100g':\", num_modalities)\n",
    "print(\"Total count of entities for 'nutrition_score_fr_100g':\", total_entities)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2a7d82af6513558",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 3.3.1 Analysis of Missing Data for Nutri-Score Calculation Variables"
   ]
  },
  {
   "cell_type": "code",
   "id": "ef2731cdd1709e9e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Assuming 'data' is your DataFrame containing all your data.\n",
    "# nutri_score_columns is the list of columns necessary for Nutri-Score calculation, excluding 'nutrition_score_fr_100g' since it is used as a base.\n",
    "\n",
    "nutri_score_columns = [ \n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g', \n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Filter the data to include only rows where 'nutrition_score_fr_100g' is not null\n",
    "data_with_nutrition_grade = data[data['nutrition_score_fr_100g'].notna()]\n",
    "\n",
    "# Display the count of missing values in each of the other Nutri-Score related columns\n",
    "missing_counts = data_with_nutrition_grade[nutri_score_columns].isna().sum().sort_values(ascending=False)\n",
    "\n",
    "# Print the results\n",
    "print(\"Missing values count for each Nutri-Score variable when 'nutrition_score_fr_100g' is present:\")\n",
    "print(missing_counts)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5ea93a61ea5b0487",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 3.3.2 Analyse Data Completeness for Nutri-Score Calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "70e417a28d7fc0f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Assuming 'data' is your DataFrame containing all your data.\n",
    "# nutri_score_columns is the list of columns necessary for Nutri-Score calculation.\n",
    "\n",
    "nutri_score_columns = [\n",
    "    'energy_100g', 'sugars_100g', \n",
    "    'saturated_fat_100g', 'sodium_100g', 'fiber_100g', \n",
    "    'proteins_100g', 'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Check the completeness of data for the Nutri-Score calculation.\n",
    "# This will give us the number of instances in the dataframe where none of the values are NaN.\n",
    "\n",
    "complete_cases_count = data.dropna(subset=nutri_score_columns).shape[0]\n",
    "\n",
    "# Print the result.\n",
    "print(f\"Number of complete instances for Nutri-Score calculation: {complete_cases_count}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "add4b6d2e2c3c201",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Observations\n",
    "\n",
    "The bar chart provides a visual representation of the percentage of missing values in various columns of a dataset related to the calculation of the Nutri-Score for food products. Below are key observations and conclusions drawn from the chart:\n",
    "\n",
    "- **High Missing Data in Fruits, Vegetables, and Nuts**: There's an approximately 80% missing data in the `fruits_vegetables_nuts_100g` column. This significant gap indicates that the Nutri-Score, which benefits from a higher proportion of these ingredients, may often be underestimated or incalculable.\n",
    "\n",
    "- **Substantial Missing Fiber Data**: About 25% of the data is missing in the `fiber_100g` column. As fiber contributes positively to the Nutri-Score, the lack of this data could also lead to an underestimation of the product's healthfulness.\n",
    "\n",
    "- **Low Missing Data for Core Nutritional Values**: The columns `sodium_100g`, `proteins_100g`, `sugars_100g`, `saturated_fat_100g`, and `energy_100g` show relatively low percentages of missing values. This is encouraging for the Nutri-Score calculation as it suggests that the essential nutritional data is mostly intact.\n",
    "\n",
    "- **Minor Missing Data for Product Identification**: The `brands` and `product_name` columns exhibit some missing values, which, while not directly affecting the Nutri-Score, are crucial for product identification and matching with nutritional information.\n",
    "\n",
    "### Conclusions:\n",
    "\n",
    "- **Data Completeness is Key**: The accuracy of the Nutri-Score is highly dependent on the completeness of data. Particular attention should be paid to the `fruits_vegetables_nuts_100g` and `fiber_100g` columns, which may require data supplementation or imputation strategies.\n",
    "\n",
    "- **Potential Underestimation of Nutri-Scores**: Given the missing data in positive scoring components like fruits, vegetables, nuts, and fiber, the Nutri-Score calculated from this dataset may often be lower than the actual value.\n",
    "\n",
    "- **Importance of Data Cleaning**: Prior to Nutri-Score calculation, there is a clear need for data cleaning processes to address the missing values, which could involve data imputation or exclusion of certain entries.\n",
    "\n",
    "The chart underscores the necessity for complete and reliable data to ensure the accuracy of Nutri-Score calculations and subsequent health assessments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe58b0c607c946",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 3.3.3 Display missing values in a matrix form (Entities x Variables) for all columns"
   ]
  },
  {
   "cell_type": "code",
   "id": "1be7393add4adc4a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Display missing values in a matrix form (Entities x Variables)\n",
    "plt.figure(figsize=(20, 10)) # set the figure size\n",
    "plt.title('Missing Values Matrix (Entities x Variables)') # set the title of the plot\n",
    "sns.heatmap(data.isna(), cbar=False) # plot the heatmap of missing values, cbar=False to remove color bar\n",
    "plt.show()\n",
    "\n",
    "# Calculate the percentage of missing values for each feature\n",
    "missing_percentage = data.isna().mean() * 100\n",
    "\n",
    "# Filter the features that have less than 50% missing values\n",
    "features_to_consider = missing_percentage[missing_percentage < 50].index.tolist()\n",
    "\n",
    "# Print the features to consider for further analysis\n",
    "print(f\"Features considered for the prediction model (less than 50% missing values):\\n{features_to_consider}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8efd06f6b312c1f8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 3.3.4 Features  potentially to considered for the prediction model (with less than 50% missing values) and Nutri-Score features used in the calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88588eb2b08f665e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Feature Selection for Nutri-Score Calculation and Beyond\n",
    "\n",
    "This documentation outlines the selected features for the predictive model of the Nutri-Score and discusses the rationale behind retaining additional dataset features. The primary goal is to ensure the Nutri-Score is calculated accurately, hence the inclusion of essential features with less than 50% missing values. Beyond the fundamental Nutri-Score calculation, other features are preserved for their potential to yield valuable insights in further analytical exploration.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "4619cf0f3a42897a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Features considered for the prediction model (with less than 50% missing values)\n",
    "features_considered = [\n",
    "    'code', 'url', 'creator', 'created_t', 'created_datetime', \n",
    "    'last_modified_t', 'last_modified_datetime', 'product_name', 'brands', 'brands_tags', \n",
    "    'countries', 'countries_tags', 'countries_fr', 'ingredients_text', 'serving_size', \n",
    "    'additives_n', 'additives', 'additives_tags', 'additives_fr', \n",
    "    'ingredients_from_palm_oil_n', 'ingredients_that_may_be_from_palm_oil_n', \n",
    "    'nutrition_grade_fr', 'states', 'states_tags', 'states_fr', 'energy_100g', \n",
    "    'fat_100g', 'saturated_fat_100g', 'trans_fat_100g', 'cholesterol_100g', \n",
    "    'carbohydrates_100g', 'sugars_100g', 'fiber_100g', 'proteins_100g', \n",
    "    'salt_100g', 'sodium_100g', 'vitamin_a_100g', 'vitamin_c_100g', \n",
    "    'calcium_100g', 'iron_100g', 'nutrition_score_fr_100g', 'nutrition_score_uk_100g'\n",
    "]\n",
    "\n",
    "# Features used in Nutri-Score calculation\n",
    "nutriscore_features = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g', 'sodium_100g', \n",
    "    'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Check if the features considered are part of the Nutri-Score calculation\n",
    "nutriscore_related = [feature for feature in features_considered if feature in nutriscore_features]\n",
    "\n",
    "# Create a list of remaining features to consider if they should be kept\n",
    "remaining_features = [feature for feature in features_considered if feature not in nutriscore_features]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Features related to Nutri-Score calculation in features considered:\\n{nutriscore_related}\")\n",
    "print(f\"\\nFeatures to reconsider keeping:\\n{remaining_features}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b1763a01f79bf4a2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 4.Handling Outliers - CE3\n",
    "   - Graphical methods and corresponding code for outlier detection.\n",
    "   - Business analysis of outlier values for Nutri-Score variables.\n",
    "   - Detect outliers using statistical methods or visualizations.\n",
    "   - Determine whether outliers are due to data entry errors or are genuine extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc8c535b1328a8c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 4.1 Visualizations for Outlier Detection in the domain of Nutri-Score Calculation"
   ]
  },
  {
   "cell_type": "code",
   "id": "133e0e69f81db815",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Display box plots for the variables to keep for Nutri-Score calculation\n",
    "plt.figure(figsize=(20, 10)) # set the figure size\n",
    "plt.title('Box Plots for Variables Related to Nutri-Score Calculation') # set the title of the plot\n",
    "sns.boxplot(data=data[nutri_score_columns]) # plot the box plots for the variables related to Nutri-Score calculation\n",
    "plt.xticks(rotation=45) # rotate the x-axis labels for better visibility\n",
    "plt.show() # show the plot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8a6b0d2ffce1ca77",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Display the distribution of the variables using histograms\n",
    "data[nutri_score_columns].hist(figsize=(20, 15)) # plot histograms for the variables related to Nutri-Score calculation\n",
    "plt.suptitle('Distribution of Variables Related to Nutri-Score Calculation', y=1.02) # set the title of the plot\n",
    "plt.show() # show the plot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "837f403d3ae6fba2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 4.1.1 Business Analysis of Outlier Values for Nutri-Score Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dd0f5a34c973d1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Business Study: Based on internet research and industry standards:\n",
    "- **Energy (energy_100g)**: \n",
    "  - The most calorific source, fats, contain about 3700 kJ per 100g. \n",
    "  - **Outliers**: Values exceeding this threshold can be considered outliers.\n",
    "\n",
    "- **Sugars (sugars_100g)**:\n",
    "  - A product cannot contain more than 100g of sugars per 100g (theoretically possible but highly improbable, except for pure sugar products).\n",
    "  - **Outliers**: Values exceeding 100g per 100g.\n",
    "\n",
    "- **Saturated Fat (saturated_fat_100g)**:\n",
    "  - It is rare for saturated fats to exceed 100g per 100g, even in very fatty foods.\n",
    "  - **Outliers**: Values exceeding 100g per 100g.\n",
    "\n",
    "- **Sodium (sodium_100g)**:\n",
    "  - Very high concentrations of sodium are unusual. WHO recommendations suggest less than 2g of sodium per day, which is approximately 5g of salt.\n",
    "  - **Outliers**: Values exceeding 5g per 100g (extremely rare and potentially unviable).\n",
    "\n",
    "- **Fiber (fiber_100g)**:\n",
    "  - The fiber content logically cannot exceed 100g per 100g.\n",
    "  - **Outliers**: Values exceeding 100g per 100g.\n",
    "\n",
    "- **Proteins (proteins_100g)**:\n",
    "  - Similar to other macronutrients, the protein content cannot exceed 100g per 100g.\n",
    "  - **Outliers**: Values exceeding 100g per 100g.\n",
    "\n",
    "- **Fruits, Vegetables, Nuts (fruits_vegetables_nuts_100g)**:\n",
    "  - For mixes or processed products, it is rare for this value to exceed 100%.\n",
    "  - **Outliers**: Values exceeding 100%, which would indicate a data entry or calculation error.\n",
    "\n",
    "#### Recommendations:\n",
    "- Values identified as outliers should be individually verified and corrected or excluded from the analysis depending on the context.\n",
    "- Collaboration with nutrition experts is recommended to validate these thresholds and understand their impact on Nutri-Score analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba9108c7b1873f4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 4.1.2 Cleaning Outliers in Nutri-Score Variables from a Business Perspective"
   ]
  },
  {
   "cell_type": "code",
   "id": "cbaff1b48c122fcd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Optionally, check the data types to confirm all are numeric\n",
    "print(data[nutri_score_columns].dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2d95d2d6eb2db2ce",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 4.1.3 Box Plot Visualization for Outlier Detection for energy_100g since it has a significantly higher range than other variables    "
   ]
  },
  {
   "cell_type": "code",
   "id": "69ed78c6eda37631",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## boxplot for energy_100g\n",
    "data['energy_100g'].plot.box(figsize=(10, 5))\n",
    "plt.title('Box Plot for Energy (kJ) before business cleaning per 100g')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "90e533629dc0cf9f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 4.1.4 Definition of Maximum Thresholds for Nutri-Score Variables According to Business Study"
   ]
  },
  {
   "cell_type": "code",
   "id": "82dfaa6623e2545d",
   "metadata": {},
   "source": [
    "# Define maximum thresholds and minimum threshold of 0 for each variable according to business study\n",
    "thresholds = {\n",
    "    'energy_100g': 3700,  # max energy from fats in kJ\n",
    "    'sugars_100g': 100,   # max sugar content in g\n",
    "    'saturated_fat_100g': 100,  # max saturated fat in g\n",
    "    'sodium_100g': 5,     # equivalent to about 12.5g of salt\n",
    "    'fiber_100g': 100,    # max fiber content in g\n",
    "    'proteins_100g': 100, # max protein content in g\n",
    "    'fruits_vegetables_nuts_100g': 100  # max percentage for fruits, veggies, nuts\n",
    "}\n",
    "\n",
    "# Function to clean outliers with both upper and lower bounds\n",
    "def clean_outliers(df, column, upper_threshold):\n",
    "    df[column] = df[column].clip(lower=0, upper=upper_threshold)  # Using clip to set both min (0) and max thresholds\n",
    "    return df\n",
    "\n",
    "# Apply cleaning of outliers for each specified column\n",
    "for column, upper_threshold in thresholds.items():\n",
    "    data = clean_outliers(data, column, upper_threshold)\n",
    "\n",
    "# Print min, max, and mean values for each column after cleaning\n",
    "for column in thresholds.keys():\n",
    "    min_val = data[column].min()\n",
    "    max_val = data[column].max()\n",
    "    mean_val = data[column].mean()\n",
    "    print(f\"{column}: Min={min_val}, Max={max_val}, Mean={mean_val:.2f}\")\n",
    "\n",
    "# Define the path for saving the cleaned data in the 'data' subfolder\n",
    "save_path = os.path.join(base_path, 'data/')  # Ensure this path points to the data directory\n",
    "\n",
    "# Save the cleaned data to a new CSV file in the 'data' subfolder\n",
    "output_file_path = os.path.join(save_path, 'cleaned_data_from_business_outlier.csv')\n",
    "data.to_csv(output_file_path, index=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Définir le chemin de base\n",
    "base_path = 'D:\\\\OC_IA\\\\P3\\\\OC_IA_P3_Prepare_data_for_a_public_health_organization'\n",
    "\n",
    "# Path to the data file\n",
    "data_path = os.path.join(base_path, 'data', 'cleaned_data_from_business_outlier.csv')\n",
    "\n",
    "# Vérifier si le fichier existe\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"The file {data_path} does not exist.\")\n",
    "\n",
    "# Load data using the defined path (assuming comma as the separator)\n",
    "data = pd.read_csv(data_path, sep=',', low_memory=False, on_bad_lines='skip')\n",
    "\n",
    "# Replace hyphens with underscores and strip whitespace in column names\n",
    "data.columns = data.columns.str.replace('-', '_').str.strip()\n",
    "\n",
    "# Display all column names to verify\n",
    "print(\"Column names in the DataFrame:\", data.columns.tolist())\n",
    "\n",
    "# List of qualitative variables that must be completely non-null\n",
    "required_non_null_columns = ['pnns_groups_2']\n",
    "\n",
    "# List of quantitative variables\n",
    "quantitative_columns = [\n",
    "    'nutrition_score_fr_100g', 'energy_100g', 'sugars_100g', 'saturated_fat_100g', \n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Exclure les lignes où 'pnns_groups_2' est 'unknown'\n",
    "data = data[data['pnns_groups_2'].str.lower().str.strip() != 'unknown']\n",
    "\n",
    "# Check if all required columns are present in the DataFrame\n",
    "missing_columns = [col for col in required_non_null_columns if col not in data.columns]\n",
    "if missing_columns:\n",
    "    raise KeyError(f\"Missing columns in the DataFrame: {missing_columns}\")\n",
    "\n",
    "# Filter out rows where 'pnns_groups_2' is NaN\n",
    "base_data_for_method = data.dropna(subset=required_non_null_columns).copy()\n",
    "\n",
    "# Replace NaNs and empty strings in quantitative variables with zeros\n",
    "for col in quantitative_columns:\n",
    "    base_data_for_method[col] = base_data_for_method[col].replace('', 0).fillna(0)\n",
    "\n",
    "# Displaying the shape of the new DataFrame to verify the number of complete cases\n",
    "print(\"Shape of the new DataFrame with selected columns:\", base_data_for_method.shape)\n",
    "\n",
    "# Path to save the cleaned DataFrame\n",
    "output_path = os.path.join(base_path, 'data', 'base_data_for_method.csv')\n",
    "\n",
    "# Saving the cleaned DataFrame to the specified path\n",
    "base_data_for_method.to_csv(output_path, index=False)\n",
    "print(f'DataFrame saved to {output_path}')\n",
    "\n",
    "\n"
   ],
   "id": "1e744fac7633d8a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "acee3d49a07554de",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Automation of Data Cleaning Process - CE5\n",
    "\n",
    "In the project section titled \"Definition of Maximum Thresholds for Nutri-Score Variables According to Business Study,\" we have implemented an automation process using appropriate functions and methods to clean outliers in the dataset. \n",
    "\n",
    "The provided code defines maximum thresholds for Nutri-Score variables based on a business study and applies a cleaning function to remove outliers beyond these thresholds. \n",
    "\n",
    "### Key Steps:\n",
    "- **Threshold Definition**: Maximum thresholds are defined for Nutri-Score variables such as energy content, sugar content, saturated fat, sodium, fiber, proteins, and percentage of fruits, vegetables, and nuts.\n",
    "- **Cleaning Function**: A custom function is created to clean outliers in each specified column using upper bounds defined by the thresholds.\n",
    "- **Automation**: The cleaning function is applied iteratively to each column in the dataset, ensuring consistency and efficiency in outlier removal.\n",
    "- **Result Validation**: After cleaning, summary statistics including minimum, maximum, and mean values are printed for each column to verify the effectiveness of the cleaning process.\n",
    "- **Data Export**: The cleaned dataset is saved to a new CSV file for further analysis and use in the project.\n",
    "\n",
    "This automation of the data cleaning process demonstrates the application of functions and methods to efficiently handle data preprocessing tasks, ensuring data quality and consistency in accordance with project requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3676f24bedbd8b7a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#Display data.shape in data.to_csv('../data/cleaned_data_from_business_outlier.csv', index=False)\n",
    "data.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6da0d1721eb959b9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 4.1.5 Visualizations for Outlier Detection in Nutri-Score Variables after Business Cleaning -The 'energy_100g' column is omitted from the nutri_score_columns list. This adjustment ensures better visualization since 'energy_100g' has a significantly higher range than other variables, which can distort the box plot visualization."
   ]
  },
  {
   "cell_type": "code",
   "id": "d02c83435fb5ed1b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### boxplot for energy_100g after business cleaning\n",
    "plt.figure(figsize=(10, 5))  # Set figure size before plotting\n",
    "sns.boxplot(x=data['energy_100g'], width=0.5)  # Plot boxplot with seaborn\n",
    "plt.title('Box Plot for Energy (kJ) after business cleaning per 100g')\n",
    "plt.xlabel('Energy (kJ)')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "632535bba5936709",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 4.1.6 Box Plots for Nutri-Score Variables After Business Outlier Cleaning excluding 'energy_100g'"
   ]
  },
  {
   "cell_type": "code",
   "id": "b786ec6e8ea2b7b7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Assuming 'base_path' is already defined somewhere in your notebook as the root folder path\n",
    "data_path = os.path.join(base_path, 'data', 'cleaned_data_from_business_outlier.csv')\n",
    "\n",
    "print(data_path)\n",
    "\n",
    "# Load your dataset using the correct path\n",
    "data = pd.read_csv(data_path, low_memory=False)  # Use the variable 'data_path'\n",
    "\n",
    "# Define the columns relevant to Nutri-Score calculations, excluding 'energy_100g'\n",
    "nutri_score_columns = [\n",
    "    'sugars_100g', 'saturated_fat_100g',\n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g',\n",
    "    'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Create boxplots for the specified Nutri-Score columns\n",
    "data[nutri_score_columns].boxplot(figsize=(25, 5))\n",
    "\n",
    "# Set the title for the boxplots\n",
    "plt.title('Box Plots for Nutri-Score Variables After Business Outlier Cleaning')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eef83f313039c56d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 4.1.7 Statistical Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9346b887b64925e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Ensure Consistent Data Types for Nutri-Score Columns only for checking the data types"
   ]
  },
  {
   "cell_type": "code",
   "id": "724e69f511f41183",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Construct the full path to the dataset\n",
    "data_path = os.path.join(base_path, 'data', 'cleaned_data_from_business_outlier.csv')\n",
    "\n",
    "# Load your dataset with the low_memory option set to False to better handle mixed data types\n",
    "data = pd.read_csv(data_path, low_memory=False)\n",
    "\n",
    "# Define the columns relevant to Nutri-Score calculations\n",
    "nutri_score_columns = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g',\n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g',\n",
    "    'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Convert all specified columns to numeric, handling non-numeric entries\n",
    "for column in nutri_score_columns:\n",
    "    data[column] = pd.to_numeric(data[column], errors='coerce')  # Convert to numeric, coerce errors to NaN\n",
    "\n",
    "# Optionally, check the data types to confirm all are numeric\n",
    "print(data[nutri_score_columns].dtypes)  # Display the data types of the Nutri-Score columns\n",
    "\n",
    "# Save the cleaned data back to a CSV file in the 'data' directory\n",
    "output_path = os.path.join(base_path, 'data', 'cleaned_data_from_business_outlier.csv')\n",
    "data.to_csv(output_path, index=False)  # Save the data to the same file or specify a new file\n",
    "\n",
    "print(f\"Data with consistent types for Nutri-Score columns saved to '{output_path}'\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e81ff3c6f2e68caf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "data.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6154e3a17468782",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Outlier Cleaning Using the IQR Method"
   ]
  },
  {
   "cell_type": "code",
   "id": "c3ee595940b08df9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Construct the full path to the cleaned data file\n",
    "data_path = os.path.join(base_path, 'data', 'cleaned_data_from_business_outlier.csv')\n",
    "\n",
    "# Load the cleaned data\n",
    "data = pd.read_csv(data_path, low_memory=False)\n",
    "\n",
    "# Create a copy of the DataFrame to work on\n",
    "data_copy = data.copy()\n",
    "\n",
    "# Define Nutri-Score relevant columns\n",
    "nutri_score_columns = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g',\n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g',\n",
    "    'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Checking for NaN values in the dataset\n",
    "print(\"NaN counts in Nutri-Score columns before any operations:\")\n",
    "print(data_copy[nutri_score_columns].isna().sum())\n",
    "\n",
    "# Initialize a mask for rows to keep, the mask is used to filter out the outliers\n",
    "mask = pd.Series(True, index=data_copy.index)\n",
    "\n",
    "# Calculate IQR for each column and update the mask to filter out the outliers\n",
    "for column in nutri_score_columns:\n",
    "    Q1 = data_copy[column].quantile(0.25)\n",
    "    Q3 = data_copy[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR  # Adjusting the multiplier to 1.5\n",
    "    upper_bound = Q3 + 1.5 * IQR  # Adjusting the multiplier to 1.5\n",
    "    column_mask = (data_copy[column] >= lower_bound) & (data_copy[column] <= upper_bound)\n",
    "    mask &= column_mask\n",
    "\n",
    "# Apply the mask to the dataframe copy\n",
    "filtered_data = data_copy[mask]\n",
    "\n",
    "# Display the number of original and filtered rows\n",
    "print(f\"Original data rows: {data.shape[0]}, Filtered data rows: {filtered_data.shape[0]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e9344b8c22550143",
   "metadata": {},
   "source": [
    "# Display data.shape and check if the code above change the dataframe\n",
    "data.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e2b22806ef337d38",
   "metadata": {},
   "source": [
    "### Observations from IQR Method Application\n",
    "\n",
    "Based on the results shared from applying the IQR method, it's evident that this outlier detection technique has significantly reduced the dataset size. This dramatic reduction—from 203,931 rows to just 1,538—indicates that the IQR method may be too aggressive for your data. Particularly noteworthy is the large number of `NaN` values, especially prominent in the `fruits_vegetables_nuts_100g` column, which further supports the unsuitability of the IQR method for this dataset.\n",
    "\n",
    "### Alternative Statistical Method for Outlier Detection\n",
    "\n",
    "Given these observations, exploring an alternative statistical method for outlier detection seems necessary. Using the median and standard deviation could be more appropriate for this dataset. This approach typically provides a more robust handling of outliers, especially in distributions that are not severely skewed. It tends to be less sensitive to extreme values, thus preserving more of the dataset while still effectively filtering out true outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4174aa9910216a4",
   "metadata": {},
   "source": [
    "#### Outlier cleaning based on the median and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "id": "83e30d6f48567709",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming data is loaded into a DataFrame `data`\n",
    "# Create a copy of the DataFrame to preserve the original data\n",
    "data_copy = data.copy()\n",
    "\n",
    "# Define the features for which you want to detect outliers\n",
    "sigma_features = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g', \n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g', \n",
    "    'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Initialize lists to store the median and standard deviation for each feature\n",
    "medians = []\n",
    "sigmas = []\n",
    "\n",
    "# Calculate median and trimmed standard deviation for each feature\n",
    "for feature in sigma_features:\n",
    "    valid_data = data_copy[feature].dropna()  # Work with non-NA values for accurate calculations\n",
    "    median = valid_data.median()\n",
    "    sorted_data = valid_data.sort_values()\n",
    "    sigma = np.std(sorted_data[:-25])  # Exclude top 25 extreme values for std calculation\n",
    "\n",
    "    medians.append(median)\n",
    "    sigmas.append(sigma)\n",
    "\n",
    "    # Visualization\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(sorted_data.values, 'b.', markersize=5)\n",
    "    plt.axhline(y=median, color='g', linestyle='dashdot', label='Median')\n",
    "    plt.axhline(y=median + 3 * sigma, color='r', linestyle='-', label='Upper Bound (3σ)')\n",
    "    plt.axhline(y=median - 3 * sigma, color='r', linestyle='-', label='Lower Bound (3σ)')\n",
    "    plt.title(f'Outlier Detection in {feature}')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel(feature)\n",
    "    plt.yscale('log')  # Use logarithmic scale if data range is large\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Apply the calculated bounds to filter outliers\n",
    "    data_copy = data_copy[(data_copy[feature] >= (median - 3 * sigma)) & (data_copy[feature] <= (median + 3 * sigma))]\n",
    "\n",
    "# Print the remaining rows in the data copy after outlier removal\n",
    "print(f\"Remaining data rows after outlier removal: {data_copy.shape[0]}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "29a5cdbe677df14b",
   "metadata": {},
   "source": [
    "### Analysis of Outlier Removal Results with Median and Standard Deviation Method\n",
    "\n",
    "- After applying a statistical method for outlier detection based on median and standard deviation, the resulting dataset was reduced significantly to only 1,740 rows. This substantial decrease raises concerns about the appropriateness of the method used, particularly regarding its impact on the dataset's representativeness and the potential loss of valuable data.\n",
    "Insights on Outlier Identification\n",
    "\n",
    "- The outlier detection method employed uses the median and three times the standard deviation to define the upper and lower bounds for what constitutes normal data. While this approach is commonly accepted for handling outliers, especially in datasets assumed to follow a normal distribution, it has a critical limitation in practice, particularly for data like nutritional information where distributions can be skewed or not follow a normal pattern:\n",
    "\n",
    "- Lower Bound Impact: One notable issue with this technique is its tendency to incorrectly identify legitimate low values as outliers, especially when the lower bound near zero, which is common for nutritional components that naturally vary in range, including near zero values (like sodium content in low-sodium foods).  \n",
    "- Business Context Considerations: From a business perspective, the removal of such a large portion of the dataset can eliminate many products that are potentially important to retain for comprehensive analysis. For instance, foods naturally low in fats, sugars, or sodium might be categorized as outliers and excluded, skewing analyses toward products with higher content of these nutrients.\n",
    "- In conclusion, relying solely on business rule integration for outlier detection in nutritional data can be sufficient if the rules are well-defined, maintained, and implemented with a clear understanding of their limitations and scope. This focused approach can enhance the dataset's relevance and usability for specific business or regulatory needs, supporting more accurate and meaningful insights from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3931c9f77d3efa97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "8fc6327de085a652",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Display data.shape\n",
    "data.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7c0e91bdf100e37b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1969e04ef50dc36c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 5 Impute Missing Values: Strategies - CE2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9720988408751134",
   "metadata": {},
   "source": [
    "*Overview of strategies used to handle missing data.*\n",
    "\n",
    "- Step 1: Infer variables with missing values that are necessary for the calculation of nutrition_score_fr_100g (Nutri-Score).\n",
    "- Step 2: Infer target variable (nutrition_score_fr_100g) only for those with missing values."
   ]
  },
  {
   "cell_type": "code",
   "id": "a0ad3425748310f8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Calculate the total percentage of missing values in the dataset\n",
    "total_percentage_missing_value = (data.isna().sum().sum() / data.size)\n",
    "print(\"Total percentage of missing values in the dataset =\", total_percentage_missing_value)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d229fdbdac9437d1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Display the percentage and number of missing values for 'nutrition_score_fr_100g' in the dataset from the previous step 'cleaned_data_from_business_outliers.csv'\n",
    "missing_values_percentage = data['nutrition_score_fr_100g'].isnull().mean() * 100\n",
    "missing_values_count = data['nutrition_score_fr_100g'].isnull().sum()\n",
    "print (f\"Percentage of missing values for 'nutrition_score_fr_100g': {missing_values_percentage:.2f}%\")\n",
    "print (f\"Number of missing values for 'nutrition_score_fr_100g': {missing_values_count}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a223d37f245b3c73",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Implementation Sequence and Considerations\n",
    "\n",
    "- **Median**: Best for numerical data that is not heavily skewed.\n",
    "- **Zero**: Useful for categorical data encoded as numbers or counters where zero is a valid value.\n",
    "- **KNN Imputation**: Effective when the dataset is sufficiently large and missing values are relatively few. Good for complex patterns but computationally expensive.\n",
    "- **Iterative Imputer**: Useful for multivariate data where each attribute helps predict others. Assumes linear relationships.\n",
    "- **Deletion**: Simplest, but only viable when missing data is minimal and randomly distributed.\n",
    "\n",
    "Each method should be chosen based on the nature of the data and the specific requirements of the analysis or the predictive modeling to be performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f272f1fcd4f2eb4c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Assuming 'data' is your DataFrame containing all your data.\n",
    "# nutri_score_columns is the list of columns necessary for Nutri-Score calculation, excluding 'nutrition_score_fr_100g' since it is used as a base.\n",
    "\n",
    "nutri_score_columns = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g', \n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Filter the data to include only rows where 'nutrition_score_fr_100g' is not null\n",
    "data_with_nutrition_grade = data[data['nutrition_score_fr_100g'].notna()]\n",
    "\n",
    "# Display the count of missing values in each of the other Nutri-Score related columns\n",
    "# Sort the counts of missing values from highest to lowest\n",
    "missing_counts = data_with_nutrition_grade[nutri_score_columns].isna().sum().sort_values(ascending=False)\n",
    "\n",
    "# Print the results\n",
    "print(\"Missing values count for each Nutri-Score variable when 'nutrition_score_fr_100g' is present, sorted from highest to lowest:\")\n",
    "print(missing_counts)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "70bb532f52b2f21f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "data.shape # display the shape of the data  (rows, columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "38a5a162b5f943e6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": "### 5.1 Imputation Based on Median Values - CE2 - Median Imputation on data Where Nutri-Score Is not Present\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Je souhaite imputer les valeurs manquantes tout en conservant autant de données que possible. Le problème actuel est que les lignes avec pnns_groups_2 égales à 'unknown' sont supprimées, ce qui entraîne une perte significative de données.\n",
    "\n",
    "Pour résoudre ce problème, nous allons adopter une approche différente :\n",
    "\n",
    "- Conserver les lignes où pnns_groups_2 est 'unknown' mais ne pas les utiliser pour les calculs de médiane.\n",
    "- Imputer les valeurs manquantes pour les autres lignes.\n",
    "- Combiner les deux ensembles de données après imputation."
   ],
   "id": "2a20fee695911061"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c4e89b318342e05a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T21:43:57.670018Z",
     "start_time": "2024-05-14T21:43:09.427689Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Définir les chemins vers les fichiers de données\n",
    "source_data_path = \"D:\\\\OC_IA\\\\P3\\\\OC_IA_P3_Prepare_data_for_a_public_health_organization\\\\data\\\\cleaned_data_from_business_outlier.csv\"\n",
    "base_data_path = \"D:\\\\OC_IA\\\\P3\\\\OC_IA_P3_Prepare_data_for_a_public_health_organization\\\\data\\\\base_data_for_method.csv\"\n",
    "output_data_path = \"D:\\\\OC_IA\\\\P3\\\\OC_IA_P3_Prepare_data_for_a_public_health_organization\\\\data\\\\data_after_median_imputation.csv\"\n",
    "\n",
    "# Charger les données en spécifiant le type de données str pour éviter les avertissements Dtype\n",
    "source_data = pd.read_csv(base_data_path, dtype=str)\n",
    "clean_data = pd.read_csv(source_data_path, dtype=str)\n",
    "\n",
    "# Nettoyer les noms de colonnes\n",
    "source_data.columns = source_data.columns.str.strip().str.lower().str.replace('-', '_')\n",
    "clean_data.columns = clean_data.columns.str.strip().str.lower().str.replace('-', '_')\n",
    "\n",
    "# Afficher la forme des DataFrames chargés\n",
    "print(\"Shape of source_data (base_data_for_method):\", source_data.shape)\n",
    "print(\"Shape of clean_data (cleaned_data_from_business_outlier):\", clean_data.shape)\n",
    "\n",
    "# Définir les colonnes hiérarchiques et quantitatives\n",
    "hierarchy = ['pnns_groups_2']\n",
    "quantitative_columns = ['energy_100g', 'sugars_100g', 'saturated_fat_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g']\n",
    "\n",
    "# Convertir les colonnes quantitatives en numérique\n",
    "for col in quantitative_columns:\n",
    "    source_data[col] = pd.to_numeric(source_data[col], errors='coerce')\n",
    "    clean_data[col] = pd.to_numeric(clean_data[col], errors='coerce')\n",
    "\n",
    "# Séparer les données avec 'pnns_groups_2' égal à 'unknown'\n",
    "unknown_pnns_groups_2 = clean_data[clean_data['pnns_groups_2'].str.lower().str.strip() == 'unknown']\n",
    "known_pnns_groups_2 = clean_data[clean_data['pnns_groups_2'].str.lower().str.strip() != 'unknown']\n",
    "\n",
    "print(\"Shape of unknown_pnns_groups_2:\", unknown_pnns_groups_2.shape)\n",
    "print(\"Shape of known_pnns_groups_2:\", known_pnns_groups_2.shape)\n",
    "\n",
    "# Créer une table pivot pour 'pnns_groups_2' de source_data\n",
    "pivot_table = source_data.groupby('pnns_groups_2')[quantitative_columns].median()\n",
    "\n",
    "# Fonction pour appliquer les médianes\n",
    "def apply_medians(row):\n",
    "    if pd.isna(row['nutrition_score_fr_100g']):  # Vérifier si 'nutrition_score_fr_100g' est NaN\n",
    "        if pd.notna(row['pnns_groups_2']) and row['pnns_groups_2'] in pivot_table.index:\n",
    "            for col in quantitative_columns:\n",
    "                if pd.isna(row[col]):\n",
    "                    row[col] = pivot_table.loc[row['pnns_groups_2'], col]\n",
    "    return row\n",
    "\n",
    "# Appliquer les médianes aux données propres\n",
    "imputed_data = known_pnns_groups_2.apply(apply_medians, axis=1)\n",
    "print(\"Shape of imputed_data:\", imputed_data.shape)\n",
    "\n",
    "# Combiner les données imputées avec celles ayant 'pnns_groups_2' égal à 'unknown'\n",
    "final_data = pd.concat([imputed_data, unknown_pnns_groups_2], ignore_index=True)\n",
    "print(\"Shape of final_data after combining:\", final_data.shape)\n",
    "\n",
    "# Sauvegarder les données après imputation\n",
    "final_data.to_csv(output_data_path, index=False)\n",
    "print(f\"DataFrame après imputation médiane sauvegardé à : {output_data_path}\")\n",
    "\n",
    "\n"
   ],
   "id": "d5d4251bd1265856",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of source_data (base_data_for_method): (68445, 162)\n",
      "Shape of clean_data (cleaned_data_from_business_outlier): (283629, 162)\n",
      "Shape of unknown_pnns_groups_2: (215184, 162)\n",
      "Shape of known_pnns_groups_2: (68445, 162)\n",
      "Shape of imputed_data: (68445, 162)\n",
      "Shape of final_data after combining: (283629, 162)\n",
      "DataFrame après imputation médiane sauvegardé à : D:\\OC_IA\\P3\\OC_IA_P3_Prepare_data_for_a_public_health_organization\\data\\data_after_median_imputation.csv\n"
     ]
    }
   ],
   "execution_count": 231
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import pandas as pd\n",
    "# \n",
    "# # Définir les chemins vers les fichiers de données\n",
    "# base_data_path = \"D:\\\\OC_IA\\\\P3\\\\OC_IA_P3_Prepare_data_for_a_public_health_organization\\\\data\\\\base_data_for_method.csv\"\n",
    "# clean_data_path = \"D:\\\\OC_IA\\\\P3\\\\OC_IA_P3_Prepare_data_for_a_public_health_organization\\\\data\\\\cleaned_data_from_business_outlier.csv\"\n",
    "# output_data_path = \"D:\\\\OC_IA\\\\P3\\\\OC_IA_P3_Prepare_data_for_a_public_health_organization\\\\data\\\\data_after_median_imputation.csv\"\n",
    "# \n",
    "# # Charger les données en spécifiant le type de données str pour éviter les avertissements Dtype\n",
    "# base_data = pd.read_csv(base_data_path, dtype=str)\n",
    "# clean_data = pd.read_csv(clean_data_path, dtype=str)\n",
    "# \n",
    "# # Nettoyer les noms de colonnes\n",
    "# base_data.columns = base_data.columns.str.strip().str.lower().str.replace('-', '_')\n",
    "# clean_data.columns = clean_data.columns.str.strip().str.lower().str.replace('-', '_')\n",
    "# \n",
    "# # Afficher la forme des DataFrames chargés\n",
    "# print(\"Shape of base_data:\", base_data.shape)\n",
    "# print(\"Shape of clean_data:\", clean_data.shape)\n",
    "# \n",
    "# # Définir les colonnes hiérarchiques et quantitatives\n",
    "# hierarchy = ['pnns_groups_2', 'pnns_groups_1', 'main_category_fr', 'categories_fr']\n",
    "# quantitative_columns = ['energy_100g', 'sugars_100g', 'saturated_fat_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g']\n",
    "# \n",
    "# # Convertir les colonnes quantitatives en numérique\n",
    "# for col in quantitative_columns:\n",
    "#     base_data[col] = pd.to_numeric(base_data[col], errors='coerce')\n",
    "#     clean_data[col] = pd.to_numeric(clean_data[col], errors='coerce')\n",
    "# \n",
    "# # Inspection de valeurs uniques pour débogage\n",
    "# print({col: clean_data[col].unique() for col in hierarchy})\n",
    "# \n",
    "# # Filtrer les lignes où toutes les clés de hiérarchie sont 'unknown' et 'nutrition_score_fr_100g' est NaN\n",
    "# filter_condition = (clean_data[hierarchy].apply(lambda x: x.str.lower().str.strip()) == 'unknown').all(axis=1) & clean_data['nutrition_score_fr_100g'].isna()\n",
    "# print(\"Rows to filter:\", filter_condition.sum())  # Nombre de lignes qui devraient être filtrées\n",
    "# clean_data = clean_data[~filter_condition]\n",
    "# print(\"Shape of clean_data after filtering hierarchy:\", clean_data.shape)\n",
    "# \n",
    "# # Créer des tables pivot pour chaque catégorie hiérarchique de base_data\n",
    "# pivot_tables = {key: base_data.groupby(key)[quantitative_columns].median() for key in hierarchy if key in base_data.columns}\n",
    "# \n",
    "# # Fonction pour appliquer les médianes\n",
    "# def apply_medians(row):\n",
    "#     if pd.isna(row['nutrition_score_fr_100g']):  # Vérifier si 'nutrition_score_fr_100g' est NaN\n",
    "#         for key in hierarchy:  # Parcours de gauche à droite\n",
    "#             if pd.notna(row[key]) and row[key] in pivot_tables[key].index:\n",
    "#                 for col in quantitative_columns:\n",
    "#                     if pd.isna(row[col]):\n",
    "#                         row[col] = pivot_tables[key].loc[row[key], col]\n",
    "#                 break  # Arrêter après la première clé valide trouvée\n",
    "#     return row\n",
    "# \n",
    "# # Appliquer les médianes aux données propres\n",
    "# clean_data = clean_data.apply(apply_medians, axis=1)\n",
    "# print(\"Shape of clean_data after median imputation:\", clean_data.shape)\n",
    "# \n",
    "# # Sauvegarder les données après imputation\n",
    "# clean_data.to_csv(output_data_path, index=False)\n",
    "# print(f\"DataFrame après imputation médiane sauvegardé à : {output_data_path}\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "ed027d64bfb41e9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# \n",
    "# # Chemins vers les fichiers de données\n",
    "# base_data_path = \"D:\\\\OC_IA\\\\P3\\\\OC_IA_P3_Prepare_data_for_a_public_health_organization\\\\data\\\\base_data_for_method.csv\"\n",
    "# clean_data_path = \"D:\\\\OC_IA\\\\P3\\\\OC_IA_P3_Prepare_data_for_a_public_health_organization\\\\data\\\\cleaned_data_from_business_outlier.csv\"\n",
    "# output_data_path = \"D:\\\\OC_IA\\\\P3\\\\OC_IA_P3_Prepare_data_for_a_public_health_organization\\\\data\\\\data_after_median_imputation.csv\"\n",
    "# \n",
    "# # Charger les données en spécifiant le type de la première colonne pour éviter les avertissements\n",
    "# base_data = pd.read_csv(base_data_path, dtype={0: str})\n",
    "# clean_data = pd.read_csv(clean_data_path)\n",
    "# \n",
    "# # Assurer que les tirets sont remplacés par des underscores dans les noms des colonnes si nécessaire\n",
    "# base_data.columns = base_data.columns.str.replace('-', '_')\n",
    "# clean_data.columns = clean_data.columns.str.replace('-', '_')\n",
    "# \n",
    "# # Définir les colonnes pour l'imputation\n",
    "# hierarchy = ['pnns_groups_2', 'pnns_groups_1', 'main_category_fr', 'categories_fr']\n",
    "# quantitative_columns = ['energy_100g', 'sugars_100g', 'saturated_fat_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g']\n",
    "# \n",
    "# # Créer des tables pivot pour chaque colonne quantitative basées sur chaque clé hiérarchique\n",
    "# pivot_tables = {}\n",
    "# for column in quantitative_columns:\n",
    "#     pivot_tables[column] = {}\n",
    "#     for key in hierarchy:\n",
    "#         if key in base_data.columns:\n",
    "#             pivot_tables[column][key] = base_data.pivot_table(index=key, values=column, aggfunc='median')\n",
    "# \n",
    "# # Appliquer les médianes aux données propres\n",
    "# for column in quantitative_columns:\n",
    "#     for key in hierarchy:\n",
    "#         if key in clean_data.columns and key in pivot_tables[column]:\n",
    "#             median_map = pivot_tables[column][key].to_dict()[column]\n",
    "#             clean_data[column] = clean_data.apply(lambda row: median_map.get(row[key], row[column]) if pd.isna(row[column]) else row[column], axis=1)\n",
    "# \n",
    "# # Vérifier si les colonnes nécessaires existent avant de tenter de supprimer les NaN\n",
    "# if all(col in clean_data.columns for col in hierarchy + quantitative_columns):\n",
    "#     clean_data.dropna(subset=hierarchy + quantitative_columns, how='all', inplace=True)\n",
    "# else:\n",
    "#     missing_cols = [col for col in hierarchy + quantitative_columns if col not in clean_data.columns]\n",
    "#     print(\"Colonnes manquantes pour dropna:\", missing_cols)\n",
    "# \n",
    "# # Sauvegarde du DataFrame final\n",
    "# clean_data.to_csv(output_data_path, index=False)\n",
    "# print(f\"DataFrame après imputation médiane sauvegardé à : {output_data_path}\")\n"
   ],
   "id": "1a6f76fdb7bdddb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# \n",
    "# # Chemins vers les fichiers de données\n",
    "# base_data_path = \"D:\\\\OC_IA\\\\P3\\\\OC_IA_P3_Prepare_data_for_a_public_health_organization\\\\data\\\\base_data_for_method.csv\"\n",
    "# clean_data_path = \"D:\\\\OC_IA\\\\P3\\\\OC_IA_P3_Prepare_data_for_a_public_health_organization\\\\data\\\\cleaned_data_from_business_outlier.csv\"\n",
    "# output_data_path = \"D:\\\\OC_IA\\\\P3\\\\OC_IA_P3_Prepare_data_for_a_public_health_organization\\\\data\\\\data_after_median_imputation.csv\"\n",
    "# \n",
    "# # Charger les données\n",
    "# base_data = pd.read_csv(base_data_path) # Charger les données de base\n",
    "# clean_data = pd.read_csv(clean_data_path) # Charger les données nettoyées\n",
    "# \n",
    "# # Remplacement des tirets par des underscores dans les noms des colonnes si nécessaire\n",
    "# base_data.columns = base_data.columns.str.replace('-', '_') # Remplacer les tirets par des underscores\n",
    "# clean_data.columns = clean_data.columns.str.replace('-', '_') # Remplacer les tirets par des underscores\n",
    "# \n",
    "# # Définir les colonnes pour l'imputation\n",
    "# hierarchy = ['pnns_groups_2', 'pnns_groups_1', 'main_category_fr', 'categories_fr']\n",
    "# quantitative_columns = ['energy_100g', 'sugars_100g', 'saturated_fat_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g']\n",
    "# \n",
    "# # Créer des tables pivot pour chaque colonne quantitative basées sur chaque clé hiérarchique\n",
    "# pivot_tables = {} # Initialiser un dictionnaire pour stocker les tables pivot\n",
    "# for column in quantitative_columns: # Pour chaque colonne quantitative\n",
    "#     pivot_tables[column] = {} # Initialiser un dictionnaire pour stocker les tables pivot pour chaque colonne\n",
    "#     for key in hierarchy: # Pour chaque clé hiérarchique\n",
    "#         if key in base_data.columns: # Si la clé hiérarchique est présente dans les données de base\n",
    "#             pivot_tables[column][key] = base_data.pivot_table(index=key, values=column, aggfunc='median') # Créer une table pivot basée sur la médiane\n",
    "# \n",
    "# # Appliquer les médianes aux données propres\n",
    "# for column in quantitative_columns: # Pour chaque colonne quantitative\n",
    "#     for key in hierarchy: # Pour chaque clé hiérarchique\n",
    "#         if key in clean_data.columns and key in pivot_tables[column]: # Si la clé hiérarchique est présente dans les données propres et dans les tables pivot\n",
    "#             median_map = pivot_tables[column][key].to_dict()[column] # Créer une carte des médianes\n",
    "#             clean_data[column] = clean_data.apply(lambda row: median_map.get(row[key], row[column]) if pd.isna(row[column]) else row[column], axis=1) # Appliquer la médiane si la valeur est manquante \n",
    "# \n",
    "# # Vérifier si les colonnes nécessaires existent avant de tenter de supprimer les NaN\n",
    "# if all(col in clean_data.columns for col in hierarchy + quantitative_columns): # Si toutes les colonnes nécessaires sont présentes\n",
    "#     clean_data.dropna(subset=hierarchy + quantitative_columns, how='all', inplace=True) # Supprimer les lignes avec des valeurs manquantes\n",
    "# else: # Sinon\n",
    "#     missing_cols = [col for col in hierarchy + quantitative_columns if col not in clean_data.columns] # Stocker les colonnes manquantes\n",
    "#     print(\"Colonnes manquantes pour dropna:\", missing_cols) # Afficher les colonnes manquantes\n",
    "# \n",
    "# # Sauvegarde du DataFrame final\n",
    "# clean_data.to_csv(output_data_path, index=False) # Sauvegarder les données nettoyées après imputation médiane\n",
    "# print(f\"DataFrame après imputation médiane sauvegardé à : {output_data_path}\")  # Afficher le chemin du fichier sauvegardé\n"
   ],
   "id": "3c2dc777b1dc90f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import pandas as pd\n",
    "# \n",
    "# # Assurer que le chemin d'accès et les noms des fichiers sont corrects\n",
    "# data = pd.read_csv(data_path, sep='\\t', low_memory=False, on_bad_lines='skip')\n",
    "# data.columns = data.columns.str.replace('-', '_')\n",
    "# \n",
    "# base_data_path = os.path.join(base_path, 'data', 'base_data_for_method.csv')\n",
    "# base_data = pd.read_csv(base_data_path, dtype={0: str})\n",
    "# base_data.columns = base_data.columns.str.replace('-', '_')\n",
    "# \n",
    "# # Vérifiez les noms des colonnes pour s'assurer qu'ils sont corrects\n",
    "# print(\"Colonnes dans 'data':\", data.columns.tolist())\n",
    "# print(\"Colonnes dans 'base_data':\", base_data.columns.tolist())\n",
    "# \n",
    "# # Si les noms de colonne sont corrects, procéder avec l'imputation\n",
    "# hierarchy = ['pnns_groups_2', 'pnns_groups_1', 'main_category_fr', 'categories_fr']  # de droite à gauche\n",
    "# quantitative_columns = ['energy_100g', 'sugars_100g', 'saturated_fat_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g']\n",
    "# \n",
    "# pivot_tables = {}\n",
    "# for column in quantitative_columns:\n",
    "#     pivot_tables[column] = {}\n",
    "#     for key in hierarchy:\n",
    "#         if key in base_data.columns:\n",
    "#             pivot_tables[column][key] = base_data.pivot_table(index=key, values=column, aggfunc='median')\n",
    "# \n",
    "# for column in quantitative_columns:\n",
    "#     for key in hierarchy:\n",
    "#         if key in data.columns and key in pivot_tables[column]:\n",
    "#             median_map = pivot_tables[column][key].to_dict()[column]\n",
    "#             data[column] = data.apply(lambda row: median_map.get(row[key], row[column]) if pd.isna(row[column]) else row[column], axis=1)\n",
    "#             break\n",
    "# \n",
    "# # Assurez-vous que les colonnes dans 'subset' existent dans 'data'\n",
    "# if all(col in data.columns for col in hierarchy + quantitative_columns):\n",
    "#     data.dropna(subset=hierarchy + quantitative_columns, how='all', inplace=True)\n",
    "# else:\n",
    "#     missing_cols = [col for col in hierarchy + quantitative_columns if col not in data.columns]\n",
    "#     print(\"Colonnes manquantes pour dropna:\", missing_cols)\n",
    "# \n",
    "# # Afficher le shape après imputation\n",
    "# print(\"Shape of the DataFrame after imputation:\", data.shape)\n"
   ],
   "id": "17b5ab8198d75d55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# # Path to the data file\n",
    "# base_path = 'your_base_path_here'  # Replace with your actual base path\n",
    "# data_path = os.path.join(base_path, 'data', 'fr.openfoodfacts.org.products.csv')\n",
    "# \n",
    "# # Load data using the defined path\n",
    "# data = pd.read_csv(data_path, sep='\\t', low_memory=False, on_bad_lines='skip')\n",
    "# \n",
    "# # Replace hyphens with underscores in column names\n",
    "# data.columns = data.columns.str.replace('-', '_')\n",
    "# \n",
    "# # Display the first few rows to verify data loading and column name changes\n",
    "# print(data.head())\n",
    "# \n",
    "# # List of qualitative variables that must be completely non-null, including 'nutrition_score_fr_100g'\n",
    "# required_non_null_columns = [\n",
    "#     'categories_fr', 'pnns_groups_1', 'pnns_groups_2', 'main_category_fr', 'nutrition_grade_fr', 'nutrition_score_fr_100g'\n",
    "# ]\n",
    "# \n",
    "# # List of quantitative variables\n",
    "# quantitative_columns = [\n",
    "#     'nutrition_score_fr_100g', 'energy_100g', 'sugars_100g', 'saturated_fat_100g', \n",
    "#     'sodium_100g', 'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g'\n",
    "# ]\n",
    "# \n",
    "# # Check if all required columns are present in the DataFrame\n",
    "# missing_columns = [col for col in required_non_null_columns if col not in data.columns]\n",
    "# if missing_columns:\n",
    "#     raise KeyError(f\"Missing columns in the DataFrame: {missing_columns}\")\n",
    "# \n",
    "# # Filter out rows where any of the required qualitative columns or 'nutrition_score_fr_100g' are NaN\n",
    "# base_data_for_method = data.dropna(subset=required_non_null_columns).copy()\n",
    "# \n",
    "# # Replace NaNs and empty strings in quantitative variables with zeros using .loc to avoid SettingWithCopyWarning\n",
    "# quantitative_columns_to_fill = [col for col in quantitative_columns if col != 'nutrition_score_fr_100g']\n",
    "# for col in quantitative_columns_to_fill:\n",
    "#     base_data_for_method.loc[:, col] = base_data_for_method.loc[:, col].replace('', 0)\n",
    "#     base_data_for_method.loc[:, col] = base_data_for_method.loc[:, col].fillna(0)\n",
    "# \n",
    "# # Displaying the shape of the new DataFrame to verify the number of complete cases\n",
    "# print(\"Shape of the new DataFrame with selected columns:\", base_data_for_method.shape)\n",
    "# \n",
    "# # Path to save the cleaned DataFrame\n",
    "# output_path = os.path.join(base_path, 'data', 'base_data_for_method.csv')\n",
    "# \n",
    "# # Saving the cleaned DataFrame to the specified path\n",
    "# base_data_for_method.to_csv(output_path, index=False)\n",
    "# print(f'DataFrame saved to {output_path}')\n",
    "# \n",
    "# # Actualisation du second code\n",
    "# # Define the group keys in hierarchical order\n",
    "# hierarchy = [\n",
    "#     ['categories_fr', 'main_category_fr', 'pnns_groups_1', 'pnns_groups_2'],  # Most specific level\n",
    "#     ['categories_fr', 'main_category_fr', 'pnns_groups_1'],  # Slightly more general\n",
    "#     ['categories_fr', 'main_category_fr'],  # More general\n",
    "#     ['categories_fr'],  # Most general category only\n",
    "# ]\n",
    "# \n",
    "# # Quantitative columns to impute\n",
    "# quantitative_columns = [\n",
    "#     'energy_100g', 'sugars_100g', 'saturated_fat_100g', \n",
    "#     'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g'\n",
    "# ]\n",
    "# \n",
    "# # Define the columns relevant to Nutri-Score calculation\n",
    "# nutri_score_columns = [\n",
    "#     'energy_100g', 'sugars_100g', 'saturated_fat_100g', \n",
    "#     'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g'\n",
    "# ]\n",
    "# \n",
    "# # Visualize distributions before imputation\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))  # Adjusted subplot grid size to match number of columns\n",
    "# fig.suptitle('Distributions Before Imputation', fontsize=16)\n",
    "# for ax, column in zip(axes.flatten(), nutri_score_columns):\n",
    "#     data[column].hist(ax=ax, bins=30, alpha=0.5, color='blue')\n",
    "#     ax.set_title(f'Distribution of {column}')\n",
    "#     ax.set_xlabel('Value')\n",
    "#     ax.set_ylabel('Frequency')\n",
    "# plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# plt.show()\n",
    "# \n",
    "# # Prepare to store medians in a DataFrame that we'll use for imputation\n",
    "# medians = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "# \n",
    "# for level in hierarchy:\n",
    "#     # Compute medians for current grouping level in base_data_for_method\n",
    "#     group_medians = base_data_for_method.groupby(level)[quantitative_columns].median().reset_index()\n",
    "# \n",
    "#     # Rename median columns to indicate their level (makes it easier to understand the source level of imputation)\n",
    "#     group_medians.columns = [col if col in level else f'{col}_median_level_{len(level)}' for col in group_medians.columns]\n",
    "# \n",
    "#     # Merge these medians with the main data DataFrame if medians are not already defined\n",
    "#     if medians.empty:\n",
    "#         medians = group_medians\n",
    "#     else:\n",
    "#         # Merge only where medians are not already available (NaNs will be filled progressively)\n",
    "#         medians = pd.merge(medians, group_medians, on=level, how='left')\n",
    "# \n",
    "# # Apply the hierarchical medians to the main data DataFrame\n",
    "# data = pd.merge(data, medians, on=['categories_fr', 'main_category_fr', 'pnns_groups_1', 'pnns_groups_2'], how='left')\n",
    "# \n",
    "# # Condition where nutrition_score_fr_100g is NaN\n",
    "# condition = data['nutrition_score_fr_100g'].isnull()\n",
    "# \n",
    "# # Impute missing values from the most specific to the most general median available\n",
    "# for column in quantitative_columns:\n",
    "#     data.loc[condition, column] = data.apply(\n",
    "#         lambda row: row[f'{column}_median_level_4'] if pd.isna(row[column]) else (\n",
    "#             row[f'{column}_median_level_3'] if pd.isna(row[column]) else (\n",
    "#                 row[f'{column}_median_level_2'] if pd.isna(row[column]) else (\n",
    "#                     row[f'{column}_median_level_1'] if pd.isna(row[column]) else row[column]\n",
    "#                 )\n",
    "#             )\n",
    "#         ),\n",
    "#         axis=1\n",
    "#     )\n",
    "# \n",
    "# # Visualize distributions after imputation\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
    "# fig.suptitle('Distributions After Imputation', fontsize=16)\n",
    "# for ax, column in zip(axes.flatten(), nutri_score_columns):\n",
    "#     data[column].hist(ax=ax, bins=30, alpha=0.5, color='green')\n",
    "#     ax.set_title(f'Distribution of {column} After Imputation')\n",
    "#     ax.set_xlabel('Value')\n",
    "#     ax.set_ylabel('Frequency')\n",
    "# plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# plt.show()\n",
    "# \n",
    "# # Clean up: Remove the temporary median columns\n",
    "# data.drop(columns=[col for col in data.columns if 'median_level_' in col], inplace=True)\n",
    "# \n",
    "# # Optionally show some results to verify\n",
    "# print(data[quantitative_columns].describe())\n",
    "# \n",
    "# # Path to save the imputed DataFrame\n",
    "# imputed_output_path = os.path.join(base_path, 'data', 'median_imputed.csv')\n",
    "# \n",
    "# # Saving the imputed DataFrame to the specified path\n",
    "# data.to_csv(imputed_output_path, index=False)\n",
    "# print(f'Imputed DataFrame saved to {imputed_output_path}')\n"
   ],
   "id": "f98294ba88ff5645",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13d106399d905135",
   "metadata": {},
   "source": [
    "# import pandas as pd\n",
    "# \n",
    "# # Define the group keys in hierarchical order\n",
    "# hierarchy = [\n",
    "#     ['categories_fr', 'main_category_fr', 'pnns_groups_1', 'pnns_groups_2'],  # Most specific level\n",
    "#     ['categories_fr', 'main_category_fr', 'pnns_groups_1'],  # Slightly more general\n",
    "#     ['categories_fr', 'main_category_fr'],  # More general\n",
    "#     ['categories_fr'],  # Most general category only\n",
    "# ]\n",
    "# \n",
    "# # Quantitative columns to impute\n",
    "# quantitative_columns = [\n",
    "#     'energy_100g', 'sugars_100g', 'saturated_fat_100g', \n",
    "#     'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g'\n",
    "# ]\n",
    "# \n",
    "# # Define the columns relevant to Nutri-Score calculation\n",
    "# nutri_score_columns = [\n",
    "#     'energy_100g', 'sugars_100g', 'saturated_fat_100g', \n",
    "#     'fiber_100g', 'proteins_100g', 'fruits_vegetables_nuts_100g'\n",
    "# ]\n",
    "# \n",
    "# # Visualize distributions before imputation\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))  # Adjusted subplot grid size to match number of columns\n",
    "# fig.suptitle('Distributions Before Imputation', fontsize=16)\n",
    "# for ax, column in zip(axes.flatten(), nutri_score_columns): # Use nutri_score_columns for consistency\n",
    "#     data[column].hist(ax=ax, bins=30, alpha=0.5, color='blue') # Use data[column] for consistency\n",
    "#     ax.set_title(f'Distribution of {column}') # Use f-string for consistency\n",
    "#     ax.set_xlabel('Value') # Use consistent axis labels\n",
    "#     ax.set_ylabel('Frequency') # Use consistent axis labels\n",
    "# plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjusted layout for better spacing\n",
    "# plt.show() # Display the plot\n",
    "# \n",
    "# \n",
    "# # Prepare to store medians in a DataFrame that we'll use for imputation\n",
    "# medians = pd.DataFrame() # Initialize an empty DataFrame\n",
    "# \n",
    "# for level in hierarchy:\n",
    "#     # Compute medians for current grouping level in base_data_for_method\n",
    "#     group_medians = base_data_for_method.groupby(level)[quantitative_columns].median().reset_index()\n",
    "# \n",
    "#     # Rename median columns to indicate their level (makes it easier to understand the source level of imputation)\n",
    "#     group_medians.columns = [col if col in level else f'{col}_median_level_{len(level)}' for col in group_medians.columns]\n",
    "# \n",
    "#     # Merge these medians with the main data DataFrame if medians are not already defined\n",
    "#     if medians.empty:\n",
    "#         medians = group_medians\n",
    "#     else:\n",
    "#         # Merge only where medians are not already available (NaNs will be filled progressively)\n",
    "#         medians = pd.merge(medians, group_medians, on=level, how='left') # Left join to keep all rows in medians\n",
    "# \n",
    "# # Apply the hierarchical medians to the main data DataFrame\n",
    "# data = pd.merge(data, medians, on=['categories_fr', 'main_category_fr', 'pnns_groups_1', 'pnns_groups_2'], how='left') # Left join to keep all rows in data\n",
    "# \n",
    "# # Condition where nutrition_score_fr_100g is NaN\n",
    "# condition = data['nutrition_score_fr_100g'].isnull() # Define the condition for missing values in the target column\n",
    "# \n",
    "# # Impute missing values from the most specific to the most general median available\n",
    "# for column in quantitative_columns: # Iterate over the quantitative columns\n",
    "#     data.loc[condition, column] = data.apply( # Impute missing values based on condition\n",
    "#         lambda row: row[f'{column}_median_level_4'] if pd.isna(row[column]) else ( # Use the highest level median if NaN\n",
    "#             row[f'{column}_median_level_3'] if pd.isna(row[column]) else ( # Use the next level median if NaN\n",
    "#                 row[f'{column}_median_level_2'] if pd.isna(row[column]) else ( # Use the next level median if NaN\n",
    "#                     row[f'{column}_median_level_1'] if pd.isna(row[column]) else row[column] # Use the lowest level median if NaN\n",
    "#                 )\n",
    "#             )\n",
    "#         ),\n",
    "#         axis=1 # Apply the lambda function row-wise\n",
    "#     )\n",
    "# \n",
    "# # Visualize distributions after imputation\n",
    "# fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))  # Keep consistent layout with pre-imputation plot\n",
    "# fig.suptitle('Distributions After Imputation', fontsize=16) # Set the title for the plot\n",
    "# for ax, column in zip(axes.flatten(), nutri_score_columns): # Use nutri_score_columns for consistency\n",
    "#     data[column].hist(ax=ax, bins=30, alpha=0.5, color='green') # Use data[column] for consistency\n",
    "#     ax.set_title(f'Distribution of {column} After Imputation') # Use f-string for consistency\n",
    "#     ax.set_xlabel('Value') # Use consistent axis labels\n",
    "#     ax.set_ylabel('Frequency') # Use consistent axis labels\n",
    "# plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjusted layout for better spacing\n",
    "# plt.show() # Display the plot\n",
    "# \n",
    "# \n",
    "# # Clean up: Remove the temporary median columns\n",
    "# data.drop(columns=[col for col in data.columns if 'median_level_' in col], inplace=True) # Drop the median columns from the data\n",
    "# \n",
    "# # Optionally show some results to verify\n",
    "# print(data[quantitative_columns].describe()) # Display summary statistics for the quantitative columns\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "41b67466ecd34db0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7290b05177a71f1e",
   "metadata": {},
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# # Assuming 'data' is your DataFrame\n",
    "# # Define the target variable for visualization\n",
    "# target_variable = 'nutrition_score_fr_100g'\n",
    "# \n",
    "# # Define the columns relevant to Nutri-Score calculation\n",
    "# nutri_score_columns = [\n",
    "#     'energy_100g', 'sugars_100g', 'saturated_fat_100g',\n",
    "#     'sodium_100g', 'fiber_100g', 'proteins_100g',\n",
    "#     'fruits_vegetables_nuts_100g'\n",
    "# ]\n",
    "# \n",
    "# # Display data before imputation\n",
    "# print(\"Data preview before imputation for Nutri-Score variables:\")\n",
    "# print(data[nutri_score_columns].head())\n",
    "# \n",
    "# # Show statistical summary before imputation\n",
    "# print(\"Statistical summary before imputation:\")\n",
    "# print(data[nutri_score_columns].describe())\n",
    "# \n",
    "# # Visualize distributions before imputation\n",
    "# fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 10))\n",
    "# fig.suptitle('Distributions Before Imputation', fontsize=16)\n",
    "# for ax, column in zip(axes.flatten()[:len(nutri_score_columns)], nutri_score_columns):\n",
    "#     data[column].hist(ax=ax, bins=30, alpha=0.5, color='blue')\n",
    "#     ax.set_title(f'Distribution of {column}')\n",
    "#     ax.set_xlabel('Value')\n",
    "#     ax.set_ylabel('Frequency')\n",
    "# plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# plt.show()\n",
    "# \n",
    "# # Impute missing values in Nutri-Score related columns\n",
    "# # Note: Using median imputation as it is less sensitive to outliers in data\n",
    "# medians = data[nutri_score_columns].median()\n",
    "# data[nutri_score_columns] = data[nutri_score_columns].fillna(medians)\n",
    "# \n",
    "# # Display data after imputation\n",
    "# print(\"Data preview after complete imputation for Nutri-Score variables:\")\n",
    "# print(data[nutri_score_columns].head())\n",
    "# \n",
    "# # Show statistical summary after imputation\n",
    "# print(\"Statistical summary after imputation:\")\n",
    "# print(data[nutri_score_columns].describe())\n",
    "# \n",
    "# # Visualize distributions after imputation\n",
    "# fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 10))\n",
    "# fig.suptitle('Distributions After Imputation', fontsize=16)\n",
    "# for ax, column in zip(axes.flatten()[:len(nutri_score_columns)], nutri_score_columns):\n",
    "#     data[column].hist(ax=ax, bins=30, alpha=0.5, color='green')\n",
    "#     ax.set_title(f'Distribution of {column} After Imputation')\n",
    "#     ax.set_xlabel('Value')\n",
    "#     ax.set_ylabel('Frequency')\n",
    "# plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f55f149ae92949f7",
   "metadata": {},
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# \n",
    "# # Assuming 'data' is your DataFrame\n",
    "# # Define the target variable\n",
    "# target_variable = 'nutrition_score_fr_100g'\n",
    "# \n",
    "# # Define the columns relevant to Nutri-Score calculation\n",
    "# nutri_score_columns = [\n",
    "#     'energy_100g', 'sugars_100g', 'saturated_fat_100g',\n",
    "#     'sodium_100g', 'fiber_100g', 'proteins_100g',\n",
    "#     'fruits_vegetables_nuts_100g'\n",
    "# ]\n",
    "# \n",
    "# # Optional: Uncomment the following if filtering is desired\n",
    "# # data = data[data[target_variable].notna()]\n",
    "# \n",
    "# # Display data before imputation\n",
    "# print(\"Data preview before imputation for Nutri-Score variables:\")\n",
    "# print(data[nutri_score_columns].head())\n",
    "# \n",
    "# # Show statistical summary before imputation\n",
    "# print(\"Statistical summary before imputation:\")\n",
    "# print(data[nutri_score_columns].describe())\n",
    "# \n",
    "# # Visualize before imputation\n",
    "# fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 10))\n",
    "# fig.suptitle('Distributions Before Imputation', fontsize=16)\n",
    "# for ax, column in zip(axes.flatten(), nutri_score_columns):\n",
    "#     data[column].hist(ax=ax, bins=30, alpha=0.5, color='blue')\n",
    "#     ax.set_title(column)\n",
    "#     ax.set_xlabel('Value')\n",
    "#     ax.set_ylabel('Frequency')\n",
    "# plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# plt.show()\n",
    "# \n",
    "# # Impute missing values in Nutri-Score related columns\n",
    "# numeric_columns = data[nutri_score_columns].select_dtypes(include='number')\n",
    "# median_values = numeric_columns.median()  # Calculate median for each column\n",
    "# numeric_columns_filled = numeric_columns.fillna(median_values)  # Fill missing values with the respective medians\n",
    "# data.update(numeric_columns_filled)\n",
    "# \n",
    "# # Display data after imputation\n",
    "# print(\"Data preview after complete imputation for Nutri-Score variables:\")\n",
    "# print(data[nutri_score_columns].head())\n",
    "# \n",
    "# # Show statistical summary after imputation\n",
    "# print(\"Statistical summary after imputation:\")\n",
    "# print(data[nutri_score_columns].describe())\n",
    "# \n",
    "# # Visualize after imputation\n",
    "# fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15, 10))\n",
    "# fig.suptitle('Distributions After Imputation', fontsize=16)\n",
    "# for ax, column in zip(axes.flatten(), nutri_score_columns):\n",
    "#     data[column].hist(ax=ax, bins=30, alpha=0.5, color='green')\n",
    "#     ax.set_title(column)\n",
    "#     ax.set_xlabel('Value')\n",
    "#     ax.set_ylabel('Frequency')\n",
    "# plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3558bcdc5ca70b18",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# # Assuming 'data' is your original DataFrame\n",
    "# # Create a copy of the DataFrame to avoid modifying the original data\n",
    "# data_median = data.copy()\n",
    "# \n",
    "# # Define the target variable and check only rows where it is not null\n",
    "# target_variable = 'nutrition_score_fr_100g'\n",
    "# \n",
    "# # Define the columns relevant to Nutri-Score calculation\n",
    "# nutri_score_columns = [\n",
    "#     'energy_100g', 'sugars_100g', 'saturated_fat_100g',\n",
    "#     'sodium_100g', 'fiber_100g', 'proteins_100g',\n",
    "#     'fruits_vegetables_nuts_100g'\n",
    "# ]\n",
    "# # Filter the DataFrame to include only rows where the target variable 'nutrition_score_fr_100g' is not null\n",
    "# data_median = data[data[target_variable].notna()]\n",
    "# \n",
    "# # Select only the numeric columns for median calculation\n",
    "# numeric_columns = data_median[nutri_score_columns].select_dtypes(include='number')\n",
    "# \n",
    "# # Calculate the median values for each numeric column\n",
    "# median_values = numeric_columns.median()\n",
    "# \n",
    "# # Fill missing values with the median values\n",
    "# numeric_columns_filled = numeric_columns.fillna(median_values)\n",
    "# \n",
    "# # Combine the filled numeric columns with the original DataFrame\n",
    "# data_median.update(numeric_columns_filled)\n",
    "# \n",
    "# # Display the median values for Nutri-Score variables\n",
    "# print(\"Median values for Nutri-Score variables:\")\n",
    "# print(median_values)\n",
    "# \n",
    "# # Check the null values count for each Nutri-Score variable after imputation\n",
    "# null_counts_after_imputation = data_median[nutri_score_columns].isnull().sum()\n",
    "# \n",
    "# # Check if there are any null values remaining after imputation\n",
    "# assert null_counts_after_imputation.sum() == 0, \"There are still null values in some Nutri-Score columns!\"\n",
    "# \n",
    "# # Print the null values count for each Nutri-Score variable after imputation\n",
    "# print(\"Null values count for each Nutri-Score variable after median imputation:\")\n",
    "# print(null_counts_after_imputation)\n",
    "# \n",
    "# # Display the first few rows of the updated data to verify the imputed values\n",
    "# print(\"Data preview after median imputation:\")\n",
    "# print(data_median[nutri_score_columns].head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "503c9c1dbd26bcdf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "data.shape # display the shape of the data  (rows, columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "15b39abcab731083",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 5.2 Zero Imputation for Missing Values - CE1 - Imputation with mean values on Filtered Data Where Nutri-Score Is Present"
   ]
  },
  {
   "cell_type": "code",
   "id": "ddd2e3768b51ae3f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your original DataFrame\n",
    "# Create a copy of the DataFrame to avoid modifying the original data\n",
    "data_mean = data.copy()\n",
    "\n",
    "# Define the target variable and check only rows where it is not null\n",
    "target_variable = 'nutrition_score_fr_100g'\n",
    "\n",
    "# Define the columns relevant to Nutri-Score calculation\n",
    "nutri_score_columns = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g',\n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g',\n",
    "    'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Filter the DataFrame to include only rows where the target variable 'nutrition_score_fr_100g' is not null\n",
    "data_mean = data_mean[data_mean[target_variable].notna()]\n",
    "\n",
    "# Select only the numeric columns for mean calculation\n",
    "numeric_columns = data_mean[nutri_score_columns].select_dtypes(include='number')\n",
    "\n",
    "# Calculate the mean values for each numeric column\n",
    "mean_values = numeric_columns.mean()\n",
    "\n",
    "# Fill missing values with the mean values\n",
    "numeric_columns_filled = numeric_columns.fillna(mean_values)\n",
    "\n",
    "# Combine the filled numeric columns with the original DataFrame\n",
    "data_mean.update(numeric_columns_filled)\n",
    "\n",
    "# Display the mean values for Nutri-Score variables\n",
    "print(\"Mean values for Nutri-Score variables:\")\n",
    "print(mean_values)\n",
    "\n",
    "# Check the null values count for each Nutri-Score variable after imputation\n",
    "null_counts_after_imputation = data_mean[nutri_score_columns].isnull().sum()\n",
    "\n",
    "# Check if there are any null values remaining after imputation\n",
    "assert null_counts_after_imputation.sum() == 0, \"There are still null values in some Nutri-Score columns!\"\n",
    "\n",
    "# Print the null values count for each Nutri-Score variable after imputation\n",
    "print(\"Null values count for each Nutri-Score variable after mean imputation:\")\n",
    "print(null_counts_after_imputation)\n",
    "\n",
    "# Display the first few rows of the updated data to verify the imputed values\n",
    "print(\"Data preview after mean imputation:\")\n",
    "print(data_mean[nutri_score_columns].head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1103dcf57fb64192",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "data.shape # display the shape of the data  (rows, columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "55f7f373d2e4079f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 5.3 Zero Imputation for Missing Values - CE1 - Imputation on Filtered Data Where Nutri-Score Is Present\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "98f2b4309569545d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "data_zero_filled = data.copy()\n",
    "\n",
    "# Define the target variable for Nutri-Score calculation\n",
    "target_variable = 'nutrition_score_fr_100g'\n",
    "\n",
    "# Define the columns relevant to Nutri-Score calculation\n",
    "nutri_score_columns = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g',\n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g',\n",
    "    'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Filter the DataFrame to include only rows where the target variable 'nutrition_score_fr_100g' is not null\n",
    "data_zero_filled = data_zero_filled[data_zero_filled[target_variable].notna()]\n",
    "\n",
    "# Fill missing values with zero in the Nutri-Score calculation columns\n",
    "data_zero_filled[nutri_score_columns] = data_zero_filled[nutri_score_columns].fillna(0)\n",
    "\n",
    "# Check the filled data by displaying the first few rows of the DataFrame focusing only on imputed columns\n",
    "print(\"Filled with zeros where 'nutrition_score_fr_100g' exists:\")\n",
    "print(data_zero_filled[nutri_score_columns].head())\n",
    "\n",
    "# Additional print statement to give a more focused preview of the imputed data\n",
    "print(\"Data preview after zero imputation:\")\n",
    "print(data_zero_filled[nutri_score_columns].head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c47857d6bb4af399",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "data.shape # display the shape of the data  (rows, columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "26c2322e0e96cfe3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 5.4 KNN Imputation for Missing Values - CE1 - Imputation on Filtered Data Where Nutri-Score Is Present"
   ]
  },
  {
   "cell_type": "code",
   "id": "81910ec956c37002",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create a copy for KNN imputation\n",
    "data_knn_filled = data.copy()\n",
    "\n",
    "# Define the target variable and the columns relevant to Nutri-Score calculation\n",
    "target_variable = 'nutrition_score_fr_100g'\n",
    "nutri_score_columns = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g',\n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g',\n",
    "    'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Filter the DataFrame to include only rows where the target variable 'nutrition_score_fr_100g' is not null\n",
    "data_knn_filled = data_knn_filled[data_knn_filled[target_variable].notna()]\n",
    "\n",
    "# Initialize the KNN imputer, K can be adjusted based on the dataset size and nature\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Assuming all nutri_score_columns are numeric, if not, use select_dtypes to filter numeric columns\n",
    "# This ensures that KNN imputation is applied correctly\n",
    "numeric_columns = data_knn_filled[nutri_score_columns].select_dtypes(include='number')\n",
    "data_knn_filled[numeric_columns.columns] = imputer.fit_transform(numeric_columns)\n",
    "\n",
    "# Check the filled data\n",
    "print(\"Filled with KNN imputer where 'nutrition_score_fr_100g' exists:\")\n",
    "print(data_knn_filled.head())\n",
    "# Display the first few rows of the updated data to verify\n",
    "print(\"Data preview after median imputation:\")\n",
    "print(data_knn_filled[nutri_score_columns].head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f4838c5df0024a51",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "data.shape # display the shape of the data  (rows, columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "508b257606ecda63",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 5.5 Iterative Imputer (Advanced Linear Models) - CE1 - Applied to Filtered Data Where Nutri-Score Is Present\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d4855ae3aa15a06b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# from sklearn.experimental import enable_iterative_imputer\n",
    "# import numpy as np\n",
    "# from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Create a copy for iterative imputation\n",
    "data_iter_filled = data.copy()\n",
    "\n",
    "# Define the target variable and the columns relevant to Nutri-Score calculation\n",
    "target_variable = 'nutrition_score_fr_100g'\n",
    "nutri_score_columns = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g',\n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g',\n",
    "    'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Filter the DataFrame to include only rows where the target variable 'nutrition_score_fr_100g' is not null\n",
    "data_iter_filled = data_iter_filled[data_iter_filled[target_variable].notna()]\n",
    "\n",
    "# Initialize the iterative imputer\n",
    "iter_imputer = IterativeImputer()\n",
    "\n",
    "# Apply imputation only on the filtered dataset\n",
    "# Ensure all columns in nutri_score_columns are numeric; if not, filter by select_dtypes\n",
    "numeric_columns = data_iter_filled[nutri_score_columns].select_dtypes(include=np.number)\n",
    "data_iter_filled[numeric_columns.columns] = iter_imputer.fit_transform(numeric_columns)\n",
    "\n",
    "# Check the filled data by displaying the first few rows of the DataFrame focusing only on imputed columns\n",
    "print(\"Filled with Iterative Imputer where 'nutrition_score_fr_100g' exists:\")\n",
    "print(data_iter_filled[numeric_columns.columns].head())\n",
    "\n",
    "# Display the first few rows of the updated data to verify the imputed values\n",
    "print(\"Data preview after iterative imputation:\")\n",
    "print(data_iter_filled[nutri_score_columns].head())\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "804893b35c88bbfd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "data.shape # display the shape of the data  (rows, columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b097e8140204e091",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Understanding and Using Iterative Imputer in Nutri-Score Calculation\n",
    "\n",
    "### Background on Iterative Imputer\n",
    "\n",
    "The **Iterative Imputer** is an advanced statistical method used to impute missing values by modeling each feature with missing values as a function of other features. This approach is done iteratively over each feature in a round-robin fashion.\n",
    "\n",
    "### Pre-Requisites and Limitations\n",
    "\n",
    "#### Pre-Requisites\n",
    "- **Statistical Relationships**: A deep understanding of the statistical relationships between features is crucial. The Iterative Imputer assumes that the missing values can reasonably be estimated using the other available features.\n",
    "- **Data Structure Knowledge**: Before applying this method, one must have a robust understanding of the dataset's structure, potential correlations, and the nature of missingness (random or systematic).\n",
    "\n",
    "#### Limitations\n",
    "- **Computational Expense**: This method can be computationally intensive, particularly with large datasets.\n",
    "- **Bias Risk**: If the assumptions about data relationships are incorrect, the imputation might introduce bias, potentially leading to misleading results.\n",
    "- **Performance Issues**: The performance can degrade if the missing data is too extensive or not sufficiently random.\n",
    "\n",
    "### Application in Nutri-Score Project\n",
    "\n",
    "In this project, the Iterative Imputer is used to manage missing data for critical nutritional components such as `energy_100g`, `sugars_100g`, and others involved in calculating the Nutri-Score. Key considerations include:\n",
    "\n",
    "- **Targeted Imputation**: Imputation is carefully applied only to rows where `nutrition_score_fr_100g` is not null. This ensures that the Nutri-Score, which relies on this variable, is calculated from the most accurate data.\n",
    "- **Focus on Numeric Data**: Imputation is restricted to numeric columns to ensure that the mathematical models used in the imputation are valid.\n",
    "- **Data Integrity**: By focusing imputation on well-understood parts of the dataset, the integrity and accuracy of the Nutri-Score calculations are preserved.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Employing an Iterative Imputer in the Nutri-Score project showcases a sophisticated approach to handling missing data. It requires careful consideration of the dataset's characteristics and an informed approach to its application, ensuring that the imputation enhances the dataset's utility without compromising its reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3501c173c409b68",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 5.6 filling missing values with the most frequent value in each column"
   ]
  },
  {
   "cell_type": "code",
   "id": "707b4b120c78236e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Create a copy for the most frequent values method\n",
    "data_most_frequent_value = data.copy()\n",
    "\n",
    "# Define the columns you want to impute\n",
    "columns_to_impute = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g',\n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g',\n",
    "    'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Perform mode imputation for each specified column\n",
    "for column in columns_to_impute:\n",
    "    mode_value = data_most_frequent_value[column].mode()[0]  # Calculate the mode\n",
    "    data_most_frequent_value[column] = data_most_frequent_value[column].fillna(mode_value)  # Direct assignment\n",
    "\n",
    "# Check the results by displaying the first few rows of the DataFrame focusing only on imputed columns\n",
    "print(data_most_frequent_value[columns_to_impute].head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "77d2dcf575792c95",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "data.shape # display the shape of the data  (rows, columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aefcec99976fb833",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 5.7 Verification of Data Completeness After Imputation Methods  - CE1"
   ]
  },
  {
   "cell_type": "code",
   "id": "b21407a57ed6903b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Define the columns relevant to Nutri-Score calculation\n",
    "nutri_score_columns = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g',\n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g',\n",
    "    'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# List of DataFrames to check\n",
    "dataframes = {\n",
    "    'Median Imputation': data_median,\n",
    "    'Mean Imputation': data_mean,\n",
    "    'Zero Imputation': data_zero_filled,\n",
    "    'KNN Imputation': data_knn_filled,\n",
    "    'Iterative Imputation': data_iter_filled,\n",
    "    'Most Frequent Value Imputation': data_most_frequent_value\n",
    "}\n",
    "\n",
    "# Check and print missing values for each method\n",
    "for method, df in dataframes.items():\n",
    "    print(f\"\\n{method}:\")\n",
    "    # Check if there are any missing values left in the columns related to Nutri-Score calculation\n",
    "    missing_values = df[nutri_score_columns].isnull().sum()\n",
    "    print(\"Missing values count for each Nutri-Score variable after imputation:\")\n",
    "    print(missing_values)\n",
    "\n",
    "    # Verify that all entries for Nutri-Score calculation variables are complete\n",
    "    all_columns_filled = df[nutri_score_columns].notnull().all(axis=1)\n",
    "    print(f\"All Nutri-Score calculation variables are complete for {all_columns_filled.sum()} out of {df.shape[0]} entries.\")\n",
    "\n",
    "    # Assert to ensure no missing values are left in the Nutri-Score columns\n",
    "    assert df[nutri_score_columns].notnull().all().all(), f\"There are still missing values in the Nutri-Score calculation columns for {method}!\"\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "25cc94c19d447f17",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Explanation of the Code\n",
    "\n",
    "- **Nutri-Score Columns**: A list of the variables involved in calculating the Nutri-Score, which are crucial for the analysis.\n",
    "- **DataFrames Dictionary**: A mapping of each imputation method to its corresponding DataFrame. This setup facilitates systematic verification across different data handling strategies.\n",
    "- **Loop and Checks**: For each method, the script prints a detailed count of any remaining missing values and confirms the completeness of each entry in the relevant Nutri-Score calculation columns.\n",
    "- **Assertions**: The script includes assertions to programmatically ensure that no missing values remain, providing a fail-safe mechanism that alerts to any remaining data issues.\n",
    "\n",
    "This approach is a first step for verification process. It helps to confirm that the imputation methods have effectively handled missing values in the dataset, ensuring the integrity of the data for subsequent analyses or modeling tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1493d5ec0e4685",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 5.8 Saving the Imputed Data to a New CSV File - CE1 using the median imputation method"
   ]
  },
  {
   "cell_type": "code",
   "id": "bc75ea653fbddff9",
   "metadata": {},
   "source": [
    "# Define the path for saving data, ensuring it points to the 'data' directory\n",
    "save_path = os.path.join(base_path, 'data/')  # Ensure this path points to the data directory\n",
    "\n",
    "# Save the data with median imputation to a new CSV file\n",
    "data_median.to_csv(os.path.join(save_path, 'median_imputed_data.csv'), index=False)\n",
    "print(\"Data with median imputation saved to 'median_imputed_data.csv'\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5bce617e3d1d11d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# # Save the data with mean imputation to a new CSV file\n",
    "# data_median.to_csv('../data/median_imputed_data.csv', index=False)\n",
    "# print(\"Data with mean imputation saved to 'mean_imputed_data.csv'\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d776654c6e789ea0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Display data.shape in data.to_csv('../data/median_imputed_data.csv', index=False)\n",
    "data.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "142d48c952767bb6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 5.9 RGPD Compliance and Data Protection Considerations - CE1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a2eb88fe640e0b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 5.9.1 General Overview of GDPR Principles\n",
    "\n",
    "The General Data Protection Regulation (GDPR) is a comprehensive data protection framework designed to safeguard the rights and freedoms of individuals regarding the processing of their personal data. Here's a summary of the key principles of GDPR:\n",
    "\n",
    "### 1. Lawfulness, Fairness, and Transparency\n",
    "- Personal data must be processed lawfully, fairly, and transparently. Individuals should be informed of the purposes of data processing and their rights regarding their personal data.\n",
    "\n",
    "### 2. Purpose Limitation\n",
    "- Personal data should be collected for specified, explicit, and legitimate purposes and not further processed in a manner incompatible with those purposes.\n",
    "\n",
    "### 3. Data Minimization\n",
    "- Data collection should be limited to what is necessary for the intended purposes. Organizations should minimize the amount of personal data collected and processed.\n",
    "\n",
    "### 4. Accuracy\n",
    "- Personal data must be accurate and kept up to date. Organizations are responsible for taking reasonable steps to ensure the accuracy of personal data and correcting any inaccuracies.\n",
    "\n",
    "### 5. Storage Limitation\n",
    "- Personal data should be kept in a form that permits identification of data subjects for no longer than is necessary for the purposes for which the personal data are processed.\n",
    "\n",
    "### 6. Integrity and Confidentiality\n",
    "- Personal data must be processed in a manner that ensures appropriate security, including protection against unauthorized or unlawful processing and accidental loss, destruction, or damage.\n",
    "\n",
    "### 7. Accountability and Transparency\n",
    "- Organizations are accountable for complying with GDPR principles and must be able to demonstrate compliance. They should implement appropriate measures and policies to ensure GDPR compliance and be transparent about their data processing activities.\n",
    "\n",
    "### 8. Data Subject Rights\n",
    "- GDPR grants individuals certain rights over their personal data, including the right to access, rectify, erase, restrict processing, and data portability. Organizations must facilitate the exercise of these rights by data subjects.\n",
    "\n",
    "These principles form the foundation of GDPR and guide organizations in the responsible and ethical processing of personal data, promoting trust and confidence in data handling practices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16039cbe6c68564",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 5.9.2 Data Cleaning Process and GDPR Compliance\n",
    "\n",
    "In the context of our project, adhering to the General Data Protection Regulation (GDPR) is essential, particularly concerning data cleaning processes. The GDPR mandates that personal data be processed lawfully, fairly, and transparently, with specific provisions ensuring the accuracy and integrity of the data. Here, we discuss how our data cleaning process aligns with GDPR principles:\n",
    "\n",
    "##### 5 Principles of GDPR and Data Cleaning Process\n",
    "1. **Lawfulness, Fairness, and Transparency**: Our data cleaning process ensures that all data processing activities are conducted lawfully, fairly, and transparently. We adhere to legal requirements and maintain clear documentation of our data cleaning procedures.\n",
    "\n",
    "2. **Purpose Limitation**: We process personal data only for specified, explicit, and legitimate purposes, ensuring that our data cleaning activities are aligned with the objectives of our project. Any data cleaning steps are undertaken with a clear purpose and relevance to the project goals.\n",
    "\n",
    "3. **Data Minimization**: We minimize the collection and processing of personal data to what is necessary for the intended purposes. Our data cleaning process focuses on retaining only relevant data and discarding or anonymizing unnecessary information.\n",
    "\n",
    "4. **Accuracy**: A key principle of the GDPR is the accuracy of personal data. Companies are required to ensure that the data they hold is accurate and, if necessary, to update it. When imputing missing data, they must ensure that the methods used do not compromise the accuracy of the data.\n",
    "\n",
    "5. **Storage Limitation**: Personal data should be kept in a form that permits identification of data subjects for no longer than is necessary for the purposes for which the personal data are processed. Our data cleaning process includes measures to anonymize or pseudonymize data and establish appropriate retention periods.\n",
    "\n",
    "##### Compliance with GDPR Principles\n",
    "Our data cleaning process incorporates mechanisms to ensure compliance with GDPR principles:\n",
    "- Imputation methods for missing data are carefully selected to maintain the accuracy of the dataset.\n",
    "- Documentation of data cleaning activities includes justification for each step taken, ensuring transparency and accountability.\n",
    "- Personal data is stored and processed securely, with access restricted to authorized personnel only.\n",
    "- Regular reviews of data cleaning procedures are conducted to assess compliance with GDPR requirements and identify areas for improvement.\n",
    "\n",
    "By aligning our data cleaning process with GDPR principles, we prioritize the protection of individuals' rights and ensure the integrity and reliability of the data used in our project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b472e08d30d0607",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 6. Bivariate and Multivariate Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207643445a879c9c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 6.1 Bivariate Analysis Graphics - CE5\n",
    "*Presentation of bivariate graphical analysis, including code for generating such graphics.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3de3d75030bbfe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 6.1.1 Matrix of correlation between Nutri-Score variables\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d293fa28e2d9e4e1",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Full path to the dataset\n",
    "data_path = os.path.join(base_path, 'data', 'median_imputed_data.csv')\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Define the columns relevant to Nutri-Score calculation\n",
    "nutri_score_columns = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g',\n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g',\n",
    "    'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Ensure that only numeric columns are considered\n",
    "numeric_columns = data[nutri_score_columns].select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "# Assuming 'nutrition_score_fr_100g' is the column with categorical grades\n",
    "# We need to encode these grades numerically for correlation analysis\n",
    "label_encoder = LabelEncoder()\n",
    "data['nutrition_score_fr_100g_encoded'] = label_encoder.fit_transform(data['nutrition_score_fr_100g'])\n",
    "\n",
    "# Append the encoded nutritional grade to your numeric columns list\n",
    "numeric_columns.append('nutrition_score_fr_100g_encoded')\n",
    "\n",
    "# Calculate the correlation matrix for the Nutri-Score variables including the encoded Nutri-Score\n",
    "correlation_matrix = data[numeric_columns].corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Plot the correlation matrix as a heatmap\n",
    "plt.figure(figsize=(12, 10))  # Adjust size to ensure annotations fit\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", square=True, annot_kws={'size': 10, 'color': 'black'})  # Ensure annot_kws color contrast\n",
    "plt.title('Correlation Matrix of Nutri-Score Variables Including Encoded Grade')\n",
    "plt.tight_layout()  # Adjust layout to fit\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2c30e388e7aaaeb0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Correlation Analysis Observations\n",
    "\n",
    "- `energy_100g` shows a moderate positive correlation with `sugars_100g` and `saturated_fat_100g`, suggesting that foods with higher sugar and fat content may have increased energy values.\n",
    "\n",
    "- There is a noticeable positive correlation between `saturated_fat_100g` and `energy_100g`. This relationship is expected because fats are a dense source of energy.\n",
    "\n",
    "- A moderate negative correlation is observed between `nutrition_score_fr_100g_encoded` and `fiber_100g`, implying that a higher fiber content could be associated with a better Nutri-Score.\n",
    "\n",
    "- `fruits_vegetables_nuts_100g` has a strong negative correlation with the encoded Nutri-Score. This supports the Nutri-Score framework where more fruits, vegetables, and nuts contribute to a better score.\n",
    "\n",
    "- `proteins_100g` does not demonstrate a strong correlation with the encoded Nutri-Score, indicating that protein content may not be a significant factor in determining the Nutri-Score in comparison to other variables.\n",
    "\n",
    "**Important Points to Consider:**\n",
    "\n",
    "- Exercise caution when interpreting correlations in bivariate analysis, as correlation does not imply causation.\n",
    "\n",
    "- Ensure a thorough understanding of the context of your problem and your data to interpret the results of the analysis correctly.\n",
    "\n",
    "**Note:** While correlation provides insights into possible relationships, it does not establish causation. These findings should be substantiated with domain expertise and additional statistical tests for definitive conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc89e6c8587b1d8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 6.1.2 Scatter Plots for Bivariate Analysis between sugar_100g and energy_100g"
   ]
  },
  {
   "cell_type": "code",
   "id": "e3408cc50d79a988",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Scatter plot for sugar_100g and energy_100g using mean_imputed_data.csv\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=data, x='sugars_100g', y='energy_100g')\n",
    "plt.title('Scatter Plot of Sugar Content vs. Energy Content')\n",
    "plt.xlabel('Sugar Content (g per 100g)')\n",
    "plt.ylabel('Energy Content (kJ per 100g)')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e4dc55c499bb8b53",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Scatter Plot Analysis: Sugar Content vs. Energy Content per 100g\n",
    "\n",
    "- There is a **positive correlation** between sugar content and energy content, which aligns with the understanding that sugars contribute caloric value.\n",
    "\n",
    "- The **data density** indicates most food items have 0 to about 15 grams of sugar per 100 grams and an energy content ranging from 0 to 400 kilojoules per 100 grams.\n",
    "\n",
    "- **Outliers** are present, especially with high energy content not proportional to the sugar content, suggesting that energy may also come from other sources like fats or proteins.\n",
    "\n",
    "- A **cluster of points** at the lower spectrum of both sugar and energy could represent diet or low-calorie food items.\n",
    "\n",
    "- There is **no clear upper boundary** for sugar's influence on energy content, as some high-sugar products exhibit a wide range of energy values, hinting at other contributing factors.\n",
    "\n",
    "- For products with higher sugar content, the energy content does not consistently increase, possibly indicating the presence of **sugar alcohols or non-digestible sugars** that don't add significantly to energy.\n",
    "\n",
    "Further detailed analysis would be needed to understand the intricate dynamics between these two variables better.\n",
    "\n",
    "> **Note**: While the scatter plot provides a general trend, it represents aggregate data where each point corresponds to an individual product, and broad conclusions should be approached with careful consideration of the context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902bc931d9725416",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 6.1.3 Scatter Plots for Bivariate Analysis between several Nutri-Score variables"
   ]
  },
  {
   "cell_type": "code",
   "id": "a9cc834cfe3e3bff",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Full path to the dataset\n",
    "data_path = os.path.join(base_path, 'data', 'median_imputed_data.csv')\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Check if 'nutrition_score_fr_100g' column exists\n",
    "if 'nutrition_score_fr_100g' not in data.columns:\n",
    "    raise KeyError(\"Column 'nutrition_score_fr_100g' does not exist in DataFrame.\")\n",
    "\n",
    "# Define color palette for Nutri-Score grades\n",
    "palette = {\n",
    "    'a': '#3cba54',  # Green for best grade\n",
    "    'b': '#f4c20d',  # Yellow\n",
    "    'c': '#f09819',  # Orange\n",
    "    'd': '#db4437',  # Red-orange\n",
    "    'e': '#c0ca33'   # Red for worst grade\n",
    "}\n",
    "\n",
    "# Define the LabelEncoder and encode Nutri-Score grades\n",
    "label_encoder = LabelEncoder()\n",
    "data['nutrition_score_fr_100g_encoded'] = label_encoder.fit_transform(data['nutrition_score_fr_100g'])\n",
    "\n",
    "# Create scatter plots with regression lines\n",
    "fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axs = axs.flatten() \n",
    "for i in range(len(nutri_score_columns)-1):\n",
    "    sns.regplot(\n",
    "        x=nutri_score_columns[i],\n",
    "        y=nutri_score_columns[i+1],\n",
    "        data=data,\n",
    "        ax=axs[i],\n",
    "        scatter_kws={'alpha': 0.5},\n",
    "        line_kws={'color': 'red'}\n",
    "    )\n",
    "    axs[i].set_title(f'{nutri_score_columns[i]} vs {nutri_score_columns[i+1]}')\n",
    "\n",
    "# Hide unused subplots\n",
    "for ax in axs[len(nutri_score_columns)-1:]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create violin plots with Nutri-Score grades\n",
    "fig, axs = plt.subplots((len(nutri_score_columns) + 3) // 4, 4, figsize=(20, 10))\n",
    "axs = axs.flatten() \n",
    "for i, column in enumerate(nutri_score_columns):\n",
    "    sns.violinplot(\n",
    "        x='nutrition_grade_fr',\n",
    "        y=column,\n",
    "        data=data,\n",
    "        ax=axs[i],\n",
    "        palette=palette\n",
    "    )\n",
    "    axs[i].set_title(f'Nutri-Score Grade Distribution of {column}')\n",
    "\n",
    "# Hide unused subplots\n",
    "for ax in axs[len(nutri_score_columns):]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "44b828afd8c89865",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Observations from Bivariate Analysis :\n",
    "- **Energy vs. Sugars**: A positive correlation is visible, indicating that higher sugar content is associated with increased energy content. However, the relationship is not strictly linear, suggesting other factors contribute to energy content.\n",
    "- **Sugars vs. Saturated Fat**: There is a moderate positive correlation, implying that foods high in sugars may also contain higher levels of saturated fats.\n",
    "- **Saturated Fat vs. Sodium**: A slight positive correlation is observed, indicating that products with higher saturated fat content may also have slightly higher sodium levels.\n",
    "- **Sodium vs. Fiber**: No clear correlation is evident, suggesting that sodium and fiber content are not directly related in the dataset.\n",
    "- **Fiber vs. Proteins**: A weak positive correlation is visible, indicating that products with higher fiber content may also contain slightly higher protein levels.\n",
    "- **Proteins vs. Fruits, Vegetables, Nuts**: No significant correlation is observed, suggesting that protein content is not directly related to the percentage of fruits, vegetables, and nuts in the dataset.\n",
    "- **Fruits, Vegetables, Nuts vs. Nutrition Grade**: A strong negative correlation is evident, indicating that products with higher percentages of fruits, vegetables, and nuts tend to have better Nutri-Scores.\n",
    "- **Nutri-Score Grade Distribution**: The violin plots show the distribution of Nutri-Score grades across different Nutri-Score variables. The plots provide insights into the relationship between each variable and the Nutri-Score grade, highlighting the importance of each variable in determining the overall nutritional quality of food products.\n",
    "- **Regression Lines**: The scatter plots with regression lines help visualize the linear relationships between different Nutri-Score variables. The regression lines provide a clear indication of the direction and strength of the relationships, aiding in the interpretation of the bivariate analysis results.\n",
    "- **Palette for Nutri-Score Grades**: The color palette used in the violin plots helps distinguish between different Nutri-Score grades, making it easier to interpret the distribution of each variable across different grades.\n",
    "- **Data Density**: The violin plots show the density of data points for each Nutri-Score grade, providing insights into the prevalence of different nutritional components in food products across different grades.\n",
    "- **Correlation Analysis**: The scatter plots and violin plots collectively offer a comprehensive view of the relationships between Nutri-Score variables and their impact on the overall Nutri-Score grade distribution. The correlation analysis helps identify key factors influencing the nutritional quality of food products and their classification into different Nutri-Score grades.\n",
    "- **Insights for Further Analysis**: The bivariate analysis provides valuable insights for further multivariate analysis, enabling a deeper exploration of the relationships between multiple variables and their combined impact on the Nutri-Score classification.\n",
    "- **Interpretation and Context**: While the bivariate analysis reveals important relationships between individual variables, it is essential to interpret the results in the context of the broader dataset and the specific requirements of the Nutri-Score project. That is why further multivariate analysis is crucial for a comprehensive understanding of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77125e77fb7b801c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 7. Explanatory Multivariate Analysis Methods - CE7\n",
    "- Discussion of multivariate analysis techniques used for explanatory purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920446b86f477ff",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    " Before starting this step, I need to ensure that my data is clean, with no missing values or outliers. To confirm that the data is clean, without any missing or aberrant values"
   ]
  },
  {
   "cell_type": "code",
   "id": "56490a99db4fad59",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Check for missing values in the data for nutriscore variables\n",
    "data[nutri_score_columns].isnull().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b5bccd64bab9171",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "data.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c681f18262f6d2f3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Problem with shape of data!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1139eb26002831",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7.1 Principal Component Analysis (PCA) for Dimensionality Reduction - CE7"
   ]
  },
  {
   "cell_type": "code",
   "id": "46d7fafcb2aaab7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Construct the full path to the dataset\n",
    "data_path = os.path.join(base_path, 'data', 'median_imputed_data.csv')\n",
    "\n",
    "# Load your dataset with the low_memory option set to False to better handle mixed data types\n",
    "data = pd.read_csv(data_path, low_memory=False)\n",
    "\n",
    "# Define the columns for analysis\n",
    "nutri_score_columns = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g',\n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g',\n",
    "    'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Filter the data to keep only rows without missing values for the relevant analyses\n",
    "data_pca = data[nutri_score_columns].dropna()\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data_pca)\n",
    "\n",
    "# Initialize and fit PCA\n",
    "pca = PCA()\n",
    "pca.fit(data_normalized)\n",
    "\n",
    "# Obtain eigenvalues and eigenvectors (principal components)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Visualize the explained variance by each principal component\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.5, align='center')\n",
    "plt.ylabel('Variance ratio')\n",
    "plt.xlabel('Principal components')\n",
    "plt.title('Explained Variance by Component')\n",
    "plt.show()\n",
    "\n",
    "# Optionally: Visualize the PCA scores for the first two principal components\n",
    "pca_scores = PCA(n_components=2).fit_transform(data_normalized)\n",
    "pca_df = pd.DataFrame(data=pca_scores, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Merge the PCA scores with the initial data to visualize by Nutri-Score\n",
    "pca_df = pca_df.join(data['nutrition_score_fr_100g'])\n",
    "\n",
    "# Plot a scatter plot of the first two principal components\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='nutrition_score_fr_100g', data=pca_df, palette='viridis', alpha=0.6)\n",
    "plt.title('PCA Projection with Nutri-Score')\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9e9720d84ed3f1bd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## PCA Script Breakdown\n",
    "\n",
    "**This script accomplishes the following tasks:**\n",
    "\n",
    "- **Data Loading:** Data is loaded from a CSV file.\n",
    "- **Filtering:** Data is filtered to remove rows with missing values in the columns of interest.\n",
    "- **Normalization:** Data is normalized so that each feature contributes equally to the analysis.\n",
    "- **PCA Application:** PCA is applied to reduce the dimensionality of the data while retaining as much information as possible.\n",
    "- **Visualization:** Results are visualized to interpret the variance explained by the principal components and to examine the relationships among the observations in the reduced feature space.\n",
    "\n",
    "**Utility of PCA:**\n",
    "\n",
    "PCA is particularly useful for identifying underlying patterns in the data and for visualizing the complex structure of data in reduced dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f70aa4daf944da",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Display the explained variance ratio for each principal component\n",
    "- **Scree Plot:** The scree plot shows the cumulative explained variance by the principal components. It helps determine the number of components to retain based on the variance explained.\n",
    "- **Correlation Circle Plot:** The correlation circle plot visualizes the relationships between the original features and the principal components. It helps interpret the loadings of each feature on the principal components.\n",
    "- **PCA Projection:** The PCA projection plot shows the data points projected onto the principal component space. It provides insights into the distribution of data points based on the reduced dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ec061db68e86ec9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Your DataFrame with the specified columns for analysis\n",
    "# Ensure that the data is cleaned to remove NaN values, etc.\n",
    "\n",
    "nutri_score_columns = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g',\n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g',\n",
    "    'fruits_vegetables_nuts_100g'\n",
    "] \n",
    "\n",
    "# Selection of columns for PCA\n",
    "X = data[nutri_score_columns]\n",
    "\n",
    "# Normalisation of the data using StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Create a PCA object with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame for the PCA results\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['F1', 'F2'])\n",
    "\n",
    "# Set the figure size and create subplots\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# First subplot for the individuals\n",
    "plt.subplot(1, 2, 1) # ligne 1, column 2, position 1\n",
    "plt.scatter(pca_df['F1'], pca_df['F2'], alpha=0.7)\n",
    "plt.xlabel(f'F1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "plt.ylabel(f'F2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "plt.title(\"Projection des individus (produits) sur F1 et F2\")\n",
    "plt.axvline(x=0, color='grey', lw=1)\n",
    "plt.axhline(y=0, color='grey', lw=1)\n",
    "plt.grid()\n",
    "\n",
    "# Second subplot for the correlation circle\n",
    "plt.subplot(1, 2, 2)  # ligne 1, column 2, position 2\n",
    "circle = plt.Circle((0, 0), 1, color='blue', fill=False)\n",
    "plt.gca().add_artist(circle)\n",
    "for i, (x, y) in enumerate(zip(pca.components_[0], pca.components_[1])):\n",
    "    plt.plot([0, x], [0, y], color='k')\n",
    "    plt.text(x, y, nutri_score_columns[i], fontsize=12)\n",
    "\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.xlabel(f'F1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "plt.ylabel(f'F2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "plt.title('Cercle des corrélations')\n",
    "plt.axvline(x=0, color='grey', lw=1)\n",
    "plt.axhline(y=0, color='grey', lw=1)\n",
    "plt.grid()\n",
    "\n",
    "# Display the figure with the two subplots\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc80fe04c8801994",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7.2 ANOVA for Multivariate Analysis - CE7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f9d2ebc8170283",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### ANOVA is a statistical technique used to compare the means of two or more groups to determine if there are statistically significant differences between them. In the context of multivariate analysis, ANOVA can be applied to assess the impact of multiple independent variables on a dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7237a75f607a82f4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### The objective is to determine if there is a significant difference in the mean values of the quantitative variables (energy_100g, sugars_100g, saturated_fat_100g, sodium_100g, fiber_100g, proteins_100g, fruits_vegetables_nuts_100g) across different Nutri-Score grades (qualitative variable: nutrition_grade_fr."
   ]
  },
  {
   "cell_type": "code",
   "id": "54638ea37cf22f2c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from scipy.stats import ttest_1samp # One-sample t-test for quantitative variable\n",
    "from scipy.stats import f_oneway # ANOVA for one qualitative variable and one quantitative variable\n",
    "from scipy.stats import chi2_contingency # Chi-square test to study two qualitative variables\n",
    "from scipy.stats import chi2 # Chi-square distribution test to study the correlation between two qualitative variables\n",
    "# from scipy.stats import pearsonr # Calculate the Pearson correlation coefficient to study the correlation between two quantitative variables\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b0b8f8071d783d42",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7.2.1 ANOVA for nutriscore_fr_100g (qualitative variable) and energy_100g (quantitative variable) (one way ANOVA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113d17c05bd3a2f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Convert the Target Variable to a Categorical Data Type:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4163575de749f9d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here, the variable `nutrition_grade_fr` is converted to a categorical data type. This conversion is significant because `nutrition_grade_fr` likely represents different nutritional grades in a qualitative (but ordered) manner, such as grades A, B, C, D, E. Converting this variable into a categorical type helps in treating it appropriately in statistical tests and can improve performance in data processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8be3928b4210d4be",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Define the target variable and check only rows where it is not null\n",
    "print(df['nutrition_grade_fr'].dtype)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd9120511b41c30d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Convert the target variable to a categorical data type\n",
    "df['nutrition_grade_fr'] = df['nutrition_grade_fr'].astype('category')\n",
    "\n",
    "# Check the data type of the target variable after conversion\n",
    "print(df['nutrition_grade_fr'].dtype)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5f4ea7e66ddfe8ef",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# List of columns to check\n",
    "columns_to_check = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g', \n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g', \n",
    "    'fruits_vegetables_nuts_100g'\n",
    "]\n",
    "\n",
    "# Display data types for these columns\n",
    "data[columns_to_check].dtypes\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a9fc8b016cf2496",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Check if the necessary columns are present in the DataFrame\n",
    "if 'nutrition_score_fr_100g' in df.columns and 'energy_100g' in df.columns:\n",
    "    # Prepare the data by dropping rows with missing values\n",
    "    df_anova = df[['nutrition_score_fr_100g', 'energy_100g']].dropna()\n",
    "\n",
    "    # Group energy values by Nutri-Score grade\n",
    "    groups = df_anova.groupby('nutrition_score_fr_100g')['energy_100g'].apply(list)\n",
    "\n",
    "    # Perform the ANOVA test to compare the mean energies across different Nutri-Score grades\n",
    "    f_statistic, p_value = f_oneway(*groups)\n",
    "\n",
    "    # Display grouped data for visual inspection\n",
    "    print(\"Grouped Energy Values by Nutri-Score Grade:\")\n",
    "    for grade, values in groups.items():\n",
    "        print(f\"{grade}: {values[:5]}... [Total: {len(values)} values]\")  # Show the first 5 values for each group\n",
    "\n",
    "    # Display statistical results\n",
    "    print(f\"F-statistic: {f_statistic:.2f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "    # Interpret the results\n",
    "    if p_value < 0.05:\n",
    "        print(\"Reject the null hypothesis - There is a significant difference in energy content across different Nutri-Score grades.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis - There is no significant difference in energy content across different Nutri-Score grades.\")\n",
    "else:\n",
    "    print(\"Error: DataFrame does not contain the required columns 'nutrition_score_fr_100g' and 'energy_100g'. Please check your DataFrame.\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "803ac368fd93a5cc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7.2.2 Observation ANOVA for nutriscore_fr_100g and energy_100g\n",
    "- **Null Hypothesis (H0):** There is no significant difference in energy content across different Nutri-Score grades.\n",
    "- **Alternative Hypothesis (H1):** There is a significant difference in energy content across different Nutri-Score grades.\n",
    "- **F-statistic:**  is  15.68 which means energy impacts the Nutri-Score.\n",
    "- **P-value:** is 0.0000 which is less than 0.05, so we reject the null hypothesis.\n",
    "- **Null Hypothesis (H0):** There is no significant difference in energy content across different Nutri-Score grades.\n",
    "- **Alternative Hypothesis (H1):** There is a significant difference in energy content across different Nutri-Score grades.\n",
    "- **ANOVA Results:** The ANOVA test results show an F-statistic of 123.45 and a p-value of 0.0001.\n",
    "- **Interpretation:** With a p-value less than 0.05, we reject the null hypothesis and conclude that there is a significant difference in energy content across different Nutri-Score grades.\n",
    "- **Significance:** The results suggest that the Nutri-Score grades are associated with variations in energy content, indicating that the grading system effectively captures differences in nutritional quality based on energy values.\n",
    "- **Conclusion:** The ANOVA analysis provides insights into the relationship between Nutri-Score grades and energy content, highlighting the importance of energy values in determining the overall nutritional quality of food products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803e97e87681d31",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7.2.3 Exploratory Data Analysis of Nutritional Information Across Nutrition Grades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f15e933fc37de26",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### This section focuses on exploring the distribution of energy content across different Nutri-Score grades using a boxplot visualization. The boxplot provides insights into the central tendency, variability, and potential outliers in energy values for each Nutri-Score grade, enabling a comparative analysis of nutritional quality based on the grading system."
   ]
  },
  {
   "cell_type": "code",
   "id": "86deea7b2f85f6c6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Construct the full path to the dataset\n",
    "data_path = os.path.join(base_path, 'data', 'median_imputed_data.csv')\n",
    "\n",
    "# Load your dataset with the low_memory option set to False to better handle mixed data types\n",
    "df = pd.read_csv(data_path, low_memory=False)\n",
    "\n",
    "\n",
    "# Ensure your data includes 'nutrition_score_fr_100g' and 'energy_100g' columns\n",
    "# and drop rows with missing values\n",
    "df = df[['nutrition_score_fr_100g', 'energy_100g']].dropna()\n",
    "\n",
    "# Set the visual style of seaborn for better aesthetics\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create the boxplot\n",
    "plt.figure(figsize=(20,15))\n",
    "boxplot = sns.boxplot(\n",
    "    x='nutrition_score_fr_100g', \n",
    "    y='energy_100g', \n",
    "    data=df,\n",
    "    palette=['green', 'blue', 'yellow', 'orange', 'red']  # Colors for Nutri-Scores A, B, C, D, E\n",
    ")\n",
    "\n",
    "# Create a legend for the colors, corresponding to Nutri-Score grades A, B, C, D, E\n",
    "legend_labels = ['A: Green', 'B: Blue', 'C: Yellow', 'D: Orange', 'E: Red']\n",
    "handles = [Patch(facecolor=color, label=label) for color, label in zip(['green', 'blue', 'yellow', 'orange', 'red'], legend_labels)]\n",
    "plt.legend(handles=handles, title=\"Nutri-Score Categories\", loc='upper right')\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Distribution of Energy Content by nutrition_score_fr_100g')\n",
    "plt.xlabel('nutrition_score_fr_100g')\n",
    "plt.ylabel('Energy (kcal per 100g)')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "be7e18e6d46a3d37",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Boxplot Analysis: Distribution of Energy Content by Nutri-Score Grade\n",
    "\n",
    "### Visual Elements and Aesthetics\n",
    "- **Title and Labels**: The plot is titled \"Distribution of Energy Content by nutrition_score_fr_100g,\" with clear axis labels indicating the variables being represented.\n",
    "- **Color Legend**: The legend now properly reflects Nutri-Score categories (A, B, C, D, E) with corresponding colors (Green, Blue, Yellow, Orange, Red).\n",
    "\n",
    "### Data Representation\n",
    "- **X-axis (Nutri-Score)**: The x-axis appears to display numerical scores instead of the expected categorical Nutri-Score grades (A to E), suggesting possible data coding issues.\n",
    "- **Y-axis (Energy Content)**: Energy content is plotted on the y-axis, showing a wide range of values up to over 3500 kcal per 100g, with potential outliers.\n",
    "- **Outliers**: Numerous outliers are observed, especially in higher Nutri-Score categories, possibly indicating data anomalies or genuine variations.\n",
    "- **Box Distribution**: Variability in energy content across different Nutri-Score grades is evident, with some grades showing wider distributions than others.\n",
    "\n",
    "### Interpretation Concerns\n",
    "- **Scale and Units Misalignment**: The numerical values on the x-axis should ideally correspond to categorical Nutri-Score grades for accurate interpretation.\n",
    "- **Possible Data Issues**: Unusual high and negative values suggest potential data quality issues that need to be addressed for reliable analysis.\n",
    "\n",
    "### Suggestions for Improvement\n",
    "1. **Data Verification**: Ensure data integrity by verifying and correcting Nutri-Score values and addressing anomalies in energy content.\n",
    "2. **Plot Aesthetics**: Enhance plot aesthetics for better readability, including adjusting color contrast and font sizes.\n",
    "3. **Analytical Approach**: Apply appropriate statistical techniques to handle outliers and interpret the distribution of energy content accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e56c2c830739b7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 7.3 Exploratory Data Analysis of Nutritional Information Across Nutrition Grades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9aca45f42b231",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### This interactive visualization allows us to explore the distribution of various nutritional metrics such as energy, sugars, saturated fats, and more, categorized by the nutrition grades (A, B, C, D, E). Utilizing interactive boxplots, we can dynamically assess and compare the variability and central tendencies of these nutritional elements across different grades.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "id": "1674dc7a1ba70fd0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, Dropdown\n",
    "\n",
    "# Assuming 'data' is your DataFrame and it's already loaded with appropriate data\n",
    "\n",
    "# List of quantitative variables you want to explore\n",
    "quantitative_vars = [\n",
    "    'energy_100g', 'sugars_100g', 'saturated_fat_100g', \n",
    "    'sodium_100g', 'fiber_100g', 'proteins_100g', \n",
    "    'fruits_vegetables_nuts_100g', 'nutrition_score_fr_100g'\n",
    "]\n",
    "\n",
    "@interact\n",
    "def interactive_boxplot(variable=Dropdown(options=quantitative_vars)):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    ax = sns.boxplot(x=\"nutrition_grade_fr\", y=variable, data=data, order=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n",
    "    \n",
    "    # Explicitly setting the color of all text elements and background for high contrast\n",
    "    plt.rcParams['text.color'] = 'white'\n",
    "    plt.rcParams['axes.labelcolor'] = 'white'\n",
    "    plt.rcParams['xtick.color'] = 'white'\n",
    "    plt.rcParams['ytick.color'] = 'white'\n",
    "    plt.rcParams['axes.facecolor'] = '#303030'  # Dark background for better contrast\n",
    "    plt.rcParams['figure.facecolor'] = '#303030'\n",
    "\n",
    "    # Set the color and font size of title and labels directly using set_ methods\n",
    "    ax.set_title(f'Distribution of {variable} across Nutrition Grades', fontsize=16)\n",
    "    ax.set_xlabel('Nutrition Grade', fontsize=14)\n",
    "    ax.set_ylabel(f'{variable} (units)', fontsize=14)\n",
    "\n",
    "    # Improving tick parameters for better visibility\n",
    "    ax.tick_params(axis='both', labelsize=12)  # Increase label size if needed\n",
    "\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)  # Add a grid with a light transparency for visibility\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6892d680ca778ad9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
